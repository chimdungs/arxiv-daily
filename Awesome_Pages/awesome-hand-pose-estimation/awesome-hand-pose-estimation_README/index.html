
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://chimdungs.github.io/arxiv-daily/Awesome_Pages/awesome-hand-pose-estimation/awesome-hand-pose-estimation_README/">
      
      
        <link rel="prev" href="../">
      
      
        <link rel="next" href="../code-of-conduct/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.1">
    
    
      
        <title>Awesome Hand Pose Estimation - arxiv-daily</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.a40c8224.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config",""),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#awesome-hand-pose-estimation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="arxiv-daily" class="md-header__button md-logo" aria-label="arxiv-daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            arxiv-daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Awesome Hand Pose Estimation
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/chimdungs/arxiv-daily" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="arxiv-daily" class="md-nav__button md-logo" aria-label="arxiv-daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    arxiv-daily
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/chimdungs/arxiv-daily" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../3D%20Vision/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    3D Vision
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            3D Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/3D%20Object%20Detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3D Object Detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/3D%20Object%20Tracking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3D Object Tracking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/3D%20Reconstruction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3D Reconstruction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud%20Completion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud Completion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud%20Matching/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud Matching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud%20Registration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud Registration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud%20Segmentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud Segmentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Awesome Pages
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Awesome Pages
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Awesome-Diffusion-Models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome Diffusion Models
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Awesome Diffusion Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-Diffusion-Models/Awesome-Diffusion-Models_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome Diffusion Models README
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Awesome-Foundation-Models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome Foundation Models
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Awesome Foundation Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-Foundation-Models/Awesome-Foundation-Models_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome-Foundation-Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Awesome-GenAI-Watermarking/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome GenAI Watermarking
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Awesome GenAI Watermarking
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-GenAI-Watermarking/Awesome-GenAI-Watermarking_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome-GenAI-Watermarking
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Awesome-LLM/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome LLM
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Awesome LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/Awesome-LLM_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome-LLM Awesome
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/LICENSE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LICENSE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/contributing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contribution Guidelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_5" >
        
          
          <label class="md-nav__link" for="__nav_3_4_5" id="__nav_3_4_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Paper list
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4_5">
            <span class="md-nav__icon md-icon"></span>
            Paper list
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/RLHF/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/Retrieval_Augmented_Generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Retrieval-Augmented Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/acceleration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Acceleration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/alignment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Alignment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/application/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Application
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/augmentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Augmentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/chain_of_thougt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Chain-of-Thought
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/code_pretraining/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code pretraining
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM-Evaluation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/in_context_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    In-context Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/instruction-tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Instruction-Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Moe
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/prompt_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Awesome-Multimodal-Large-Language-Models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome Multimodal Large Language Models
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            Awesome Multimodal Large Language Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-Multimodal-Large-Language-Models/Awesome-Multimodal-Large-Language-Models_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome-Multimodal-Large-Language-Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5_3" >
        
          
          <label class="md-nav__link" for="__nav_3_5_3" id="__nav_3_5_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Images
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5_3">
            <span class="md-nav__icon md-icon"></span>
            Images
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-Multimodal-Large-Language-Models/images/readme/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Readme
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../awesome-3D-generation/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    awesome 3D generation
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            awesome 3D generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-3D-generation/awesome-3D-generation_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome 3D Generation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../awesome-3d-reconstruction-papers/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome 3d reconstruction papers
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            Awesome 3d reconstruction papers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-3d-reconstruction-papers/awesome-3d-reconstruction-papers_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome 3D Reconstruction Papers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome hand pose estimation
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_8">
            <span class="md-nav__icon md-icon"></span>
            Awesome hand pose estimation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Awesome Hand Pose Estimation
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Awesome Hand Pose Estimation
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#contents" class="md-nav__link">
    <span class="md-ellipsis">
      Contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arxiv-papers" class="md-nav__link">
    <span class="md-ellipsis">
      arXiv Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="arXiv Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arxiv220604927-pushing-the-envelope-for-depth-based-semi-supervised-3d-hand-pose-estimation-with-consistency-training-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2206.04927] Pushing the Envelope for Depth-Based Semi-Supervised 3D Hand Pose Estimation with Consistency Training. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv220604927-ego2handspose-a-dataset-for-egocentric-two-hand-3d-global-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2206.04927] Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv220607117-trihorn-net-a-model-for-accurate-depth-based-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2206.07117] TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv220204533-nimble-a-non-rigid-hand-model-with-bones-and-muscles-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2202.04533] NIMBLE: A Non-rigid Hand Model with Bones and Muscles. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv220109548-consistent-3d-hand-reconstruction-in-video-via-self-supervised-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2201.09548] Consistent 3D Hand Reconstruction in Video via self-supervised Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv211106500-dynamic-iterative-refinement-for-efficient-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2111.06500] Dynamic Iterative Refinement for Efficient 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210914744-the-object-at-hand-automated-editing-for-mixed-reality-video-guidance-from-hand-object-interactions-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2109.14744] The Object at Hand: Automated Editing for Mixed Reality Video Guidance from Hand-Object Interactions. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210914657-understanding-egocentric-hand-object-interactions-from-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2109.14657] Understanding Egocentric Hand-Object Interactions from Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210911747-a-multi-view-video-based-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2109.11747] A Multi-View Video-Based 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210813995-realistic-hands-a-hybrid-model-for-3d-hand-reconstruction-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2108.13995] Realistic Hands: A Hybrid Model for 3D Hand Reconstruction. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210807044-towards-unconstrained-joint-hand-object-reconstruction-from-rgb-videos-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2108.07044] Towards unconstrained joint hand-object reconstruction from RGB videos. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210700887-ho-3d_v3-improving-the-accuracy-of-hand-object-annotations-of-the-ho-3d-dataset-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2107.00887] HO-3D_v3: Improving the Accuracy of Hand-Object Annotations of the HO-3D Dataset. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210605954-adversarial-motion-modelling-helps-semi-supervised-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2106.05954] Adversarial Motion Modelling helps Semi-supervised Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210604324-contrastive-representation-learning-for-hand-shape-estimation-pdf-project-code-data" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2106.04324] Contrastive Representation Learning for Hand Shape Estimation. [PDF] [Project] [Code] [Data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210414639-handsformer-keypoint-transformer-for-monocular-3d-pose-estimation-ofhands-and-object-in-interaction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2104.14639] HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation ofHands and Object in Interaction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210207067-fasthand-fast-hand-pose-estimation-from-a-monocular-camera-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2102.07067] FastHand: Fast Hand Pose Estimation From A Monocular Camera. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv201211260-unsupervised-domain-adaptation-with-temporal-consistent-self-training-for-3d-hand-object-joint-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2012.11260] Unsupervised Domain Adaptation with Temporal-Consistent Self-Training for 3D Hand-Object Joint Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv200808324-frankmocap-fast-monocular-3d-hand-and-body-motion-capture-by-regression-and-integration-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2008.08324] FrankMocap: Fast Monocular 3D Hand and Body Motion Capture by Regression and Integration. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv200605927-recent-advances-in-3d-object-and-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2006.05927] Recent Advances in 3D Object and Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv200108047-attention-a-lightweight-2d-hand-pose-estimation-approach-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2001.08047] Attention! A Lightweight 2D Hand Pose Estimation Approach. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv200100702-handaugment-a-simple-data-augmentation-method-for-depth-based-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2001.00702] HandAugment: A Simple Data Augmentation Method for Depth-Based 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv191212436-silhouette-net-3d-hand-pose-estimation-from-silhouettes-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1912.12436] Silhouette-Net: 3D Hand Pose Estimation from Silhouettes. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv191112501-an-end-to-end-framework-for-unconstrained-monocular-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1911.12501] An End-to-end Framework for Unconstrained Monocular 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv191201875-graphposegan-3d-hand-pose-estimation-from-a-monocular-rgb-image-via-adversarial-learning-on-graphs-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1912.01875] GraphPoseGAN: 3D Hand Pose Estimation from a Monocular RGB Image via Adversarial Learning on Graphs. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv191107424-capturing-hand-articulations-using-recurrent-neural-network-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1911.07424] Capturing Hand Articulations using Recurrent Neural Network for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv180700898-model-based-hand-pose-estimation-for-generalized-hand-shape-with-appearance-normalization-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1807.00898] Model-based Hand Pose Estimation for Generalized Hand Shape with Appearance Normalization. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv170509606-end-to-end-global-to-local-cnn-learning-for-hand-pose-recovery-in-depth-data-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1705.09606] End-to-end Global to Local CNN Learning for Hand Pose Recovery in Depth data. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv170402224-hand3d-hand-pose-estimation-using-3d-neural-network-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1704.02224] Hand3D: Hand Pose Estimation using 3D Neural Network. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv161200596-learning-to-search-on-manifolds-for-3d-pose-estimation-of-articulated-objects-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1612.00596] Learning to Search on Manifolds for 3D Pose Estimation of Articulated Objects. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#journal-papers" class="md-nav__link">
    <span class="md-ellipsis">
      Journal Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Journal Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tpami-ijcv" class="md-nav__link">
    <span class="md-ellipsis">
      TPAMI / IJCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TPAMI / IJCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2024-tpami-evhandpose-event-based-3d-hand-pose-estimation-with-sparse-supervision-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2024 TPAMI] EvHandPose: Event-Based 3D Hand Pose Estimation With Sparse Supervision. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2024-tpami-learning-a-contact-potential-field-for-modeling-the-hand-object-interaction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2024 TPAMI] Learning a Contact Potential Field for Modeling the Hand-Object Interaction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-tpami-consistent-3d-hand-reconstruction-in-video-via-self-supervised-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 TPAMI] Consistent 3D Hand Reconstruction in Video via self-supervised Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-tpami-recurrent-3d-hand-pose-estimation-using-cascaded-pose-guided-3d-alignments-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 TPAMI] Recurrent 3D Hand Pose Estimation Using Cascaded Pose-guided 3D Alignments. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-tpami-handvoxnet-3d-hand-shape-and-pose-estimation-using-voxel-based-neural-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 TPAMI] HandVoxNet++: 3D Hand Shape and Pose Estimation using Voxel-Based Neural Networks. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-tpami-3d-hand-pose-estimation-using-synthetic-data-and-weakly-labeled-rgb-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 TPAMI] 3D Hand Pose Estimation Using Synthetic Data and Weakly Labeled RGB Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-tpami-generalized-feedback-loop-for-joint-hand-object-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 TPAMI] Generalized Feedback Loop for Joint Hand-Object Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-tpami-feature-boosting-network-for-3d-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 TPAMI] Feature Boosting Network For 3D Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tpami-opening-the-black-box-hierarchical-sampling-optimization-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TPAMI] Opening the Black Box: Hierarchical Sampling Optimization for Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-ijcv-depth-based-hand-pose-estimation-methods-data-and-challenges-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 IJCV] Depth-Based Hand Pose Estimation: Methods, Data, and Challenges. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tpami-real-time-3d-hand-pose-estimation-with-3d-convolutional-neural-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TPAMI] Real-time 3D Hand Pose Estimation with 3D Convolutional Neural Networks. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-ijcv-lie-x-depth-image-based-articulated-object-pose-estimation-tracking-and-action-recognition-on-lie-groups-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 IJCV] Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-tpami-latent-regression-forest-structured-estimation-of-3d-hand-poses-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 TPAMI] Latent Regression Forest: Structured Estimation of 3D Hand Poses. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-ijcv-capturing-hands-in-action-using-discriminative-salient-points-and-physics-simulation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 IJCV] Capturing Hands in Action using Discriminative Salient Points and Physics Simulation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-ijcv-estimate-hand-poses-efficiently-from-single-depth-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 IJCV] Estimate Hand Poses Efficiently from Single Depth Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-journals" class="md-nav__link">
    <span class="md-ellipsis">
      Other Journals
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Other Journals">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2023-eswa-trihorn-net-a-model-for-accurate-depth-based-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 ESWA] TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-tip-a-dual-branch-self-boosting-framework-for-self-supervised-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 TIP] A Dual-Branch Self-Boosting Framework for Self-Supervised 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-technologies-a-survey-on-gan-based-data-augmentation-for-hand-pose-estimation-problem-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 Technologies] A Survey on GAN-Based Data Augmentation for Hand Pose Estimation Problem. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-tcsvt-3d-hand-pose-estimation-from-monocular-rgb-with-feature-interaction-module-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 TCSVT] 3D Hand Pose Estimation from Monocular RGB with Feature Interaction Module. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-tip-hand-pose-understanding-with-large-scale-photo-realistic-rendering-dataset-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 TIP] Hand Pose Understanding with Large-Scale Photo-Realistic Rendering Dataset. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-tip-joint-hand-object-3d-reconstruction-from-a-single-image-with-cross-branch-feature-fusion-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 TIP] Joint Hand-object 3D Reconstruction from a Single Image with Cross-branch Feature Fusion. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-neurocomputing-spatial-aware-stacked-regression-network-for-real-time-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 Neurocomputing] Spatial-aware Stacked Regression Network for Real-time 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-tmm-differentiable-spatial-regression-a-novel-method-for-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 TMM] Differentiable Spatial Regression: A Novel Method for 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-tip-weakly-supervised-learning-for-single-depth-based-hand-shape-recovery-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 TIP] Weakly-supervised Learning for Single Depth based Hand Shape Recovery. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-signal-process-image-commun-accurate-3d-hand-pose-estimation-network-utilizing-joints-information-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 Signal Process Image Commun] Accurate 3D Hand Pose Estimation Network Utilizing Joints Information. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-tcsvt-improve-regression-network-on-depth-hand-pose-estimation-with-auxiliary-variable-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 TCSVT] Improve Regression Network on Depth Hand Pose Estimation with Auxiliary Variable. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-tvcg-3d-hand-tracking-in-the-presence-of-excessive-motion-blur-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 TVCG] 3D Hand Tracking in the Presence of Excessive Motion Blur. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-computers-graphics-simple-and-effective-deep-hand-shape-and-pose-regression-from-a-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 Computers &amp; Graphics] Simple and effective deep hand shape and pose regression from a single depth image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-tip-srhandnet-real-time-2d-hand-pose-estimation-with-simultaneous-region-localization-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 TIP] SRHandNet: Real-time 2D Hand Pose Estimation with Simultaneous Region Localization. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-sensors-whsp-net-a-weakly-supervised-approach-for-3d-hand-shape-and-pose-recovery-from-a-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 Sensors] WHSP-Net: A Weakly-Supervised Approach for 3D Hand Shape and Pose Recovery from a Single Depth Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-ra-l-variational-object-aware-3d-hand-pose-from-a-single-rgb-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 RA-L] Variational Object-aware 3D Hand Pose from a Single RGB Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-pr-a-survey-on-3d-hand-pose-estimation-cameras-methods-and-datasets-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 PR] A Survey on 3D Hand Pose Estimation: Cameras, Methods, and Datasets. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-neurocomputing-a-crnn-module-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 Neurocomputing] A CRNN module for hand pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-ivc-large-scale-multiview-3d-hand-pose-dataset-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 IVC] Large-scale Multiview 3D Hand Pose Dataset. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tcsvt-mask-pose-cascaded-cnn-for-2d-hand-pose-estimation-from-single-color-image-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TCSVT] Mask-pose Cascaded CNN for 2D Hand Pose Estimation from Single Color Image. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-ivc-top-down-model-fitting-for-hand-pose-recovery-in-sequences-of-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 IVC] Top-down model fitting for hand pose recovery in sequences of depth images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tcyb-context-aware-deep-spatio-temporal-network-for-hand-pose-estimation-from-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TCYB] Context-Aware Deep Spatio-Temporal Network for Hand Pose Estimation from Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-ieee-access-shpr-net-deep-semantic-hand-pose-regression-from-point-clouds-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 IEEE Access] SHPR-Net: Deep Semantic Hand Pose Regression From Point Clouds. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-neurocomputing-pose-guided-structured-region-ensemble-network-for-cascaded-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 Neurocomputing] Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-pr-learning-a-deep-network-with-spherical-part-model-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 PR] Learning a deep network with spherical part model for 3D hand pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tip-robust-3d-hand-pose-estimation-from-single-depth-images-using-multi-view-cnns-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TIP] Robust 3D Hand Pose Estimation from Single Depth Images using Multi-View CNNs. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-jvci-region-ensemble-network-towards-good-practices-for-deep-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 JVCI] Region Ensemble Network: Towards Good Practices for Deep 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-tcyb-hough-forest-with-optimized-leaves-for-global-hand-pose-estimation-with-arbitrary-postures-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 TCYB] Hough Forest with Optimized Leaves for Global Hand Pose Estimation with Arbitrary Postures. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-tcsvt-robust-rgb-d-hand-tracking-using-deep-learning-priors-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 TCSVT] Robust RGB-D Hand Tracking Using Deep Learning Priors. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-cviu-hand-pose-estimation-through-semi-supervised-and-weakly-supervised-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 CVIU] Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-neurocomputing-multi-task-multi-domain-learning-application-to-semantic-segmentation-and-pose-regression-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 Neurocomputing] Multi-task, Multi-domain Learning: application to semantic segmentation and pose regression. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-cviu-guided-optimisation-through-classification-and-regression-for-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 CVIU] Guided Optimisation through Classification and Regression for Hand Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-tcsvt-resolving-ambiguous-hand-pose-predictions-by-exploiting-part-correlations-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 TCSVT] Resolving Ambiguous Hand Pose Predictions by Exploiting Part Correlations. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2014-tmm-parsing-the-hand-in-depth-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2014 TMM] Parsing the Hand in Depth Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conference-papers" class="md-nav__link">
    <span class="md-ellipsis">
      Conference Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Conference Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2024-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2024 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2024 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#handdagt-a-denoising-adaptive-graph-transformer-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dense-hand-objectho-graspnet-with-full-grasping-taxonomy-and-dynamics-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Dense Hand-Object(HO) GraspNet with Full Grasping Taxonomy and Dynamics. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-pose-estimation-in-everyday-egocentric-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Pose Estimation in Everyday Egocentric Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-reconstruction-of-objects-in-hands-without-real-world-3d-supervision-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Reconstruction of Objects in Hands without Real World 3D Supervision. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-3d-hand-reconstruction-with-knowledge-prior-and-uncertainty-guidance-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mlphand-real-time-multi-view-3d-hand-reconstruction-via-mlp-modeling-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • MLPHand: Real Time Multi-View 3D Hand Reconstruction via MLP Modeling. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#are-synthetic-data-useful-for-egocentric-hand-object-interaction-detection-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-sequence-recovery-from-real-blurry-images-and-event-stream-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Sequence Recovery from Real Blurry Images and Event Stream. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coarse-to-fine-implicit-representation-learning-for-3d-hand-object-reconstruction-from-a-single-rgb-d-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handdgp-camera-space-hand-mesh-prediction-with-differentiable-global-positioning-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#controlling-the-world-by-sleight-of-hand-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Controlling the World by Sleight of Hand. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-cross-hand-policies-of-high-dof-reaching-and-grasping-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning Cross-hand Policies of High-DOF Reaching and Grasping. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-sco-dual-stream-conditional-diffusion-for-monocular-hand-held-object-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nl2contact-natural-language-guided-3d-hand-object-contact-modeling-with-diffusion-model-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks-and-challenges-in-pose-estimation-for-egocentric-hand-interactions-with-objects-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on-the-utility-of-3d-hand-poses-for-action-recognition-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • On the Utility of 3D Hand Poses for Action Recognition. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attentionhand-text-driven-controllable-hand-image-generation-for-3d-hand-reconstruction-in-the-wild-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2024-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2024 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2024 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#physics-aware-hand-object-interaction-denoising" class="md-nav__link">
    <span class="md-ellipsis">
      • Physics-aware Hand-object Interaction Denoising
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hoidiffusion-generating-realistic-3d-hand-object-interaction-data" class="md-nav__link">
    <span class="md-ellipsis">
      • HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#urhand-universal-relightable-hands-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • URHand: Universal Relightable Hands. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#oakink2-a-dataset-of-embodied-hands-object-manipulation-in-long-horizon-complex-task-completion" class="md-nav__link">
    <span class="md-ellipsis">
      • OakInk2: A Dataset of Embodied Hands-Object Manipulation in Long-Horizon Complex Task Completion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interhandgen-two-hand-interaction-generation-via-cascaded-reverse-diffusion" class="md-nav__link">
    <span class="md-ellipsis">
      • InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moho-learning-single-view-hand-held-object-reconstruction-with-multi-view-occlusion-aware-supervision-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ohta-one-shot-hand-avatar-via-data-driven-implicit-priors-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • OHTA: One-shot Hand Avatar via Data-driven Implicit Priors. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handbooster-boosting-3d-hand-mesh-reconstruction-by-conditional-synthesis-and-sampling-of-hand-object-interactions" class="md-nav__link">
    <span class="md-ellipsis">
      • HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handdiff-3d-hand-pose-estimation-with-diffusion-on-image-point-cloud" class="md-nav__link">
    <span class="md-ellipsis">
      • HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text2hoi-text-guided-3d-motion-generation-for-hand-object-interaction" class="md-nav__link">
    <span class="md-ellipsis">
      • Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#both2hands-inferring-3d-hands-from-both-text-prompts-and-body-dynamics-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gears-local-geometry-aware-hand-object-interaction-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      • GEARS: Local Geometry-aware Hand-object Interaction Synthesis.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-simple-baseline-for-efficient-hand-mesh-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • A Simple Baseline for Efficient Hand Mesh Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hold-category-agnostic-3d-reconstruction-of-interacting-hands-and-objects-from-video-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ms-mano-enabling-hand-pose-tracking-with-biomechanical-constraints" class="md-nav__link">
    <span class="md-ellipsis">
      • MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hoist-former-hand-held-objects-identification-segmentation-and-tracking-in-the-wild" class="md-nav__link">
    <span class="md-ellipsis">
      • HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bitt-bi-directional-texture-reconstruction-of-interacting-two-hands-from-a-single-image" class="md-nav__link">
    <span class="md-ellipsis">
      • BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#authentic-hand-avatar-from-a-phone-scan-via-universal-hand-model" class="md-nav__link">
    <span class="md-ellipsis">
      • Authentic Hand Avatar from a Phone Scan via Universal Hand Model.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reconstructing-hands-in-3d-with-transformers-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Reconstructing Hands in 3D with Transformers. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#complementing-event-streams-and-rgb-frames-for-hand-mesh-reconstruction" class="md-nav__link">
    <span class="md-ellipsis">
      • Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#single-to-dual-view-adaptation-for-egocentric-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hoisdf-constraining-3d-hand-object-pose-estimation-with-global-signed-distance-fields-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HOISDF: Constraining 3D Hand Object Pose Estimation with Global Signed Distance Fields. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g-hop-generative-hand-object-prior-for-interaction-reconstruction-and-grasp-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      • G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hhmr-holistic-hand-mesh-recovery-by-enhancing-the-multimodal-controllability-of-graph-diffusion-models" class="md-nav__link">
    <span class="md-ellipsis">
      • HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models.
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conference-papers_1" class="md-nav__link">
    <span class="md-ellipsis">
      Conference Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Conference Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2023-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2023 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2023 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-probabilistic-attention-model-with-occlusion-aware-texture-regression-for-3d-hand-reconstruction-from-a-single-rgb-image" class="md-nav__link">
    <span class="md-ellipsis">
      • A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a2j-transformer-anchor-to-joint-transformer-network-for-3d-interacting-hand-pose-estimation-from-a-single-rgb-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memahand-exploiting-mesh-mano-interaction-for-single-image-two-hand-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arctic-a-dataset-for-dexterous-bimanual-hand-object-manipulation-project" class="md-nav__link">
    <span class="md-ellipsis">
      • ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation. [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemblyhands-towards-egocentric-activity-understanding-via-3d-hand-pose-estimation-project" class="md-nav__link">
    <span class="md-ellipsis">
      • AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation. [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#high-fidelity-3d-hand-shape-reconstruction-via-scalable-graph-frequency-decomposition" class="md-nav__link">
    <span class="md-ellipsis">
      • High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handnerf-neural-radiance-fields-for-animatable-interacting-hands-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HandNeRF: Neural Radiance Fields for Animatable Interacting Hands. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#poem-reconstructing-hand-in-a-point-embedded-multi-view-stereo-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • POEM: Reconstructing Hand in a Point Embedded Multi-view Stereo. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#harp-personalized-hand-reconstruction-from-a-monocular-rgb-video-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • HARP: Personalized Hand Reconstruction from a Monocular RGB Video. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relightablehands-efficient-neural-relighting-of-articulated-hand-models-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • RelightableHands: Efficient Neural Relighting of Articulated Hand Models. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#h2onet-hand-occlusion-and-orientation-aware-network-for-real-time-3d-hand-mesh-reconstruction" class="md-nav__link">
    <span class="md-ellipsis">
      • H2ONet: Hand-Occlusion-and-Orientation-aware Network for Real-time 3D Hand Mesh Reconstruction.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#affordance-diffusion-synthesizing-hand-object-interactions-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Affordance Diffusion: Synthesizing Hand-Object Interactions. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gsdf-geometry-driven-signed-distance-functions-for-3d-hand-object-reconstruction-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#harmonious-feature-learning-for-interactive-hand-object-pose-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      • Harmonious Feature Learning for Interactive Hand-Object Pose Estimation.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handy-towards-a-high-fidelity-3d-hand-shape-and-appearance-model" class="md-nav__link">
    <span class="md-ellipsis">
      • Handy: Towards a high fidelity 3D hand shape and appearance model.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-avatar-free-pose-hand-animation-and-rendering-from-monocular-video-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-domain-3d-hand-pose-estimation-with-dual-modalities-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Cross-domain 3D Hand Pose Estimation with Dual Modalities. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overcoming-the-tradeoff-in-accuracy-and-plausibility-for-3d-hand-shape-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Overcoming the Tradeoff in Accuracy and Plausibility for 3D Hand Shape Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hierarchical-temporal-transformer-for-3d-hand-pose-estimation-and-action-recognition-from-egocentric-rgb-videos-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#im2hands-learning-attentive-implicit-representation-of-interacting-two-hand-shapes-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-voting-field-for-camera-space-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Neural Voting Field for Camera-Space 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bringing-inputs-to-shared-domains-for-3d-interacting-hands-recovery-in-the-wild-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recovering-3d-hand-mesh-sequence-from-a-single-blurry-image-a-new-dataset-and-temporal-unfolding-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semi-supervised-hand-appearance-recovery-via-structure-disentanglement-and-dual-adversarial-discrimination-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-based-unified-recognition-of-two-hands-manipulating-objects-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Transformer-based Unified Recognition of Two Hands Manipulating Objects. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#acr-attention-collaboration-based-regressor-for-arbitrary-two-hand-reconstruction-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2023 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2023 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#novel-view-synthesis-and-pose-estimation-for-hand-object-interaction-from-sparse-views-pdf-project-code-data" class="md-nav__link">
    <span class="md-ellipsis">
      • Novel-view Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views. [PDF] Project] [Code] [Data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contactgen-generative-contact-modeling-for-grasp-generation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • ContactGen: Generative Contact Modeling for Grasp Generation [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#diffusion-guided-reconstruction-of-everyday-hand-object-interaction-clips-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handr2n2-iterative-3d-hand-pose-estimation-using-a-residual-recurrent-neural-network-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HandR2N2: Iterative 3D Hand Pose Estimation Using a Residual Recurrent Neural Network. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hamuco-hand-pose-estimation-via-multiview-collaborative-self-supervised-learning-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deformer-dynamic-fusion-transformer-for-robust-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phrit-parametric-hand-representation-with-implicit-template-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • PHRIT: Parametric Hand Representation with Implicit Template. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chord-category-level-hand-held-object-reconstruction-via-shape-deformation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#uncertainty-aware-state-space-transformer-for-egocentric-3d-hand-trajectory-forecasting-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spectral-graphormer-spectral-graph-based-transformer-for-egocentric-two-hand-reconstruction-using-multi-view-color-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Spectral Graphormer: Spectral Graph-Based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reconstructing-interacting-hands-with-interaction-prior-from-monocular-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Reconstructing Interacting Hands with Interaction Prior from Monocular Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ochid-fi-occlusion-robust-hand-pose-estimation-in-3d-via-rf-vision-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dynamic-hyperbolic-attention-network-for-fine-hand-object-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoupled-iterative-refinement-framework-for-interacting-hands-reconstruction-from-a-single-rgb-image-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#affordpose-a-large-scale-dataset-of-hand-object-interactions-with-affordance-driven-hand-pose-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • AffordPose: A Large-Scale Dataset of Hand-Object Interactions with Affordance-Driven Hand Pose. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#egopca-a-new-framework-for-egocentric-hand-object-interaction-understanding-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#renderih-a-large-scale-synthetic-dataset-for-3d-interacting-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#source-free-domain-adaptive-human-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Source-free Domain Adaptive Human Pose Estimation [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#livehand-real-time-and-photorealistic-neural-hand-rendering-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • LiveHand: Real-time and Photorealistic Neural Hand Rendering [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-others" class="md-nav__link">
    <span class="md-ellipsis">
      2023 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2023 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2023-neurips-fourierhandflow-neural-4d-hand-representation-using-fourier-query-flow-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 NeurIPS] FourierHandFlow: Neural 4D Hand Representation Using Fourier Query Flow. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-iccvw-showme-benchmarking-object-agnostic-hand-object-3d-reconstruction-pdf-project-code-data" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 ICCVW] SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction. [PDF] [Project] [Code] [Data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-aaai-two-heads-are-better-than-one-image-point-cloud-network-for-depth-based-3d-hand-pose-estimation-pdf-aaai-23-distinguished-papers" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 AAAI] Two Heads are Better than One: Image-Point Cloud Network for Depth-Based 3D Hand Pose Estimation. [[PDF]] (AAAI-23 Distinguished Papers)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-aaai-tracking-and-reconstructing-hand-object-interactions-from-point-cloud-sequences-in-the-wild-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 AAAI] Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-wacv-thor-net-end-to-end-graformer-based-realistic-two-hands-and-object-reconstruction-with-self-supervision-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 WACV] THOR-Net: End-to-End Graformer-Based Realistic Two Hands and Object Reconstruction With Self-Supervision. [PDF] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2022 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2022 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#identity-aware-hand-mesh-estimation-and-personalization-from-rgb-images-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Identity-aware Hand Mesh Estimation and Personalization from RGB Images . [[PDF]] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alignsdf-pose-aligned-signed-distance-fields-for-hand-object-reconstruction-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#s2contact-graph-based-network-for-3d-hand-object-contact-estimation-with-semi-supervised-learning-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • S2Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#domain-adaptive-hand-keypoint-and-pixel-localization-in-the-wild-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-interacting-hand-pose-estimation-by-hand-de-occlusion-and-removal-pdf-projectdataset" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal. [PDF] [Project][Dataset]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-attention-of-disentangled-modalities-for-3d-human-mesh-recovery-with-transformers-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2022 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2022 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#whats-in-your-hands-3d-reconstruction-of-generic-objects-in-hands-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • What's in your hands? 3D Reconstruction of Generic Objects in Hands. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mining-multi-view-information-a-strong-self-supervised-framework-for-depth-based-3d-hand-pose-and-mesh-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Mining Multi-View Information: A Strong Self-Supervised Framework for Depth-based 3D Hand Pose and Mesh Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handoccnet-occlusion-robust-3d-hand-mesh-estimation-network-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#keypoint-transformer-solving-joint-identification-in-challenging-hands-and-object-interactions-for-accurate-3d-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#collaborative-learning-for-hand-and-object-reconstruction-with-attention-guided-graph-convolution-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spatial-temporal-parallel-transformer-for-arm-hand-dynamic-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Spatial-Temporal Parallel Transformer for Arm-Hand Dynamic Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-grasp-physically-plausible-dynamic-grasp-synthesis-for-hand-object-interactions-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goal-generating-4d-whole-body-motion-for-hand-object-grasping-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#oakink-a-large-scale-knowledge-repository-for-understanding-hand-object-interaction-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#artiboost-boosting-articulated-3d-hand-object-pose-estimation-via-online-exploration-and-synthesis-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interacting-attention-graph-for-single-image-two-hand-reconstruction-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Interacting Attention Graph for Single Image Two-Hand Reconstruction. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mobrecon-mobile-friendly-hand-mesh-reconstruction-from-monocular-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lisa-learning-implicit-shape-and-appearance-of-hands-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • LISA: Learning Implicit Shape and Appearance of Hands. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-others" class="md-nav__link">
    <span class="md-ellipsis">
      2022 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2022 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2022-aaai-efficient-virtual-view-selection-for-3d-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 AAAI] Efficient Virtual View Selection for 3D Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2021 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2021 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#toward-human-like-grasp-dexterous-grasping-via-semantic-representation-of-object-hand-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Toward Human-Like Grasp: Dexterous Grasping via Semantic Representation of Object-Hand. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-transfer-learning-for-hand-mesh-recovery-from-binocular-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Self-Supervised Transfer Learning for Hand Mesh Recovery From Binocular Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-3d-hand-pose-estimation-from-monocular-rgb-via-contrastive-learning-pdf-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. [PDF] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#towards-accurate-alignment-in-real-time-3d-hand-mesh-reconstruction-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eventhands-real-time-neural-3d-hand-reconstruction-from-an-event-stream-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • EventHands: Real-Time Neural 3D Hand Reconstruction from an Event Stream. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reconstructing-hand-object-interactions-in-the-wild-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Reconstructing Hand-Object Interactions in the Wild. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handfoldingnet-a-3d-hand-pose-estimation-network-using-multiscale-feature-guided-folding-of-a-2d-hand-skeleton-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#h2o-two-hands-manipulating-objects-for-first-person-interaction-recognition-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • H2O: Two Hands Manipulating Objects for First Person Interaction Recognition. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i2uv-handnet-image-to-uv-prediction-network-for-accurate-and-high-fidelity-3d-hand-mesh-modeling-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • I2UV-HandNet: Image-to-UV Prediction Network for Accurate and High-fidelity 3D Hand Mesh Modeling. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semihand-semi-supervised-hand-pose-estimation-with-consistency-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • SemiHand: Semi-supervised Hand Pose Estimation with Consistency. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#end-to-end-detection-and-pose-estimation-of-two-interacting-hands-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • End-to-End Detection and Pose Estimation of Two Interacting Hands. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-object-contact-consistency-reasoning-for-human-grasps-generation-pdf-project-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand-Object Contact Consistency Reasoning for Human Grasps Generation. [PDF] [Project] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-image-understanding-via-deep-multi-task-learning-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Image Understanding via Deep Multi-Task Learning. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpf-learning-a-contact-potential-field-to-model-the-hand-object-interaction-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • CPF: Learning a Contact Potential Field to Model the Hand-object Interaction. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#travelnet-self-supervised-physically-plausible-hand-motion-learning-from-monocular-color-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • TravelNet: Self-supervised Physically Plausible Hand Motion Learning from Monocular Color Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interacting-two-hand-3d-pose-and-shape-reconstruction-from-single-color-image-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Interacting Two-Hand 3D Pose and Shape Reconstruction from Single Color Image. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#removing-the-bias-of-integral-pose-regression-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Removing the Bias of Integral Pose Regression. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2021 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2021 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monocular-real-time-full-body-capture-with-inter-part-correlations-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Monocular Real-time Full Body Capture with Inter-part Correlations. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#end-to-end-human-pose-and-mesh-reconstruction-with-transformers-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • End-to-End Human Pose and Mesh Reconstruction with Transformers. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dexycb-a-benchmark-for-capturing-hand-grasping-of-objects-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • DexYCB: A Benchmark for Capturing Hand Grasping of Objects. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#body2hands-learning-to-infer-3d-hands-from-conversational-gesture-body-dynamics-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-space-hand-mesh-recovery-via-semantic-aggregation-and-adaptive-2d-1d-registration-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-based-3d-hand-reconstruction-via-self-supervised-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Model-based 3D Hand Reconstruction via Self-Supervised Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semi-supervised-3d-hand-object-poses-estimation-with-interactions-in-time-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-others" class="md-nav__link">
    <span class="md-ellipsis">
      2021 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2021 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2021-3dv-a-skeleton-driven-neural-occupancy-representation-for-articulated-hands-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 3DV] A Skeleton-Driven Neural Occupancy Representation for Articulated Hands. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-3dv-learning-to-disambiguate-strongly-interacting-hands-via-probabilistic-per-pixel-part-segmentation-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 3DV] Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-dicta-semi-supervised-3d-hand-shape-and-pose-estimation-with-label-propagation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 DICTA] Semi-Supervised 3D Hand Shape and Pose Estimation with Label Propagation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-bmvc-joint-aware-regression-rethinking-regression-based-method-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 BMVC] Joint-Aware Regression: Rethinking Regression-Based Method for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-bmvc-local-and-global-point-cloud-reconstruction-for-3d-hand-pose-estimation-pdf-data" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 BMVC] Local and Global Point Cloud Reconstruction for 3D Hand Pose Estimation. [PDF] [Data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-bmvc-handtailor-towards-high-precision-monocular-3d-hand-recovery-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 BMVC] HandTailor: Towards High-Precision Monocular 3D Hand Recovery. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-bmvc-multi-view-image-based-hand-geometry-refinement-using-differentiable-monte-carlo-ray-tracing-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 BMVC] Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-siggraph-manipnet-neural-manipulation-synthesis-with-a-hand-object-spatial-representation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 SIGGRAPH] ManipNet: Neural Manipulation Synthesis with a Hand-Object Spatial Representation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-siggraph-single-depth-view-based-real-time-reconstruction-of-hand-object-interactions-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 SIGGRAPH] Single Depth View-Based Real-time Reconstruction of Hand-object Interactions. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-iros-dynamic-modeling-of-hand-object-interactions-via-tactile-sensing-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 IROS] Dynamic Modeling of Hand-Object Interactions via Tactile Sensing. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-aaai-exploiting-learnable-joint-groups-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 AAAI] Exploiting Learnable Joint Groups for Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-wacv-active-learning-for-bayesian-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 WACV] Active Learning for Bayesian 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-wacv-two-hand-global-3d-pose-estimation-using-monocular-rgb-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 WACV] Two-hand Global 3D Pose Estimation Using Monocular RGB. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-wacv-mvhm-a-large-scale-multi-view-hand-mesh-benchmark-for-accurate-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 WACV] MVHM: A Large-Scale Multi-View Hand Mesh Benchmark for Accurate 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-wacv-temporal-aware-self-supervised-learning-for-3d-hand-pose-and-mesh-estimation-in-videos-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 WACV] Temporal-Aware Self-Supervised Learning for 3D Hand Pose and Mesh Estimation in Videos. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-petra-weakly-supervised-hand-part-segmentation-from-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 PETRA] Weakly-supervised hand part segmentation from depth images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-petra-a-pipeline-for-hand-2-d-keypoint-localization-using-unpaired-image-to-image-translation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 PETRA] A Pipeline for Hand 2-D Keypoint Localization Using Unpaired Image to Image Translation. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2020 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2020 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grab-a-dataset-of-whole-body-human-grasping-of-objects-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • GRAB: A Dataset of Whole-Body Human Grasping of Objects. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monocular-expressive-body-regression-through-body-driven-attention-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Monocular Expressive Body Regression through Body-Driven Attention. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-phong-surface-efficient-3d-model-fitting-using-lifted-optimization-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • The Phong Surface: Efficient 3D Model Fitting using Lifted Optimization. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#whole-body-human-pose-estimation-in-the-wild-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Whole-Body Human Pose Estimation in the Wild. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dual-grid-net-hand-mesh-vertex-regression-from-single-depth-maps-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Dual Grid Net: hand mesh vertex regression from single depth maps. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-transformer-non-autoregressive-structured-modeling-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand-Transformer: Non-Autoregressive Structured Modeling for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contactpose-a-dataset-of-grasps-with-object-contact-and-hand-pose-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • ContactPose: A Dataset of Grasps with Object Contact and Hand Pose. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seqhand-rgb-sequence-based-3d-hand-pose-and-shape-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • SeqHAND: RGB-Sequence-Based 3D Hand Pose and Shape Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#html-a-parametric-hand-texture-model-for-3d-hand-reconstruction-and-personalizationm-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • HTML: A Parametric Hand Texture Model for 3D Hand Reconstruction and Personalizationm. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jgr-p2o-joint-graph-reasoning-based-pixel-to-offset-prediction-network-for-3d-hand-pose-estimation-from-a-single-depth-image-pdf-code-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • JGR-P2O: Joint Graph Reasoning based Pixel-to-Offset Prediction Network for 3D Hand Pose Estimation from a Single Depth Image. [PDF] [Code] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptive-computationally-efficient-network-for-monocular-3d-hand-pose-estimation-pdf-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Adaptive Computationally Efficient Network for Monocular 3D Hand Pose Estimation. [PDF] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#collaborative-learning-of-gesture-recognition-and-3d-hand-pose-estimation-with-multi-order-feature-analysis-pdf-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-Order Feature Analysis. [PDF] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deephandmesh-weakly-supervised-deep-encoder-decoder-framework-for-high-fidelity-hand-mesh-modeling-from-a-single-rgb-image-pdf-project-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • DeepHandMesh: Weakly-supervised Deep Encoder-Decoder Framework for High-fidelity Hand Mesh Modeling from a Single RGB Image. [PDF] [Project] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interhand26m-a-new-large-scale-dataset-and-baseline-for-3d-single-and-interacting-hand-pose-estimation-from-a-single-rgb-image-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • InterHand2.6M: A New Large-scale Dataset and Baseline for 3D Single and Interacting Hand Pose Estimation from a Single RGB Image. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i2l-meshnet-image-to-lixel-prediction-network-for-accurate-3d-human-pose-and-mesh-estimation-from-a-single-rgb-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-3d-hand-pose-estimation-via-biomechanical-constraints-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-Supervised 3D Hand Pose Estimation via Biomechanical Constraints. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#measuring-generalisation-to-unseen-viewpoints-articulations-shapes-and-objects-for-3d-hand-pose-estimation-under-hand-object-interaction-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Measuring Generalisation to Unseen Viewpoints, Articulations, Shapes and Objects for 3D Hand Pose Estimation under Hand-Object Interaction. [PDF] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2020 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2020 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-mesh-convolutional-hand-reconstruction-in-the-wild-pdf-project-oral-paper-award-nominees" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild. [PDF] [Project] (Oral) (Paper Award Nominees)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-domain-adaptation-via-gan-and-mesh-model-for-estimating-3d-hand-poses-interacting-objects-pdf-code-oral-paper-award-nominees" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects. [PDF] [Code] (Oral) (Paper Award Nominees)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ganhand-predicting-human-grasp-affordances-in-multi-object-scenes-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modal-variational-alignment-of-latent-spaces-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Cross-Modal Variational Alignment of Latent Spaces. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#humbi-a-large-multiview-dataset-of-human-body-expressions-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • HUMBI: A Large Multiview Dataset of Human Body Expressions. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#epipolar-transformers-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Epipolar Transformers. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handvoxnet-deep-voxel-based-network-for-3d-hand-shape-and-pose-estimation-from-a-single-depth-map-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HandVoxNet: Deep Voxel-Based Network for 3D Hand Shape and Pose Estimation from a Single Depth Map. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#knowledge-as-priors-cross-modal-knowledge-generalization-for-datasets-without-superior-knowledge-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leveraging-photometric-consistency-over-time-for-sparsely-supervised-hand-object-reconstruction-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monocular-real-time-hand-shape-and-motion-capture-using-multi-modal-data-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hope-net-a-graph-based-model-for-hand-object-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#honnotate-a-method-for-3d-annotation-of-hand-and-objects-poses-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HOnnotate: A method for 3D Annotation of Hand and Objects Poses. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-others" class="md-nav__link">
    <span class="md-ellipsis">
      2020 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2020 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2020-icra-robust-occlusion-aware-pose-estimation-for-objects-grasped-by-adaptive-hands-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ICRA] Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands . [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-cvprw-mediapipe-hands-on-device-real-time-hand-tracking-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 CVPRW] MediaPipe Hands: On-device Real-time Hand Tracking. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-ismar-3d-hand-pose-estimation-with-a-single-infrared-camera-via-domain-transfer-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ISMAR] 3D Hand Pose Estimation with a Single Infrared Camera via Domain Transfer Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-ismar-bare-hand-depth-inpainting-for-3d-tracking-of-hand-interacting-with-object-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ISMAR] Bare-hand Depth Inpainting for 3D Tracking of Hand Interacting with Object. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-3dv-grasping-field-learning-implicit-representations-for-human-grasps-pdf-code-best-paper-award" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 3DV] Grasping Field: Learning Implicit Representations for Human Grasps. [PDF] [Code] (Best Paper Award)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-uist-deepfisheye-near-surface-multi-finger-tracking-technology-using-fisheye-camera-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 UIST] DeepFisheye: Near-Surface Multi-Finger Tracking Technology Using Fisheye Camera. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-siggraph-asia-constraining-dense-hand-surface-tracking-with-elasticity-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 SIGGRAPH Asia] Constraining Dense Hand Surface Tracking With Elasticity. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-siggraph-asia-rgb2hands-real-time-tracking-of-3d-hand-interactions-from-monocular-rgb-video-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 SIGGRAPH Asia] RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB Video. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-siggraph-megatrack-monochrome-egocentric-articulated-hand-tracking-for-virtual-reality-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 SIGGRAPH] MEgATrack: Monochrome Egocentric Articulated Hand-Tracking for Virtual Reality. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-mm-mm-hand-3d-aware-multi-modal-guided-hand-generation-for-3d-hand-pose-synthesis-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 MM] MM-Hand: 3D-Aware Multi-Modal Guided Hand Generation for 3D Hand Pose Synthesis. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-mm-adaptive-wasserstein-hourglass-for-weakly-supervised-hand-pose-estimation-from-monocular-rgb-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 MM] Adaptive Wasserstein Hourglass for Weakly Supervised Hand Pose Estimation from Monocular RGB. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-mm-hot-net-non-autoregressive-transformer-for-3d-hand-object-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 MM] HOT-Net: Non-Autoregressive Transformer for 3D Hand-Object Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-bmvc-pmd-net-privileged-modality-distillation-network-for-3d-hand-pose-estimation-from-a-single-rgb-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 BMVC] PMD-Net: Privileged Modality Distillation Network for 3D Hand Pose Estimation from a Single RGB Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-bmvc-sia-gcn-a-spatial-information-aware-graph-neural-network-with-2d-convolutions-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 BMVC] SIA-GCN: A Spatial Information Aware Graph Neural Network with 2D Convolutions for Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-bmvc-explicit-knowledge-distillation-for-3d-hand-pose-estimation-from-monocular-rgb-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 BMVC] Explicit Knowledge Distillation for 3D Hand Pose Estimation from Monocular RGB. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-bmvc-bihand-recovering-hand-mesh-with-multi-stage-bisected-hourglass-networks-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 BMVC] BiHand: Recovering Hand Mesh with Multi-stage Bisected Hourglass Networks. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-ubicomp-fingertrak-continuous-3d-hand-pose-tracking-by-deep-learning-hand-silhouettes-captured-by-miniature-thermal-cameras-on-wrist-pdf-eccv-2020-demo-award-nominee" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 Ubicomp] FingerTrak: Continuous 3D Hand Pose Tracking by Deep Learning Hand Silhouettes Captured by Miniature Thermal Cameras on Wrist. [PDF] (ECCV 2020 Demo Award Nominee)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-fg-hand-tracking-from-monocular-rgb-with-dense-semantic-labels-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 FG] Hand tracking from monocular RGB with dense semantic labels. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-fg-generative-model-based-loss-to-the-rescue-a-method-to-overcome-annotation-errors-for-depth-based-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 FG] Generative Model-Based Loss to the Rescue: A Method to Overcome Annotation Errors for Depth-Based Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-iros-physics-based-dexterous-manipulations-with-estimated-hand-poses-and-residual-reinforcement-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 IROS] Physics-Based Dexterous Manipulations with Estimated Hand Poses and Residual Reinforcement Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-chi-evaluation-of-machine-learning-techniques-for-hand-pose-estimation-on-handheld-device-with-proximity-sensor-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 CHI] Evaluation of Machine Learning Techniques for Hand Pose Estimation on Handheld Device with Proximity Sensor. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-aaai-awr-adaptive-weighting-regression-for-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 AAAI] AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-wacv-nonparametric-structure-regularization-machine-for-2d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 WACV] Nonparametric Structure Regularization Machine for 2D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-wacv-3d-hand-pose-estimation-with-disentangled-cross-modal-latent-space-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 WACV] 3D Hand Pose Estimation with Disentangled Cross-Modal Latent Space. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-wacv-dggan-depth-image-guided-generative-adversarial-networks-for-disentangling-rgb-and-depth-images-in-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 WACV] DGGAN: Depth-image Guided Generative Adversarial Networks for Disentangling RGB and Depth Images in 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-wacv-rotation-invariant-mixed-graphical-model-network-for-2d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 WACV] Rotation-invariant Mixed Graphical Model Network for 2D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-icassp-weakly-supervised-segmentation-guided-hand-pose-estimation-during-interaction-with-unknown-objects-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ICASSP] Weakly Supervised Segmentation Guided Hand Pose Estimation During Interaction With Unknown Objects. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-icassp-hand-3d-studio-a-new-multi-view-system-for-3d-hand-reconstruction-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ICASSP] Hand-3D-Studio: A New Multi-view System for 3D Hand Reconstruction. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2019 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2019 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#freihand-a-dataset-for-markerless-capture-of-hand-pose-and-shape-from-single-rgb-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from Single RGB Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a2j-anchor-to-joint-regression-network-for-3d-articulated-pose-estimation-from-a-single-depth-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exploiting-spatial-temporal-relationships-for-3d-pose-estimation-via-graph-convolutional-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Exploiting Spatial-temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resolving-3d-human-pose-ambiguities-with-3d-scene-constraints-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Resolving 3D Human Pose Ambiguities with 3D Scene Constraints. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#so-handnet-self-organizing-network-for-3d-hand-pose-estimation-with-semi-supervised-learning-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#end-to-end-hand-mesh-recovery-from-a-monocular-rgb-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • End-to-end Hand Mesh Recovery from a Monocular RGB Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aligning-latent-spaces-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Aligning Latent Spaces for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands19-workshop-disentangling-pose-from-appearance-in-monochrome-hand-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [HANDS19 Workshop] Disentangling Pose from Appearance in Monochrome Hand Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands19-workshop-rgb-based-3d-hand-pose-estimation-via-privileged-learning-with-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [HANDS19 Workshop] RGB-based 3D Hand Pose Estimation via Privileged Learning with Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands19-workshop-explicit-pose-deformation-learning-for-tracking-human-poses-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [HANDS19 Workshop] Explicit Pose Deformation Learning for Tracking Human Poses. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands19-workshop-hand-pose-ensemble-learning-based-on-grouping-features-of-hand-point-sets-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [HANDS19 Workshop] Hand Pose Ensemble Learning based on Grouping Features of Hand Point Sets. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2019 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2019 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#disentangling-latent-hands-for-image-synthesis-and-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Disentangling Latent Hands for Image Synthesis and Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#point-to-pose-voting-based-hand-pose-estimation-using-residual-permutation-equivariant-layer-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation Equivariant Layer. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ho-unified-egocentric-recognition-of-3d-hand-object-poses-and-interactions-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • H•O: Unified Egocentric Recognition of 3D Hand-Object Poses and Interactions. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-3d-hand-pose-estimation-pdf-code-oral-best-paper-finalists" class="md-nav__link">
    <span class="md-ellipsis">
      • Self supervised 3D hand pose estimation. [PDF] [Code] (Oral) (Best Paper Finalists)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crossinfonet-multi-task-information-sharing-based-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • CrossInfoNet: Multi-Task Information Sharing Based Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#expressive-body-capture-3d-hands-face-and-body-from-a-single-image-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-joint-reconstruction-of-hands-and-manipulated-objects-pdf-code-code-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning joint reconstruction of hands and manipulated objects. [PDF] [Code] [Code] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-shape-and-pose-estimation-from-a-single-rgb-image-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Shape and Pose Estimation from a Single RGB Image. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-shape-and-pose-from-images-in-the-wild-pdf-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Shape and Pose from Images in the Wild. [PDF] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pushing-the-envelope-for-rgb-based-dense-3d-hand-pose-estimation-via-neural-rendering-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Pushing the Envelope for RGB-based Dense 3D Hand Pose Estimation via Neural Rendering. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monocular-total-capture-posing-face-body-and-hands-in-the-wild-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Monocular Total Capture: Posing Face, Body, and Hands in the Wild. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-others" class="md-nav__link">
    <span class="md-ellipsis">
      2019 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2019 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2019-siggraph-interactive-hand-pose-estimation-using-a-stretch-sensing-soft-glove-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 SIGGRAPH] Interactive Hand Pose Estimation using a Stretch-Sensing Soft Glove. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-siggraph-interactionfusion-real-time-reconstruction-of-hand-poses-and-deformable-objects-in-hand-object-interactions-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 SIGGRAPH] InteractionFusion: Real-time Reconstruction of Hand Poses and Deformable Objects in Hand-object Interactions. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-siggraph-real-time-pose-and-shape-reconstruction-of-two-interacting-hands-with-a-single-depth-camera-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 SIGGRAPH] Real-time pose and shape reconstruction of two interacting hands with a single depth camera. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-fg-deep-conditional-variational-estimation-for-depth-based-hand-poses-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 FG] Deep Conditional Variational Estimation for Depth-Based Hand Poses. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-unified-2d-and-3d-hand-pose-estimation-from-a-single-visible-or-x-ray-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] Unified 2D and 3D Hand Pose Estimation from a Single Visible or X-ray Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-tagan-tonality-aligned-generative-adversarial-networks-for-realistic-handpose-synthesis-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] TAGAN: Tonality Aligned Generative Adversarial Networks for Realistic HandPose Synthesis. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-single-image-3d-hand-reconstruction-with-mesh-convolutions-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] Single Image 3D Hand Reconstruction with Mesh Convolutions. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-adaptive-graphical-model-network-for-2d-handpose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] Adaptive Graphical Model Network for 2D Handpose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-srn-stacked-regression-network-for-real-time-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] SRN: Stacked Regression Network for Real-time 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-end-to-end-3d-hand-pose-estimation-from-stereo-cameras-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] End-to-End 3D Hand Pose Estimation from Stereo Cameras. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-accv-hand-pose-estimation-based-on-3d-residual-network-with-data-padding-and-skeleton-steadying-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 ACCV] Hand Pose Estimation Based on 3D Residual Network with Data Padding and Skeleton Steadying. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-icassp-cascaded-point-network-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 ICASSP] Cascaded Point Network for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-icassp-a-novel-framework-of-hand-localization-and-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 ICASSP] A Novel Framework of Hand Localization and Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-icra-vision-based-teleoperation-of-shadow-dexterous-hand-using-end-to-end-deep-neural-network-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 ICRA] Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-wacv-murauer-mapping-unlabeled-real-data-for-label-austerity-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 WACV] MURAUER: Mapping Unlabeled Real Data for Label AUstERity. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2018 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2018 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#handmap-robust-hand-pose-estimation-via-intermediate-dense-guidance-map-supervision-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HandMap: Robust Hand Pose Estimation via Intermediate Dense Guidance Map Supervision. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hbe-hand-branch-ensemble-network-for-real-time-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HBE: Hand Branch Ensemble network for real time 3D hand pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#point-to-point-regression-pointnet-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Point-to-Point Regression PointNet for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-3d-hand-pose-estimation-from-monocular-rgb-images-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-supervised 3D Hand Pose Estimation from Monocular RGB Images. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#joint-3d-tracking-of-a-deformable-object-in-interaction-with-a-hand-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Joint 3D tracking of a deformable object in interaction with a hand. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#occlusion-aware-hand-pose-estimation-using-hierarchical-mixture-density-network-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density Network. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-pose-estimation-via-latent-25d-heatmap-regression-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Pose Estimation via Latent 2.5D Heatmap Regression. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands18-workshop-adapting-egocentric-visual-hand-pose-estimation-towards-a-robot-controlled-exoskeleton-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands18 Workshop] Adapting Egocentric Visual Hand Pose Estimation Towards a Robot-Controlled Exoskeleton. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands18-workshop-estimating-2d-multi-hand-poses-from-single-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands18 Workshop] Estimating 2D Multi-Hand Poses From Single Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands18-workshop-task-oriented-hand-motion-retargeting-for-dexterous-manipulation-imitation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands18 Workshop] Task-Oriented Hand Motion Retargeting for Dexterous Manipulation Imitation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2018 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2018 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#first-person-hand-action-benchmark-with-rgb-d-videos-and-3d-hand-pose-annotations-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-pose-specific-representations-by-predicting-different-views-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning Pose Specific Representations by Predicting Different Views. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-pointnet-3d-hand-pose-estimation-using-point-sets-pdf-project-code-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand PointNet: 3D Hand Pose Estimation using Point Sets. [PDF] [Project] [Code] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dense-3d-regression-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Dense 3D Regression for Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modal-deep-variational-hand-pose-estimation-pdf-project-code-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Cross-modal Deep Variational Hand Pose Estimation. [PDF] [Project] [Code] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature-mapping-for-learning-fast-and-accurate-3d-pose-inference-from-synthetic-images-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ganerated-hands-for-real-time-3d-hand-tracking-from-monocular-rgb-pdf-supp-project-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB. [PDF] [Supp] [Project] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#v2v-posenet-voxel-to-voxel-prediction-network-for-accurate-3d-hand-and-human-pose-estimation-from-a-single-depth-map-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#depth-based-3d-hand-pose-estimation-from-current-achievements-to-future-goals-pdf-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future Goals. [PDF] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#augmented-skeleton-space-transfer-for-depth-based-hand-pose-estimation-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Augmented skeleton space transfer for depth-based hand pose estimation. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-humans-workshop-monocular-rgb-hand-pose-inference-from-unsupervised-refinable-nets-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [3D HUMANS Workshop] Monocular RGB Hand Pose Inference From Unsupervised Refinable Nets. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-others" class="md-nav__link">
    <span class="md-ellipsis">
      2018 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2018 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2018-ismar-hybrid-3d-hand-articulations-tracking-guided-by-classification-and-search-space-adaptation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ISMAR] Hybrid 3D Hand Articulations Tracking Guided by Classification and Search Space Adaptation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-accv-hand-pose-estimation-based-on-3d-residual-network-with-data-padding-and-skeleton-steadying-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ACCV] Hand Pose Estimation based on 3D Residual Network with Data Padding and Skeleton Steadying. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-accv-partially-occluded-hands-a-challenging-new-dataset-for-single-image-hand-pose-estimation-pdf-project-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ACCV] Partially Occluded Hands: A challenging new dataset for single-image hand pose estimation. [PDF] [Project] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-accv-domain-transfer-for-3d-pose-estimation-from-color-images-without-manual-annotations-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ACCV] Domain Transfer for 3D Pose Estimation from Color Images without Manual Annotations. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-pcm-hand-pose-estimation-with-attention-and-sequence-network-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 PCM] Hand Pose Estimation with Attention-and-Sequence Network. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-pcm-mutiple-transfer-net-with-region-ensemble-for-deep-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 PCM] Mutiple Transfer Net with Region Ensemble for Deep Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icpr-local-regression-based-hourglass-network-for-hand-pose-estimation-from-a-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICPR] Local Regression Based Hourglass Network for Hand Pose Estimation from a Single Depth Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icpr-dynamic-projected-segmentation-networks-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICPR] Dynamic Projected Segmentation Networks for Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-3dv-deephps-end-to-end-estimation-of-3d-hand-pose-and-shape-by-learning-from-synthetic-depth-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 3DV] DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning from Synthetic Depth. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icip-networks-effectively-utilizing-2d-spatial-information-for-accurate-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICIP] Networks Effectively Utilizing 2D Spatial Information for Accurate 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icip-on-the-fusion-of-rgb-and-depth-information-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICIP] On the Fusion of RGB and Depth Information For Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icip-fast-lifting-for-3d-hand-pose-estimation-in-arvr-applications-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICIP] Fast Lifting for 3D Hand Pose Estimation in AR/VR Applications. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-bmvc-structure-aware-3d-hourglass-network-for-hand-pose-estimation-from-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 BMVC] Structure-Aware 3D Hourglass Network for Hand Pose Estimation from Single Depth Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-bmvc-3d-hand-pose-estimation-using-simulation-and-partial-supervision-with-a-shared-latent-space-pdf-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 BMVC] 3D Hand Pose Estimation using Simulation and Partial-Supervision with a Shared Latent Space. [PDF] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-fg-kinematic-constrained-cascaded-autoencoder-for-real-time-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 FG] Kinematic Constrained Cascaded Autoencoder for Real-time Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-wacv-using-a-single-rgb-frame-for-real-time-3d-hand-pose-estimation-in-the-wild-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 WACV] Using a single RGB frame for real time 3D hand pose estimation in the wild. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2017 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2017 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning-to-estimate-3d-hand-pose-from-single-rgb-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning to Estimate 3D Hand Pose from Single RGB Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-time-hand-tracking-under-occlusion-from-an-egocentric-rgb-d-sensor-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#robust-hand-pose-estimation-during-the-interaction-with-an-unknown-object-pdf-supp-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Robust Hand Pose Estimation during the Interaction with an Unknown Object. [PDF] [Supp] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-hand-articulations-by-hallucinating-heat-distribution-pdf-supp-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning Hand Articulations by Hallucinating Heat Distribution. [PDF] [Supp] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#low-dimensionality-calibration-through-local-anisotropic-scaling-for-robust-hand-model-personalization-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Low-Dimensionality Calibration through Local Anisotropic Scaling for Robust Hand Model Personalization. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands17-workshop-back-to-rgb-3d-tracking-of-hands-and-hand-object-interactions-based-on-short-baseline-stereo-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands17 Workshop] Back to RGB: 3D tracking of hands and hand-object interactions based on short-baseline stereo. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands17-workshop-deepprior-improving-fast-and-accurate-3d-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands17 Workshop] DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands17-workshop-hand-pose-estimation-using-deep-stereovision-and-markov-chain-monte-carlo-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands17 Workshop] Hand Pose Estimation Using Deep Stereovision and Markov-chain Monte Carlo. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2017 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2017 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hand-keypoint-detection-in-single-images-using-multiview-bootstrapping-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Keypoint Detection in Single Images using Multiview Bootstrapping. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crossing-nets-combining-gans-and-vaes-with-a-shared-latent-space-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Crossing Nets: Combining GANs and VAEs with a Shared Latent Space for Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-hand-22m-benchmark-hand-pose-data-set-and-state-of-the-art-analysis-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Big Hand 2.2M Benchmark: Hand Pose Data Set and State of the Art Analysis. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-convolutional-neural-networks-for-efficient-and-robust-hand-pose-estimation-from-single-depth-images-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation from Single Depth Images. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-others" class="md-nav__link">
    <span class="md-ellipsis">
      2017 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2017 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2017-3dv-simultaneous-hand-pose-and-skeleton-bone-lengths-estimation-from-a-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 3DV] Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a Single Depth Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-3dv-how-to-refine-3d-hand-pose-estimation-from-unlabelled-depth-data-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 3DV] How to Refine 3D Hand Pose Estimation from Unlabelled Depth Data? [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-icip-region-ensemble-network-improving-convolutional-network-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 ICIP] Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-icip-a-hand-pose-tracking-benchmark-from-stereo-matching-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 ICIP] A Hand Pose Tracking Benchmark from Stereo Matching. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-siggraph-asia-articulated-distance-fields-for-ultra-fast-tracking-of-hands-interacting-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 SIGGRAPH Asia] Articulated distance fields for ultra-fast tracking of hands interacting. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-siggraph-asia-online-generative-model-personalization-for-hand-tracking-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 SIGGRAPH Asia] Online Generative Model Personalization for Hand Tracking. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-siggraph-asia-embodied-hands-modeling-and-capturing-hands-and-bodies-together-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 SIGGRAPH Asia] Embodied Hands: Modeling and Capturing Hands and Bodies Together. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-bmvc-hand-pose-learning-combining-deep-learning-and-hierarchical-refinement-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 BMVC] Hand Pose Learning: Combining Deep Learning and Hierarchical Refinement for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-bmvc-generative-3d-hand-tracking-with-spatially-constrained-pose-sampling-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 BMVC] Generative 3D Hand Tracking with Spatially Constrained Pose Sampling. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-icra-learning-a-deep-network-with-spherical-part-model-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 ICRA] Learning a deep network with spherical part model for 3D hand pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-fg-occlusion-aware-hand-pose-recovery-from-sequences-of-depth-images-pdf-slide" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 FG] Occlusion aware hand pose recovery from sequences of depth images. [PDF] [Slide]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-fg-3d-hand-object-pose-estimation-from-depth-with-convolutional-neural-networks-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 FG] 3D Hand-Object Pose Estimation from Depth with Convolutional Neural Networks. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2016 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2016 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#spatial-attention-deep-net-with-partial-pso-for-hierarchical-hybrid-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-pose-estimation-from-local-surface-normals-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Pose Estimation from Local Surface Normals. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-time-joint-tracking-of-a-hand-manipulating-an-object-from-rgb-d-input-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Real-time Joint Tracking of a Hand Manipulating an Object from RGB-D Input. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2016 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2016 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#robust-3d-hand-pose-estimation-in-single-depth-images-from-single-view-cnn-to-multi-view-cnns-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Robust 3D Hand Pose Estimation in Single Depth Images: From Single-View CNN to Multi-View CNNs. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deephand-robust-hand-pose-estimation-by-completing-a-matrix-imputed-with-deep-features-pdfproject" class="md-nav__link">
    <span class="md-ellipsis">
      • DeepHand: Robust Hand Pose Estimation by Completing a Matrix Imputed With Deep Features. [PDF][Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#efficiently-creating-3d-training-data-for-fine-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Efficiently Creating 3D Training Data for Fine Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fits-like-a-glove-rapid-and-reliable-hand-shape-personalization-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Fits Like a Glove: Rapid and Reliable Hand Shape Personalization. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-others" class="md-nav__link">
    <span class="md-ellipsis">
      2016 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2016 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2016-nips-disco-nets-dissimilarity-coefficient-networks-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 NIPS] DISCO Nets : Dissimilarity Coefficient Networks. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-accv-hand-pose-regression-via-a-classification-guided-approach-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 ACCV] Hand Pose Regression via A Classification-guided Approach. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-icpr-deep-learning-for-integrated-hand-detection-and-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 ICPR] Deep learning for integrated hand detection and pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-icpr-depth-based-3d-hand-pose-tracking-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 ICPR] Depth-based 3D hand pose tracking. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-ijcai-model-based-deep-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 IJCAI] Model-based Deep Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-siggraph-efficient-and-precise-interactive-hand-tracking-through-joint-continuous-optimization-of-pose-and-correspondences-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 SIGGRAPH] Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-siggraph-asia-sphere-meshes-for-real-time-hand-modeling-and-tracking-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 SIGGRAPH Asia] Sphere-Meshes for Real-Time Hand Modeling and Tracking. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2015 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2015 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-a-feedback-loop-for-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Training a Feedback Loop for Hand Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opening-the-black-box-hierarchical-sampling-optimization-for-estimating-human-hand-pose-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#depth-based-hand-pose-estimation-data-methods-and-challenges-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Depth-based hand pose estimation: data, methods, and challenges. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-pose-estimation-using-randomized-decision-forest-with-segmentation-index-points-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Pose Estimation Using Randomized Decision Forest with Segmentation Index Points. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-collaborative-filtering-approach-to-real-time-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • A collaborative filtering approach to real-time hand pose estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lending-a-hand-detecting-hands-and-recognizing-activities-in-complex-egocentric-interactions-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#understanding-everyday-hands-in-action-from-rgb-d-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Understanding Everyday Hands in Action from RGB-D Images. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2015 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2015 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cascaded-hand-pose-regression-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Cascaded Hand Pose Regression. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fast-and-robust-hand-tracking-using-detection-guided-optimization-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Fast and Robust Hand Tracking Using Detection-Guided Optimization. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-an-efficient-model-of-hand-shape-variation-from-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning an Efficient Model of Hand Shape Variation from Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-others" class="md-nav__link">
    <span class="md-ellipsis">
      2015 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2015 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2015-bmvc-hybrid-one-shot-3d-hand-pose-estimation-by-exploiting-uncertainties-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 BMVC] Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-bmvc-rule-of-thumb-deep-derotation-for-improved-fingertip-detection-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 BMVC] Rule of Thumb: Deep Derotation for Improved Fingertip Detection. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-chi-accurate-robust-and-flexible-real-time-hand-tracking-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 CHI] Accurate, Robust, and Flexible Real-time Hand Tracking. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-cvww-hands-deep-in-deep-learning-for-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 CVWW] Hands Deep in Deep Learning for Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-fg-combining-discriminative-and-model-based-approaches-for-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 FG] Combining Discriminative and Model Based Approaches for Hand Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-sgp-robust-articulated-icp-for-real-time-hand-tracking-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 SGP] Robust Articulated-ICP for Real-Time Hand Tracking. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2014-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2014 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2014 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#realtime-and-robust-hand-tracking-from-depth-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Realtime and robust hand tracking from depth. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-regression-forest-structured-estimation-of-3d-articulated-hand-posture-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Latent regression forest: Structured estimation of 3d articulated hand posture. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#user-specific-hand-modeling-from-monocular-depth-sequences-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • User-specific hand modeling from monocular depth sequences. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evolutionary-quasi-random-search-for-hand-articulations-tracking-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Evolutionary Quasi-random Search for Hand Articulations Tracking. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2014-others-before" class="md-nav__link">
    <span class="md-ellipsis">
      2014 Others &amp; Before
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2014 Others & Before">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2014-siggraph-real-time-continuous-pose-recovery-of-human-hands-using-convolutional-networks-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2014 SIGGRAPH] Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2013-iccv-real-time-articulated-hand-pose-estimation-using-semi-supervised-transductive-regression-forests-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2013 ICCV] Real-time Articulated Hand Pose Estimation using Semi-supervised Transductive Regression Forests. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2013-iccv-interactive-markerless-articulated-hand-motion-tracking-using-rgb-and-depth-data-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2013 ICCV] Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2013-iccv-efficient-hand-pose-estimation-from-a-single-depth-image-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2013 ICCV] Efficient Hand Pose Estimation from a Single Depth Image. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2012-eccv-motion-capture-of-hands-in-action-using-discriminative-salient-points-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2012 ECCV] Motion Capture of Hands in Action using Discriminative Salient Points. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2012-eccv-hand-pose-estimation-and-hand-shape-classification-using-multi-layered-randomized-decision-forests" class="md-nav__link">
    <span class="md-ellipsis">
      • [2012 ECCV] Hand pose estimation and hand shape classification using multi-layered randomized decision forests.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2011-cvprw-real-time-hand-pose-estimation-using-depth-sensors-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2011 CVPRW] Real Time Hand Pose Estimation using Depth Sensors. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2011-bmvc-efficient-model-based-3d-tracking-of-hand-articulations-using-kinect-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2011 BMVC] Efficient Model-based 3D Tracking of Hand Articulations using Kinect. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#theses" class="md-nav__link">
    <span class="md-ellipsis">
      Theses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Theses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2022-hand-analysis-from-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022] Hand Analysis From Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-learning-without-labeling-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020] Learning without Labeling for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-computational-learning-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] Computational Learning for Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-3d-hand-pose-estimation-methods-datasets-and-challenges-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] 3D hand pose estimation: methods, datasets, and challenges. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-3d-hand-pose-estimation-using-convolutional-neural-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] 3D hand pose estimation using convolutional neural networks. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-3d-hand-pose-estimation-from-images-for-interactive-applications-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] 3D Hand Pose Estimation from Images for Interactive Applications. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-articulated-human-pose-estimation-in-unconstrained-images-and-videos-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] Articulated Human Pose Estimation in Unconstrained Images and Videos. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-real-time-generative-hand-modeling-and-tracking-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] Real-Time Generative Hand Modeling and Tracking. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-recovery-of-the-3d-virtual-human-monocular-estimation-of-3d-shape-and-pose-with-data-driven-priors-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] Recovery of the 3D Virtual Human: Monocular Estimation of 3D Shape and Pose with Data Driven Priors. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-human-segmentation-pose-estimation-and-applications-pdf-slides" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017] Human Segmentation, Pose Estimation and Applications. [PDF] [Slides]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-capturing-hand-object-interaction-and-reconstruction-of-manipulated-objects-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017] Capturing Hand-Object Interaction and Reconstruction of Manipulated Objects. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-tracking-hands-in-action-for-gesture-based-computer-input-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016] Tracking Hands in Action for Gesture-based Computer Input. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-3d-hand-pose-regression-with-variants-of-decision-forests-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016] 3D hand pose regression with variants of decision forests. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-deep-learning-for-human-motion-analysis-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016] Deep Learning for Human Motion Analysis. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-real-time-hand-pose-estimation-for-human-computer-interaction-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016] Real time hand pose estimation for human computer interaction. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-efficient-tracking-of-the-3d-articulated-motion-of-human-hands-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015] Efficient Tracking of the 3D Articulated Motion of Human Hands. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-vision-based-hand-pose-estimation-and-gesture-recognition-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015] Vision-based hand pose estimation and gesture recognition. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-localization-of-humans-in-images-using-convolutional-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015] Localization of Humans in Images Using Convolutional Networks. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#datasets" class="md-nav__link">
    <span class="md-ellipsis">
      Datasets
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../code-of-conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributor Covenant Code of Conduct
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contribution Guidelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../evaluation/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Evaluation
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_8_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8_5">
            <span class="md-nav__icon md-icon"></span>
            Evaluation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../awesome-scene-understanding/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome scene understanding
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_9">
            <span class="md-nav__icon md-icon"></span>
            Awesome scene understanding
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-scene-understanding/awesome-scene-understanding_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome Scene Understanding Awesome
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Computer%20Vision/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Computer Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/3D%20Reconstruction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3D Reconstruction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Image%20Classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Classification
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Image%20Matching/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Matching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Instance%20Segmentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Instance Segmentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Keypoint%20Detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Keypoint Detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Multi-Object%20Tracking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi Object Tracking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Object%20Detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Object Detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Object%20Tracking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Object Tracking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Semantic%20Segmentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Semantic Segmentation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Contrastive%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Contrastive Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Contrastive Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Contrastive%20Learning/Contrastive%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contrastive Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Federated%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Federated Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Federated Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Asynchronous/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Asynchronous
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Benchmark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Benchmark
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataset
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Federated%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Federated Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Framework/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Heterogeneous/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Heterogeneous
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Personalized/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Personalized
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Few-shot%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Few shot Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_7" id="__nav_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Few shot Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Few-shot%20Learning/Few-shot%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Few shot Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Few-shot%20Learning/Meta%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Meta Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Few-shot%20Learning/One-shot%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    One shot Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Graph%20Neural%20Network/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Graph Neural Network
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_8" id="__nav_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Graph Neural Network
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Graph%20Neural%20Network/Graph%20Neural%20Network/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Graph Neural Network
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    HuggingFace
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            HuggingFace
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../HuggingFace/huggface_news/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face News
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Large-Language%20Model/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Large Language Model
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10" id="__nav_10_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Large Language Model
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Large-Language%20Model/Large-Language%20Model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Large Language Model
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Medical%20Application/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Medical Application
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11" id="__nav_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            Medical Application
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Medical%20Application/Medical%20Application/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Medical Application
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Medical%20Application/Medical%20Image%20Analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Medical Image Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Medical%20Application/Medical%20Multi-modal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Medical Multi modal
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Multi-modal/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Multi modal
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_12" id="__nav_12_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12">
            <span class="md-nav__icon md-icon"></span>
            Multi modal
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Alignment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Alignment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Image%20Caption/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Caption
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Multi-modal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi modal
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Text%20and%20Image%20Generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text and Image Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/VQA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VQA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Vision-Language/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision Language
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Reinforcement%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13" id="__nav_13_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reinforcement%20Learning/Reinforcement%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_14" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Robotics/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Robotics
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_14" id="__nav_14_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_14">
            <span class="md-nav__icon md-icon"></span>
            Robotics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Robotics/Robotics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Robotics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Robotics/SFM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SFM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Robotics/SLAM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Robotics/Visual%20Localization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Visual Localization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_15" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Transfer%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transfer Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_15" id="__nav_15_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_15">
            <span class="md-nav__icon md-icon"></span>
            Transfer Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Transfer%20Learning/Transfer%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transfer Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_16" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Transformer/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformer
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_16" id="__nav_16_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_16">
            <span class="md-nav__icon md-icon"></span>
            Transformer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Transformer/Transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Transformer/Vision%20Transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_17" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Unsupervised%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_17" id="__nav_17_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_17">
            <span class="md-nav__icon md-icon"></span>
            Unsupervised Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Unsupervised%20Learning/GAN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GAN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Unsupervised%20Learning/Unsupervised%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_18" >
        
          
          <label class="md-nav__link" for="__nav_18" id="__nav_18_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    History
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_18">
            <span class="md-nav__icon md-icon"></span>
            History
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-01-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-01-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-01-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-01-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-02-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#contents" class="md-nav__link">
    <span class="md-ellipsis">
      Contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arxiv-papers" class="md-nav__link">
    <span class="md-ellipsis">
      arXiv Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="arXiv Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arxiv220604927-pushing-the-envelope-for-depth-based-semi-supervised-3d-hand-pose-estimation-with-consistency-training-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2206.04927] Pushing the Envelope for Depth-Based Semi-Supervised 3D Hand Pose Estimation with Consistency Training. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv220604927-ego2handspose-a-dataset-for-egocentric-two-hand-3d-global-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2206.04927] Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv220607117-trihorn-net-a-model-for-accurate-depth-based-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2206.07117] TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv220204533-nimble-a-non-rigid-hand-model-with-bones-and-muscles-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2202.04533] NIMBLE: A Non-rigid Hand Model with Bones and Muscles. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv220109548-consistent-3d-hand-reconstruction-in-video-via-self-supervised-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2201.09548] Consistent 3D Hand Reconstruction in Video via self-supervised Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv211106500-dynamic-iterative-refinement-for-efficient-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2111.06500] Dynamic Iterative Refinement for Efficient 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210914744-the-object-at-hand-automated-editing-for-mixed-reality-video-guidance-from-hand-object-interactions-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2109.14744] The Object at Hand: Automated Editing for Mixed Reality Video Guidance from Hand-Object Interactions. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210914657-understanding-egocentric-hand-object-interactions-from-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2109.14657] Understanding Egocentric Hand-Object Interactions from Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210911747-a-multi-view-video-based-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2109.11747] A Multi-View Video-Based 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210813995-realistic-hands-a-hybrid-model-for-3d-hand-reconstruction-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2108.13995] Realistic Hands: A Hybrid Model for 3D Hand Reconstruction. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210807044-towards-unconstrained-joint-hand-object-reconstruction-from-rgb-videos-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2108.07044] Towards unconstrained joint hand-object reconstruction from RGB videos. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210700887-ho-3d_v3-improving-the-accuracy-of-hand-object-annotations-of-the-ho-3d-dataset-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2107.00887] HO-3D_v3: Improving the Accuracy of Hand-Object Annotations of the HO-3D Dataset. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210605954-adversarial-motion-modelling-helps-semi-supervised-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2106.05954] Adversarial Motion Modelling helps Semi-supervised Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210604324-contrastive-representation-learning-for-hand-shape-estimation-pdf-project-code-data" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2106.04324] Contrastive Representation Learning for Hand Shape Estimation. [PDF] [Project] [Code] [Data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210414639-handsformer-keypoint-transformer-for-monocular-3d-pose-estimation-ofhands-and-object-in-interaction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2104.14639] HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation ofHands and Object in Interaction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv210207067-fasthand-fast-hand-pose-estimation-from-a-monocular-camera-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2102.07067] FastHand: Fast Hand Pose Estimation From A Monocular Camera. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv201211260-unsupervised-domain-adaptation-with-temporal-consistent-self-training-for-3d-hand-object-joint-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2012.11260] Unsupervised Domain Adaptation with Temporal-Consistent Self-Training for 3D Hand-Object Joint Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv200808324-frankmocap-fast-monocular-3d-hand-and-body-motion-capture-by-regression-and-integration-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2008.08324] FrankMocap: Fast Monocular 3D Hand and Body Motion Capture by Regression and Integration. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv200605927-recent-advances-in-3d-object-and-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2006.05927] Recent Advances in 3D Object and Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv200108047-attention-a-lightweight-2d-hand-pose-estimation-approach-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2001.08047] Attention! A Lightweight 2D Hand Pose Estimation Approach. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv200100702-handaugment-a-simple-data-augmentation-method-for-depth-based-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:2001.00702] HandAugment: A Simple Data Augmentation Method for Depth-Based 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv191212436-silhouette-net-3d-hand-pose-estimation-from-silhouettes-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1912.12436] Silhouette-Net: 3D Hand Pose Estimation from Silhouettes. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv191112501-an-end-to-end-framework-for-unconstrained-monocular-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1911.12501] An End-to-end Framework for Unconstrained Monocular 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv191201875-graphposegan-3d-hand-pose-estimation-from-a-monocular-rgb-image-via-adversarial-learning-on-graphs-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1912.01875] GraphPoseGAN: 3D Hand Pose Estimation from a Monocular RGB Image via Adversarial Learning on Graphs. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv191107424-capturing-hand-articulations-using-recurrent-neural-network-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1911.07424] Capturing Hand Articulations using Recurrent Neural Network for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv180700898-model-based-hand-pose-estimation-for-generalized-hand-shape-with-appearance-normalization-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1807.00898] Model-based Hand Pose Estimation for Generalized Hand Shape with Appearance Normalization. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv170509606-end-to-end-global-to-local-cnn-learning-for-hand-pose-recovery-in-depth-data-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1705.09606] End-to-end Global to Local CNN Learning for Hand Pose Recovery in Depth data. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv170402224-hand3d-hand-pose-estimation-using-3d-neural-network-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1704.02224] Hand3D: Hand Pose Estimation using 3D Neural Network. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arxiv161200596-learning-to-search-on-manifolds-for-3d-pose-estimation-of-articulated-objects-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [arXiv:1612.00596] Learning to Search on Manifolds for 3D Pose Estimation of Articulated Objects. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#journal-papers" class="md-nav__link">
    <span class="md-ellipsis">
      Journal Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Journal Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tpami-ijcv" class="md-nav__link">
    <span class="md-ellipsis">
      TPAMI / IJCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TPAMI / IJCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2024-tpami-evhandpose-event-based-3d-hand-pose-estimation-with-sparse-supervision-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2024 TPAMI] EvHandPose: Event-Based 3D Hand Pose Estimation With Sparse Supervision. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2024-tpami-learning-a-contact-potential-field-for-modeling-the-hand-object-interaction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2024 TPAMI] Learning a Contact Potential Field for Modeling the Hand-Object Interaction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-tpami-consistent-3d-hand-reconstruction-in-video-via-self-supervised-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 TPAMI] Consistent 3D Hand Reconstruction in Video via self-supervised Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-tpami-recurrent-3d-hand-pose-estimation-using-cascaded-pose-guided-3d-alignments-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 TPAMI] Recurrent 3D Hand Pose Estimation Using Cascaded Pose-guided 3D Alignments. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-tpami-handvoxnet-3d-hand-shape-and-pose-estimation-using-voxel-based-neural-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 TPAMI] HandVoxNet++: 3D Hand Shape and Pose Estimation using Voxel-Based Neural Networks. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-tpami-3d-hand-pose-estimation-using-synthetic-data-and-weakly-labeled-rgb-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 TPAMI] 3D Hand Pose Estimation Using Synthetic Data and Weakly Labeled RGB Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-tpami-generalized-feedback-loop-for-joint-hand-object-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 TPAMI] Generalized Feedback Loop for Joint Hand-Object Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-tpami-feature-boosting-network-for-3d-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 TPAMI] Feature Boosting Network For 3D Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tpami-opening-the-black-box-hierarchical-sampling-optimization-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TPAMI] Opening the Black Box: Hierarchical Sampling Optimization for Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-ijcv-depth-based-hand-pose-estimation-methods-data-and-challenges-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 IJCV] Depth-Based Hand Pose Estimation: Methods, Data, and Challenges. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tpami-real-time-3d-hand-pose-estimation-with-3d-convolutional-neural-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TPAMI] Real-time 3D Hand Pose Estimation with 3D Convolutional Neural Networks. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-ijcv-lie-x-depth-image-based-articulated-object-pose-estimation-tracking-and-action-recognition-on-lie-groups-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 IJCV] Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-tpami-latent-regression-forest-structured-estimation-of-3d-hand-poses-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 TPAMI] Latent Regression Forest: Structured Estimation of 3D Hand Poses. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-ijcv-capturing-hands-in-action-using-discriminative-salient-points-and-physics-simulation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 IJCV] Capturing Hands in Action using Discriminative Salient Points and Physics Simulation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-ijcv-estimate-hand-poses-efficiently-from-single-depth-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 IJCV] Estimate Hand Poses Efficiently from Single Depth Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-journals" class="md-nav__link">
    <span class="md-ellipsis">
      Other Journals
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Other Journals">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2023-eswa-trihorn-net-a-model-for-accurate-depth-based-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 ESWA] TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-tip-a-dual-branch-self-boosting-framework-for-self-supervised-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 TIP] A Dual-Branch Self-Boosting Framework for Self-Supervised 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-technologies-a-survey-on-gan-based-data-augmentation-for-hand-pose-estimation-problem-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 Technologies] A Survey on GAN-Based Data Augmentation for Hand Pose Estimation Problem. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-tcsvt-3d-hand-pose-estimation-from-monocular-rgb-with-feature-interaction-module-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 TCSVT] 3D Hand Pose Estimation from Monocular RGB with Feature Interaction Module. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-tip-hand-pose-understanding-with-large-scale-photo-realistic-rendering-dataset-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 TIP] Hand Pose Understanding with Large-Scale Photo-Realistic Rendering Dataset. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-tip-joint-hand-object-3d-reconstruction-from-a-single-image-with-cross-branch-feature-fusion-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 TIP] Joint Hand-object 3D Reconstruction from a Single Image with Cross-branch Feature Fusion. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-neurocomputing-spatial-aware-stacked-regression-network-for-real-time-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 Neurocomputing] Spatial-aware Stacked Regression Network for Real-time 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-tmm-differentiable-spatial-regression-a-novel-method-for-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 TMM] Differentiable Spatial Regression: A Novel Method for 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-tip-weakly-supervised-learning-for-single-depth-based-hand-shape-recovery-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 TIP] Weakly-supervised Learning for Single Depth based Hand Shape Recovery. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-signal-process-image-commun-accurate-3d-hand-pose-estimation-network-utilizing-joints-information-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 Signal Process Image Commun] Accurate 3D Hand Pose Estimation Network Utilizing Joints Information. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-tcsvt-improve-regression-network-on-depth-hand-pose-estimation-with-auxiliary-variable-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 TCSVT] Improve Regression Network on Depth Hand Pose Estimation with Auxiliary Variable. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-tvcg-3d-hand-tracking-in-the-presence-of-excessive-motion-blur-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 TVCG] 3D Hand Tracking in the Presence of Excessive Motion Blur. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-computers-graphics-simple-and-effective-deep-hand-shape-and-pose-regression-from-a-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 Computers &amp; Graphics] Simple and effective deep hand shape and pose regression from a single depth image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-tip-srhandnet-real-time-2d-hand-pose-estimation-with-simultaneous-region-localization-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 TIP] SRHandNet: Real-time 2D Hand Pose Estimation with Simultaneous Region Localization. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-sensors-whsp-net-a-weakly-supervised-approach-for-3d-hand-shape-and-pose-recovery-from-a-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 Sensors] WHSP-Net: A Weakly-Supervised Approach for 3D Hand Shape and Pose Recovery from a Single Depth Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-ra-l-variational-object-aware-3d-hand-pose-from-a-single-rgb-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 RA-L] Variational Object-aware 3D Hand Pose from a Single RGB Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-pr-a-survey-on-3d-hand-pose-estimation-cameras-methods-and-datasets-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 PR] A Survey on 3D Hand Pose Estimation: Cameras, Methods, and Datasets. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-neurocomputing-a-crnn-module-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 Neurocomputing] A CRNN module for hand pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-ivc-large-scale-multiview-3d-hand-pose-dataset-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 IVC] Large-scale Multiview 3D Hand Pose Dataset. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tcsvt-mask-pose-cascaded-cnn-for-2d-hand-pose-estimation-from-single-color-image-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TCSVT] Mask-pose Cascaded CNN for 2D Hand Pose Estimation from Single Color Image. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-ivc-top-down-model-fitting-for-hand-pose-recovery-in-sequences-of-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 IVC] Top-down model fitting for hand pose recovery in sequences of depth images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tcyb-context-aware-deep-spatio-temporal-network-for-hand-pose-estimation-from-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TCYB] Context-Aware Deep Spatio-Temporal Network for Hand Pose Estimation from Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-ieee-access-shpr-net-deep-semantic-hand-pose-regression-from-point-clouds-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 IEEE Access] SHPR-Net: Deep Semantic Hand Pose Regression From Point Clouds. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-neurocomputing-pose-guided-structured-region-ensemble-network-for-cascaded-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 Neurocomputing] Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-pr-learning-a-deep-network-with-spherical-part-model-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 PR] Learning a deep network with spherical part model for 3D hand pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-tip-robust-3d-hand-pose-estimation-from-single-depth-images-using-multi-view-cnns-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 TIP] Robust 3D Hand Pose Estimation from Single Depth Images using Multi-View CNNs. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-jvci-region-ensemble-network-towards-good-practices-for-deep-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 JVCI] Region Ensemble Network: Towards Good Practices for Deep 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-tcyb-hough-forest-with-optimized-leaves-for-global-hand-pose-estimation-with-arbitrary-postures-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 TCYB] Hough Forest with Optimized Leaves for Global Hand Pose Estimation with Arbitrary Postures. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-tcsvt-robust-rgb-d-hand-tracking-using-deep-learning-priors-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 TCSVT] Robust RGB-D Hand Tracking Using Deep Learning Priors. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-cviu-hand-pose-estimation-through-semi-supervised-and-weakly-supervised-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 CVIU] Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-neurocomputing-multi-task-multi-domain-learning-application-to-semantic-segmentation-and-pose-regression-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 Neurocomputing] Multi-task, Multi-domain Learning: application to semantic segmentation and pose regression. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-cviu-guided-optimisation-through-classification-and-regression-for-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 CVIU] Guided Optimisation through Classification and Regression for Hand Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-tcsvt-resolving-ambiguous-hand-pose-predictions-by-exploiting-part-correlations-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 TCSVT] Resolving Ambiguous Hand Pose Predictions by Exploiting Part Correlations. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2014-tmm-parsing-the-hand-in-depth-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2014 TMM] Parsing the Hand in Depth Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conference-papers" class="md-nav__link">
    <span class="md-ellipsis">
      Conference Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Conference Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2024-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2024 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2024 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#handdagt-a-denoising-adaptive-graph-transformer-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dense-hand-objectho-graspnet-with-full-grasping-taxonomy-and-dynamics-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Dense Hand-Object(HO) GraspNet with Full Grasping Taxonomy and Dynamics. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-pose-estimation-in-everyday-egocentric-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Pose Estimation in Everyday Egocentric Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-reconstruction-of-objects-in-hands-without-real-world-3d-supervision-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Reconstruction of Objects in Hands without Real World 3D Supervision. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-3d-hand-reconstruction-with-knowledge-prior-and-uncertainty-guidance-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mlphand-real-time-multi-view-3d-hand-reconstruction-via-mlp-modeling-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • MLPHand: Real Time Multi-View 3D Hand Reconstruction via MLP Modeling. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#are-synthetic-data-useful-for-egocentric-hand-object-interaction-detection-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-sequence-recovery-from-real-blurry-images-and-event-stream-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Sequence Recovery from Real Blurry Images and Event Stream. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coarse-to-fine-implicit-representation-learning-for-3d-hand-object-reconstruction-from-a-single-rgb-d-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handdgp-camera-space-hand-mesh-prediction-with-differentiable-global-positioning-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#controlling-the-world-by-sleight-of-hand-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Controlling the World by Sleight of Hand. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-cross-hand-policies-of-high-dof-reaching-and-grasping-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning Cross-hand Policies of High-DOF Reaching and Grasping. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-sco-dual-stream-conditional-diffusion-for-monocular-hand-held-object-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nl2contact-natural-language-guided-3d-hand-object-contact-modeling-with-diffusion-model-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks-and-challenges-in-pose-estimation-for-egocentric-hand-interactions-with-objects-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on-the-utility-of-3d-hand-poses-for-action-recognition-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • On the Utility of 3D Hand Poses for Action Recognition. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attentionhand-text-driven-controllable-hand-image-generation-for-3d-hand-reconstruction-in-the-wild-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2024-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2024 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2024 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#physics-aware-hand-object-interaction-denoising" class="md-nav__link">
    <span class="md-ellipsis">
      • Physics-aware Hand-object Interaction Denoising
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hoidiffusion-generating-realistic-3d-hand-object-interaction-data" class="md-nav__link">
    <span class="md-ellipsis">
      • HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#urhand-universal-relightable-hands-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • URHand: Universal Relightable Hands. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#oakink2-a-dataset-of-embodied-hands-object-manipulation-in-long-horizon-complex-task-completion" class="md-nav__link">
    <span class="md-ellipsis">
      • OakInk2: A Dataset of Embodied Hands-Object Manipulation in Long-Horizon Complex Task Completion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interhandgen-two-hand-interaction-generation-via-cascaded-reverse-diffusion" class="md-nav__link">
    <span class="md-ellipsis">
      • InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moho-learning-single-view-hand-held-object-reconstruction-with-multi-view-occlusion-aware-supervision-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ohta-one-shot-hand-avatar-via-data-driven-implicit-priors-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • OHTA: One-shot Hand Avatar via Data-driven Implicit Priors. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handbooster-boosting-3d-hand-mesh-reconstruction-by-conditional-synthesis-and-sampling-of-hand-object-interactions" class="md-nav__link">
    <span class="md-ellipsis">
      • HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handdiff-3d-hand-pose-estimation-with-diffusion-on-image-point-cloud" class="md-nav__link">
    <span class="md-ellipsis">
      • HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text2hoi-text-guided-3d-motion-generation-for-hand-object-interaction" class="md-nav__link">
    <span class="md-ellipsis">
      • Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#both2hands-inferring-3d-hands-from-both-text-prompts-and-body-dynamics-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gears-local-geometry-aware-hand-object-interaction-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      • GEARS: Local Geometry-aware Hand-object Interaction Synthesis.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-simple-baseline-for-efficient-hand-mesh-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • A Simple Baseline for Efficient Hand Mesh Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hold-category-agnostic-3d-reconstruction-of-interacting-hands-and-objects-from-video-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ms-mano-enabling-hand-pose-tracking-with-biomechanical-constraints" class="md-nav__link">
    <span class="md-ellipsis">
      • MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hoist-former-hand-held-objects-identification-segmentation-and-tracking-in-the-wild" class="md-nav__link">
    <span class="md-ellipsis">
      • HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bitt-bi-directional-texture-reconstruction-of-interacting-two-hands-from-a-single-image" class="md-nav__link">
    <span class="md-ellipsis">
      • BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#authentic-hand-avatar-from-a-phone-scan-via-universal-hand-model" class="md-nav__link">
    <span class="md-ellipsis">
      • Authentic Hand Avatar from a Phone Scan via Universal Hand Model.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reconstructing-hands-in-3d-with-transformers-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Reconstructing Hands in 3D with Transformers. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#complementing-event-streams-and-rgb-frames-for-hand-mesh-reconstruction" class="md-nav__link">
    <span class="md-ellipsis">
      • Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#single-to-dual-view-adaptation-for-egocentric-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hoisdf-constraining-3d-hand-object-pose-estimation-with-global-signed-distance-fields-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HOISDF: Constraining 3D Hand Object Pose Estimation with Global Signed Distance Fields. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g-hop-generative-hand-object-prior-for-interaction-reconstruction-and-grasp-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      • G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hhmr-holistic-hand-mesh-recovery-by-enhancing-the-multimodal-controllability-of-graph-diffusion-models" class="md-nav__link">
    <span class="md-ellipsis">
      • HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models.
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conference-papers_1" class="md-nav__link">
    <span class="md-ellipsis">
      Conference Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Conference Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2023-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2023 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2023 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-probabilistic-attention-model-with-occlusion-aware-texture-regression-for-3d-hand-reconstruction-from-a-single-rgb-image" class="md-nav__link">
    <span class="md-ellipsis">
      • A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a2j-transformer-anchor-to-joint-transformer-network-for-3d-interacting-hand-pose-estimation-from-a-single-rgb-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memahand-exploiting-mesh-mano-interaction-for-single-image-two-hand-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#arctic-a-dataset-for-dexterous-bimanual-hand-object-manipulation-project" class="md-nav__link">
    <span class="md-ellipsis">
      • ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation. [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemblyhands-towards-egocentric-activity-understanding-via-3d-hand-pose-estimation-project" class="md-nav__link">
    <span class="md-ellipsis">
      • AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation. [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#high-fidelity-3d-hand-shape-reconstruction-via-scalable-graph-frequency-decomposition" class="md-nav__link">
    <span class="md-ellipsis">
      • High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handnerf-neural-radiance-fields-for-animatable-interacting-hands-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HandNeRF: Neural Radiance Fields for Animatable Interacting Hands. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#poem-reconstructing-hand-in-a-point-embedded-multi-view-stereo-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • POEM: Reconstructing Hand in a Point Embedded Multi-view Stereo. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#harp-personalized-hand-reconstruction-from-a-monocular-rgb-video-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • HARP: Personalized Hand Reconstruction from a Monocular RGB Video. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relightablehands-efficient-neural-relighting-of-articulated-hand-models-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • RelightableHands: Efficient Neural Relighting of Articulated Hand Models. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#h2onet-hand-occlusion-and-orientation-aware-network-for-real-time-3d-hand-mesh-reconstruction" class="md-nav__link">
    <span class="md-ellipsis">
      • H2ONet: Hand-Occlusion-and-Orientation-aware Network for Real-time 3D Hand Mesh Reconstruction.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#affordance-diffusion-synthesizing-hand-object-interactions-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Affordance Diffusion: Synthesizing Hand-Object Interactions. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gsdf-geometry-driven-signed-distance-functions-for-3d-hand-object-reconstruction-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#harmonious-feature-learning-for-interactive-hand-object-pose-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      • Harmonious Feature Learning for Interactive Hand-Object Pose Estimation.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handy-towards-a-high-fidelity-3d-hand-shape-and-appearance-model" class="md-nav__link">
    <span class="md-ellipsis">
      • Handy: Towards a high fidelity 3D hand shape and appearance model.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-avatar-free-pose-hand-animation-and-rendering-from-monocular-video-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-domain-3d-hand-pose-estimation-with-dual-modalities-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Cross-domain 3D Hand Pose Estimation with Dual Modalities. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overcoming-the-tradeoff-in-accuracy-and-plausibility-for-3d-hand-shape-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Overcoming the Tradeoff in Accuracy and Plausibility for 3D Hand Shape Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hierarchical-temporal-transformer-for-3d-hand-pose-estimation-and-action-recognition-from-egocentric-rgb-videos-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#im2hands-learning-attentive-implicit-representation-of-interacting-two-hand-shapes-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-voting-field-for-camera-space-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Neural Voting Field for Camera-Space 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bringing-inputs-to-shared-domains-for-3d-interacting-hands-recovery-in-the-wild-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recovering-3d-hand-mesh-sequence-from-a-single-blurry-image-a-new-dataset-and-temporal-unfolding-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semi-supervised-hand-appearance-recovery-via-structure-disentanglement-and-dual-adversarial-discrimination-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-based-unified-recognition-of-two-hands-manipulating-objects-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Transformer-based Unified Recognition of Two Hands Manipulating Objects. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#acr-attention-collaboration-based-regressor-for-arbitrary-two-hand-reconstruction-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2023 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2023 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#novel-view-synthesis-and-pose-estimation-for-hand-object-interaction-from-sparse-views-pdf-project-code-data" class="md-nav__link">
    <span class="md-ellipsis">
      • Novel-view Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views. [PDF] Project] [Code] [Data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contactgen-generative-contact-modeling-for-grasp-generation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • ContactGen: Generative Contact Modeling for Grasp Generation [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#diffusion-guided-reconstruction-of-everyday-hand-object-interaction-clips-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handr2n2-iterative-3d-hand-pose-estimation-using-a-residual-recurrent-neural-network-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HandR2N2: Iterative 3D Hand Pose Estimation Using a Residual Recurrent Neural Network. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hamuco-hand-pose-estimation-via-multiview-collaborative-self-supervised-learning-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deformer-dynamic-fusion-transformer-for-robust-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phrit-parametric-hand-representation-with-implicit-template-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • PHRIT: Parametric Hand Representation with Implicit Template. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chord-category-level-hand-held-object-reconstruction-via-shape-deformation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#uncertainty-aware-state-space-transformer-for-egocentric-3d-hand-trajectory-forecasting-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spectral-graphormer-spectral-graph-based-transformer-for-egocentric-two-hand-reconstruction-using-multi-view-color-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Spectral Graphormer: Spectral Graph-Based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reconstructing-interacting-hands-with-interaction-prior-from-monocular-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Reconstructing Interacting Hands with Interaction Prior from Monocular Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ochid-fi-occlusion-robust-hand-pose-estimation-in-3d-via-rf-vision-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dynamic-hyperbolic-attention-network-for-fine-hand-object-reconstruction-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoupled-iterative-refinement-framework-for-interacting-hands-reconstruction-from-a-single-rgb-image-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#affordpose-a-large-scale-dataset-of-hand-object-interactions-with-affordance-driven-hand-pose-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • AffordPose: A Large-Scale Dataset of Hand-Object Interactions with Affordance-Driven Hand Pose. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#egopca-a-new-framework-for-egocentric-hand-object-interaction-understanding-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#renderih-a-large-scale-synthetic-dataset-for-3d-interacting-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#source-free-domain-adaptive-human-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Source-free Domain Adaptive Human Pose Estimation [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#livehand-real-time-and-photorealistic-neural-hand-rendering-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • LiveHand: Real-time and Photorealistic Neural Hand Rendering [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-others" class="md-nav__link">
    <span class="md-ellipsis">
      2023 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2023 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2023-neurips-fourierhandflow-neural-4d-hand-representation-using-fourier-query-flow-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 NeurIPS] FourierHandFlow: Neural 4D Hand Representation Using Fourier Query Flow. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-iccvw-showme-benchmarking-object-agnostic-hand-object-3d-reconstruction-pdf-project-code-data" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 ICCVW] SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction. [PDF] [Project] [Code] [Data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-aaai-two-heads-are-better-than-one-image-point-cloud-network-for-depth-based-3d-hand-pose-estimation-pdf-aaai-23-distinguished-papers" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 AAAI] Two Heads are Better than One: Image-Point Cloud Network for Depth-Based 3D Hand Pose Estimation. [[PDF]] (AAAI-23 Distinguished Papers)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-aaai-tracking-and-reconstructing-hand-object-interactions-from-point-cloud-sequences-in-the-wild-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 AAAI] Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2023-wacv-thor-net-end-to-end-graformer-based-realistic-two-hands-and-object-reconstruction-with-self-supervision-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2023 WACV] THOR-Net: End-to-End Graformer-Based Realistic Two Hands and Object Reconstruction With Self-Supervision. [PDF] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2022 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2022 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#identity-aware-hand-mesh-estimation-and-personalization-from-rgb-images-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Identity-aware Hand Mesh Estimation and Personalization from RGB Images . [[PDF]] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alignsdf-pose-aligned-signed-distance-fields-for-hand-object-reconstruction-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#s2contact-graph-based-network-for-3d-hand-object-contact-estimation-with-semi-supervised-learning-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • S2Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#domain-adaptive-hand-keypoint-and-pixel-localization-in-the-wild-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-interacting-hand-pose-estimation-by-hand-de-occlusion-and-removal-pdf-projectdataset" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal. [PDF] [Project][Dataset]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-attention-of-disentangled-modalities-for-3d-human-mesh-recovery-with-transformers-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2022 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2022 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#whats-in-your-hands-3d-reconstruction-of-generic-objects-in-hands-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • What's in your hands? 3D Reconstruction of Generic Objects in Hands. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mining-multi-view-information-a-strong-self-supervised-framework-for-depth-based-3d-hand-pose-and-mesh-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Mining Multi-View Information: A Strong Self-Supervised Framework for Depth-based 3D Hand Pose and Mesh Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handoccnet-occlusion-robust-3d-hand-mesh-estimation-network-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#keypoint-transformer-solving-joint-identification-in-challenging-hands-and-object-interactions-for-accurate-3d-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#collaborative-learning-for-hand-and-object-reconstruction-with-attention-guided-graph-convolution-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spatial-temporal-parallel-transformer-for-arm-hand-dynamic-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Spatial-Temporal Parallel Transformer for Arm-Hand Dynamic Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-grasp-physically-plausible-dynamic-grasp-synthesis-for-hand-object-interactions-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#goal-generating-4d-whole-body-motion-for-hand-object-grasping-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#oakink-a-large-scale-knowledge-repository-for-understanding-hand-object-interaction-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#artiboost-boosting-articulated-3d-hand-object-pose-estimation-via-online-exploration-and-synthesis-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interacting-attention-graph-for-single-image-two-hand-reconstruction-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Interacting Attention Graph for Single Image Two-Hand Reconstruction. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mobrecon-mobile-friendly-hand-mesh-reconstruction-from-monocular-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lisa-learning-implicit-shape-and-appearance-of-hands-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • LISA: Learning Implicit Shape and Appearance of Hands. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2022-others" class="md-nav__link">
    <span class="md-ellipsis">
      2022 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2022 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2022-aaai-efficient-virtual-view-selection-for-3d-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022 AAAI] Efficient Virtual View Selection for 3D Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2021 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2021 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#toward-human-like-grasp-dexterous-grasping-via-semantic-representation-of-object-hand-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Toward Human-Like Grasp: Dexterous Grasping via Semantic Representation of Object-Hand. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-transfer-learning-for-hand-mesh-recovery-from-binocular-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Self-Supervised Transfer Learning for Hand Mesh Recovery From Binocular Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-3d-hand-pose-estimation-from-monocular-rgb-via-contrastive-learning-pdf-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. [PDF] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#towards-accurate-alignment-in-real-time-3d-hand-mesh-reconstruction-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eventhands-real-time-neural-3d-hand-reconstruction-from-an-event-stream-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • EventHands: Real-Time Neural 3D Hand Reconstruction from an Event Stream. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reconstructing-hand-object-interactions-in-the-wild-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Reconstructing Hand-Object Interactions in the Wild. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handfoldingnet-a-3d-hand-pose-estimation-network-using-multiscale-feature-guided-folding-of-a-2d-hand-skeleton-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#h2o-two-hands-manipulating-objects-for-first-person-interaction-recognition-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • H2O: Two Hands Manipulating Objects for First Person Interaction Recognition. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i2uv-handnet-image-to-uv-prediction-network-for-accurate-and-high-fidelity-3d-hand-mesh-modeling-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • I2UV-HandNet: Image-to-UV Prediction Network for Accurate and High-fidelity 3D Hand Mesh Modeling. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semihand-semi-supervised-hand-pose-estimation-with-consistency-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • SemiHand: Semi-supervised Hand Pose Estimation with Consistency. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#end-to-end-detection-and-pose-estimation-of-two-interacting-hands-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • End-to-End Detection and Pose Estimation of Two Interacting Hands. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-object-contact-consistency-reasoning-for-human-grasps-generation-pdf-project-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand-Object Contact Consistency Reasoning for Human Grasps Generation. [PDF] [Project] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-image-understanding-via-deep-multi-task-learning-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Image Understanding via Deep Multi-Task Learning. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpf-learning-a-contact-potential-field-to-model-the-hand-object-interaction-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • CPF: Learning a Contact Potential Field to Model the Hand-object Interaction. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#travelnet-self-supervised-physically-plausible-hand-motion-learning-from-monocular-color-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • TravelNet: Self-supervised Physically Plausible Hand Motion Learning from Monocular Color Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interacting-two-hand-3d-pose-and-shape-reconstruction-from-single-color-image-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Interacting Two-Hand 3D Pose and Shape Reconstruction from Single Color Image. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#removing-the-bias-of-integral-pose-regression-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Removing the Bias of Integral Pose Regression. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2021 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2021 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monocular-real-time-full-body-capture-with-inter-part-correlations-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Monocular Real-time Full Body Capture with Inter-part Correlations. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#end-to-end-human-pose-and-mesh-reconstruction-with-transformers-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • End-to-End Human Pose and Mesh Reconstruction with Transformers. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dexycb-a-benchmark-for-capturing-hand-grasping-of-objects-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • DexYCB: A Benchmark for Capturing Hand Grasping of Objects. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#body2hands-learning-to-infer-3d-hands-from-conversational-gesture-body-dynamics-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-space-hand-mesh-recovery-via-semantic-aggregation-and-adaptive-2d-1d-registration-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-based-3d-hand-reconstruction-via-self-supervised-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Model-based 3D Hand Reconstruction via Self-Supervised Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semi-supervised-3d-hand-object-poses-estimation-with-interactions-in-time-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-others" class="md-nav__link">
    <span class="md-ellipsis">
      2021 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2021 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2021-3dv-a-skeleton-driven-neural-occupancy-representation-for-articulated-hands-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 3DV] A Skeleton-Driven Neural Occupancy Representation for Articulated Hands. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-3dv-learning-to-disambiguate-strongly-interacting-hands-via-probabilistic-per-pixel-part-segmentation-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 3DV] Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-dicta-semi-supervised-3d-hand-shape-and-pose-estimation-with-label-propagation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 DICTA] Semi-Supervised 3D Hand Shape and Pose Estimation with Label Propagation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-bmvc-joint-aware-regression-rethinking-regression-based-method-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 BMVC] Joint-Aware Regression: Rethinking Regression-Based Method for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-bmvc-local-and-global-point-cloud-reconstruction-for-3d-hand-pose-estimation-pdf-data" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 BMVC] Local and Global Point Cloud Reconstruction for 3D Hand Pose Estimation. [PDF] [Data]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-bmvc-handtailor-towards-high-precision-monocular-3d-hand-recovery-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 BMVC] HandTailor: Towards High-Precision Monocular 3D Hand Recovery. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-bmvc-multi-view-image-based-hand-geometry-refinement-using-differentiable-monte-carlo-ray-tracing-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 BMVC] Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-siggraph-manipnet-neural-manipulation-synthesis-with-a-hand-object-spatial-representation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 SIGGRAPH] ManipNet: Neural Manipulation Synthesis with a Hand-Object Spatial Representation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-siggraph-single-depth-view-based-real-time-reconstruction-of-hand-object-interactions-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 SIGGRAPH] Single Depth View-Based Real-time Reconstruction of Hand-object Interactions. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-iros-dynamic-modeling-of-hand-object-interactions-via-tactile-sensing-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 IROS] Dynamic Modeling of Hand-Object Interactions via Tactile Sensing. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-aaai-exploiting-learnable-joint-groups-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 AAAI] Exploiting Learnable Joint Groups for Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-wacv-active-learning-for-bayesian-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 WACV] Active Learning for Bayesian 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-wacv-two-hand-global-3d-pose-estimation-using-monocular-rgb-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 WACV] Two-hand Global 3D Pose Estimation Using Monocular RGB. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-wacv-mvhm-a-large-scale-multi-view-hand-mesh-benchmark-for-accurate-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 WACV] MVHM: A Large-Scale Multi-View Hand Mesh Benchmark for Accurate 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-wacv-temporal-aware-self-supervised-learning-for-3d-hand-pose-and-mesh-estimation-in-videos-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 WACV] Temporal-Aware Self-Supervised Learning for 3D Hand Pose and Mesh Estimation in Videos. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-petra-weakly-supervised-hand-part-segmentation-from-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 PETRA] Weakly-supervised hand part segmentation from depth images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2021-petra-a-pipeline-for-hand-2-d-keypoint-localization-using-unpaired-image-to-image-translation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2021 PETRA] A Pipeline for Hand 2-D Keypoint Localization Using Unpaired Image to Image Translation. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2020 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2020 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grab-a-dataset-of-whole-body-human-grasping-of-objects-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • GRAB: A Dataset of Whole-Body Human Grasping of Objects. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monocular-expressive-body-regression-through-body-driven-attention-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Monocular Expressive Body Regression through Body-Driven Attention. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-phong-surface-efficient-3d-model-fitting-using-lifted-optimization-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • The Phong Surface: Efficient 3D Model Fitting using Lifted Optimization. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#whole-body-human-pose-estimation-in-the-wild-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Whole-Body Human Pose Estimation in the Wild. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dual-grid-net-hand-mesh-vertex-regression-from-single-depth-maps-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Dual Grid Net: hand mesh vertex regression from single depth maps. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-transformer-non-autoregressive-structured-modeling-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand-Transformer: Non-Autoregressive Structured Modeling for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contactpose-a-dataset-of-grasps-with-object-contact-and-hand-pose-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • ContactPose: A Dataset of Grasps with Object Contact and Hand Pose. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seqhand-rgb-sequence-based-3d-hand-pose-and-shape-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • SeqHAND: RGB-Sequence-Based 3D Hand Pose and Shape Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#html-a-parametric-hand-texture-model-for-3d-hand-reconstruction-and-personalizationm-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • HTML: A Parametric Hand Texture Model for 3D Hand Reconstruction and Personalizationm. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jgr-p2o-joint-graph-reasoning-based-pixel-to-offset-prediction-network-for-3d-hand-pose-estimation-from-a-single-depth-image-pdf-code-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • JGR-P2O: Joint Graph Reasoning based Pixel-to-Offset Prediction Network for 3D Hand Pose Estimation from a Single Depth Image. [PDF] [Code] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptive-computationally-efficient-network-for-monocular-3d-hand-pose-estimation-pdf-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Adaptive Computationally Efficient Network for Monocular 3D Hand Pose Estimation. [PDF] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#collaborative-learning-of-gesture-recognition-and-3d-hand-pose-estimation-with-multi-order-feature-analysis-pdf-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-Order Feature Analysis. [PDF] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deephandmesh-weakly-supervised-deep-encoder-decoder-framework-for-high-fidelity-hand-mesh-modeling-from-a-single-rgb-image-pdf-project-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • DeepHandMesh: Weakly-supervised Deep Encoder-Decoder Framework for High-fidelity Hand Mesh Modeling from a Single RGB Image. [PDF] [Project] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interhand26m-a-new-large-scale-dataset-and-baseline-for-3d-single-and-interacting-hand-pose-estimation-from-a-single-rgb-image-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • InterHand2.6M: A New Large-scale Dataset and Baseline for 3D Single and Interacting Hand Pose Estimation from a Single RGB Image. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i2l-meshnet-image-to-lixel-prediction-network-for-accurate-3d-human-pose-and-mesh-estimation-from-a-single-rgb-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-3d-hand-pose-estimation-via-biomechanical-constraints-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-Supervised 3D Hand Pose Estimation via Biomechanical Constraints. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#measuring-generalisation-to-unseen-viewpoints-articulations-shapes-and-objects-for-3d-hand-pose-estimation-under-hand-object-interaction-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Measuring Generalisation to Unseen Viewpoints, Articulations, Shapes and Objects for 3D Hand Pose Estimation under Hand-Object Interaction. [PDF] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2020 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2020 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-mesh-convolutional-hand-reconstruction-in-the-wild-pdf-project-oral-paper-award-nominees" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild. [PDF] [Project] (Oral) (Paper Award Nominees)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-domain-adaptation-via-gan-and-mesh-model-for-estimating-3d-hand-poses-interacting-objects-pdf-code-oral-paper-award-nominees" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects. [PDF] [Code] (Oral) (Paper Award Nominees)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ganhand-predicting-human-grasp-affordances-in-multi-object-scenes-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modal-variational-alignment-of-latent-spaces-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Cross-Modal Variational Alignment of Latent Spaces. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#humbi-a-large-multiview-dataset-of-human-body-expressions-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • HUMBI: A Large Multiview Dataset of Human Body Expressions. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#epipolar-transformers-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Epipolar Transformers. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handvoxnet-deep-voxel-based-network-for-3d-hand-shape-and-pose-estimation-from-a-single-depth-map-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HandVoxNet: Deep Voxel-Based Network for 3D Hand Shape and Pose Estimation from a Single Depth Map. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#knowledge-as-priors-cross-modal-knowledge-generalization-for-datasets-without-superior-knowledge-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leveraging-photometric-consistency-over-time-for-sparsely-supervised-hand-object-reconstruction-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monocular-real-time-hand-shape-and-motion-capture-using-multi-modal-data-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hope-net-a-graph-based-model-for-hand-object-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#honnotate-a-method-for-3d-annotation-of-hand-and-objects-poses-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HOnnotate: A method for 3D Annotation of Hand and Objects Poses. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-others" class="md-nav__link">
    <span class="md-ellipsis">
      2020 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2020 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2020-icra-robust-occlusion-aware-pose-estimation-for-objects-grasped-by-adaptive-hands-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ICRA] Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands . [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-cvprw-mediapipe-hands-on-device-real-time-hand-tracking-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 CVPRW] MediaPipe Hands: On-device Real-time Hand Tracking. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-ismar-3d-hand-pose-estimation-with-a-single-infrared-camera-via-domain-transfer-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ISMAR] 3D Hand Pose Estimation with a Single Infrared Camera via Domain Transfer Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-ismar-bare-hand-depth-inpainting-for-3d-tracking-of-hand-interacting-with-object-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ISMAR] Bare-hand Depth Inpainting for 3D Tracking of Hand Interacting with Object. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-3dv-grasping-field-learning-implicit-representations-for-human-grasps-pdf-code-best-paper-award" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 3DV] Grasping Field: Learning Implicit Representations for Human Grasps. [PDF] [Code] (Best Paper Award)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-uist-deepfisheye-near-surface-multi-finger-tracking-technology-using-fisheye-camera-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 UIST] DeepFisheye: Near-Surface Multi-Finger Tracking Technology Using Fisheye Camera. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-siggraph-asia-constraining-dense-hand-surface-tracking-with-elasticity-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 SIGGRAPH Asia] Constraining Dense Hand Surface Tracking With Elasticity. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-siggraph-asia-rgb2hands-real-time-tracking-of-3d-hand-interactions-from-monocular-rgb-video-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 SIGGRAPH Asia] RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB Video. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-siggraph-megatrack-monochrome-egocentric-articulated-hand-tracking-for-virtual-reality-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 SIGGRAPH] MEgATrack: Monochrome Egocentric Articulated Hand-Tracking for Virtual Reality. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-mm-mm-hand-3d-aware-multi-modal-guided-hand-generation-for-3d-hand-pose-synthesis-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 MM] MM-Hand: 3D-Aware Multi-Modal Guided Hand Generation for 3D Hand Pose Synthesis. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-mm-adaptive-wasserstein-hourglass-for-weakly-supervised-hand-pose-estimation-from-monocular-rgb-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 MM] Adaptive Wasserstein Hourglass for Weakly Supervised Hand Pose Estimation from Monocular RGB. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-mm-hot-net-non-autoregressive-transformer-for-3d-hand-object-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 MM] HOT-Net: Non-Autoregressive Transformer for 3D Hand-Object Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-bmvc-pmd-net-privileged-modality-distillation-network-for-3d-hand-pose-estimation-from-a-single-rgb-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 BMVC] PMD-Net: Privileged Modality Distillation Network for 3D Hand Pose Estimation from a Single RGB Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-bmvc-sia-gcn-a-spatial-information-aware-graph-neural-network-with-2d-convolutions-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 BMVC] SIA-GCN: A Spatial Information Aware Graph Neural Network with 2D Convolutions for Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-bmvc-explicit-knowledge-distillation-for-3d-hand-pose-estimation-from-monocular-rgb-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 BMVC] Explicit Knowledge Distillation for 3D Hand Pose Estimation from Monocular RGB. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-bmvc-bihand-recovering-hand-mesh-with-multi-stage-bisected-hourglass-networks-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 BMVC] BiHand: Recovering Hand Mesh with Multi-stage Bisected Hourglass Networks. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-ubicomp-fingertrak-continuous-3d-hand-pose-tracking-by-deep-learning-hand-silhouettes-captured-by-miniature-thermal-cameras-on-wrist-pdf-eccv-2020-demo-award-nominee" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 Ubicomp] FingerTrak: Continuous 3D Hand Pose Tracking by Deep Learning Hand Silhouettes Captured by Miniature Thermal Cameras on Wrist. [PDF] (ECCV 2020 Demo Award Nominee)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-fg-hand-tracking-from-monocular-rgb-with-dense-semantic-labels-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 FG] Hand tracking from monocular RGB with dense semantic labels. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-fg-generative-model-based-loss-to-the-rescue-a-method-to-overcome-annotation-errors-for-depth-based-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 FG] Generative Model-Based Loss to the Rescue: A Method to Overcome Annotation Errors for Depth-Based Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-iros-physics-based-dexterous-manipulations-with-estimated-hand-poses-and-residual-reinforcement-learning-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 IROS] Physics-Based Dexterous Manipulations with Estimated Hand Poses and Residual Reinforcement Learning. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-chi-evaluation-of-machine-learning-techniques-for-hand-pose-estimation-on-handheld-device-with-proximity-sensor-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 CHI] Evaluation of Machine Learning Techniques for Hand Pose Estimation on Handheld Device with Proximity Sensor. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-aaai-awr-adaptive-weighting-regression-for-3d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 AAAI] AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-wacv-nonparametric-structure-regularization-machine-for-2d-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 WACV] Nonparametric Structure Regularization Machine for 2D Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-wacv-3d-hand-pose-estimation-with-disentangled-cross-modal-latent-space-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 WACV] 3D Hand Pose Estimation with Disentangled Cross-Modal Latent Space. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-wacv-dggan-depth-image-guided-generative-adversarial-networks-for-disentangling-rgb-and-depth-images-in-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 WACV] DGGAN: Depth-image Guided Generative Adversarial Networks for Disentangling RGB and Depth Images in 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-wacv-rotation-invariant-mixed-graphical-model-network-for-2d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 WACV] Rotation-invariant Mixed Graphical Model Network for 2D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-icassp-weakly-supervised-segmentation-guided-hand-pose-estimation-during-interaction-with-unknown-objects-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ICASSP] Weakly Supervised Segmentation Guided Hand Pose Estimation During Interaction With Unknown Objects. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-icassp-hand-3d-studio-a-new-multi-view-system-for-3d-hand-reconstruction-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020 ICASSP] Hand-3D-Studio: A New Multi-view System for 3D Hand Reconstruction. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2019 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2019 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#freihand-a-dataset-for-markerless-capture-of-hand-pose-and-shape-from-single-rgb-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from Single RGB Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a2j-anchor-to-joint-regression-network-for-3d-articulated-pose-estimation-from-a-single-depth-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exploiting-spatial-temporal-relationships-for-3d-pose-estimation-via-graph-convolutional-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Exploiting Spatial-temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resolving-3d-human-pose-ambiguities-with-3d-scene-constraints-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Resolving 3D Human Pose Ambiguities with 3D Scene Constraints. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#so-handnet-self-organizing-network-for-3d-hand-pose-estimation-with-semi-supervised-learning-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#end-to-end-hand-mesh-recovery-from-a-monocular-rgb-image-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • End-to-end Hand Mesh Recovery from a Monocular RGB Image. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aligning-latent-spaces-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Aligning Latent Spaces for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands19-workshop-disentangling-pose-from-appearance-in-monochrome-hand-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [HANDS19 Workshop] Disentangling Pose from Appearance in Monochrome Hand Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands19-workshop-rgb-based-3d-hand-pose-estimation-via-privileged-learning-with-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [HANDS19 Workshop] RGB-based 3D Hand Pose Estimation via Privileged Learning with Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands19-workshop-explicit-pose-deformation-learning-for-tracking-human-poses-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [HANDS19 Workshop] Explicit Pose Deformation Learning for Tracking Human Poses. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands19-workshop-hand-pose-ensemble-learning-based-on-grouping-features-of-hand-point-sets-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [HANDS19 Workshop] Hand Pose Ensemble Learning based on Grouping Features of Hand Point Sets. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2019 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2019 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#disentangling-latent-hands-for-image-synthesis-and-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Disentangling Latent Hands for Image Synthesis and Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#point-to-pose-voting-based-hand-pose-estimation-using-residual-permutation-equivariant-layer-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation Equivariant Layer. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ho-unified-egocentric-recognition-of-3d-hand-object-poses-and-interactions-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • H•O: Unified Egocentric Recognition of 3D Hand-Object Poses and Interactions. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-3d-hand-pose-estimation-pdf-code-oral-best-paper-finalists" class="md-nav__link">
    <span class="md-ellipsis">
      • Self supervised 3D hand pose estimation. [PDF] [Code] (Oral) (Best Paper Finalists)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crossinfonet-multi-task-information-sharing-based-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • CrossInfoNet: Multi-Task Information Sharing Based Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#expressive-body-capture-3d-hands-face-and-body-from-a-single-image-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-joint-reconstruction-of-hands-and-manipulated-objects-pdf-code-code-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning joint reconstruction of hands and manipulated objects. [PDF] [Code] [Code] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-shape-and-pose-estimation-from-a-single-rgb-image-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Shape and Pose Estimation from a Single RGB Image. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-shape-and-pose-from-images-in-the-wild-pdf-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Shape and Pose from Images in the Wild. [PDF] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pushing-the-envelope-for-rgb-based-dense-3d-hand-pose-estimation-via-neural-rendering-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Pushing the Envelope for RGB-based Dense 3D Hand Pose Estimation via Neural Rendering. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monocular-total-capture-posing-face-body-and-hands-in-the-wild-pdf-project-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Monocular Total Capture: Posing Face, Body, and Hands in the Wild. [PDF] [Project] [Code] (Oral)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-others" class="md-nav__link">
    <span class="md-ellipsis">
      2019 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2019 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2019-siggraph-interactive-hand-pose-estimation-using-a-stretch-sensing-soft-glove-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 SIGGRAPH] Interactive Hand Pose Estimation using a Stretch-Sensing Soft Glove. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-siggraph-interactionfusion-real-time-reconstruction-of-hand-poses-and-deformable-objects-in-hand-object-interactions-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 SIGGRAPH] InteractionFusion: Real-time Reconstruction of Hand Poses and Deformable Objects in Hand-object Interactions. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-siggraph-real-time-pose-and-shape-reconstruction-of-two-interacting-hands-with-a-single-depth-camera-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 SIGGRAPH] Real-time pose and shape reconstruction of two interacting hands with a single depth camera. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-fg-deep-conditional-variational-estimation-for-depth-based-hand-poses-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 FG] Deep Conditional Variational Estimation for Depth-Based Hand Poses. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-unified-2d-and-3d-hand-pose-estimation-from-a-single-visible-or-x-ray-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] Unified 2D and 3D Hand Pose Estimation from a Single Visible or X-ray Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-tagan-tonality-aligned-generative-adversarial-networks-for-realistic-handpose-synthesis-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] TAGAN: Tonality Aligned Generative Adversarial Networks for Realistic HandPose Synthesis. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-single-image-3d-hand-reconstruction-with-mesh-convolutions-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] Single Image 3D Hand Reconstruction with Mesh Convolutions. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-adaptive-graphical-model-network-for-2d-handpose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] Adaptive Graphical Model Network for 2D Handpose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-srn-stacked-regression-network-for-real-time-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] SRN: Stacked Regression Network for Real-time 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-bmvc-end-to-end-3d-hand-pose-estimation-from-stereo-cameras-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 BMVC] End-to-End 3D Hand Pose Estimation from Stereo Cameras. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-accv-hand-pose-estimation-based-on-3d-residual-network-with-data-padding-and-skeleton-steadying-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 ACCV] Hand Pose Estimation Based on 3D Residual Network with Data Padding and Skeleton Steadying. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-icassp-cascaded-point-network-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 ICASSP] Cascaded Point Network for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-icassp-a-novel-framework-of-hand-localization-and-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 ICASSP] A Novel Framework of Hand Localization and Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-icra-vision-based-teleoperation-of-shadow-dexterous-hand-using-end-to-end-deep-neural-network-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 ICRA] Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2019-wacv-murauer-mapping-unlabeled-real-data-for-label-austerity-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2019 WACV] MURAUER: Mapping Unlabeled Real Data for Label AUstERity. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2018 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2018 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#handmap-robust-hand-pose-estimation-via-intermediate-dense-guidance-map-supervision-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • HandMap: Robust Hand Pose Estimation via Intermediate Dense Guidance Map Supervision. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hbe-hand-branch-ensemble-network-for-real-time-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • HBE: Hand Branch Ensemble network for real time 3D hand pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#point-to-point-regression-pointnet-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Point-to-Point Regression PointNet for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weakly-supervised-3d-hand-pose-estimation-from-monocular-rgb-images-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Weakly-supervised 3D Hand Pose Estimation from Monocular RGB Images. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#joint-3d-tracking-of-a-deformable-object-in-interaction-with-a-hand-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Joint 3D tracking of a deformable object in interaction with a hand. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#occlusion-aware-hand-pose-estimation-using-hierarchical-mixture-density-network-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density Network. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-pose-estimation-via-latent-25d-heatmap-regression-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Pose Estimation via Latent 2.5D Heatmap Regression. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands18-workshop-adapting-egocentric-visual-hand-pose-estimation-towards-a-robot-controlled-exoskeleton-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands18 Workshop] Adapting Egocentric Visual Hand Pose Estimation Towards a Robot-Controlled Exoskeleton. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands18-workshop-estimating-2d-multi-hand-poses-from-single-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands18 Workshop] Estimating 2D Multi-Hand Poses From Single Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands18-workshop-task-oriented-hand-motion-retargeting-for-dexterous-manipulation-imitation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands18 Workshop] Task-Oriented Hand Motion Retargeting for Dexterous Manipulation Imitation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2018 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2018 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#first-person-hand-action-benchmark-with-rgb-d-videos-and-3d-hand-pose-annotations-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-pose-specific-representations-by-predicting-different-views-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning Pose Specific Representations by Predicting Different Views. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-pointnet-3d-hand-pose-estimation-using-point-sets-pdf-project-code-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand PointNet: 3D Hand Pose Estimation using Point Sets. [PDF] [Project] [Code] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dense-3d-regression-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Dense 3D Regression for Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modal-deep-variational-hand-pose-estimation-pdf-project-code-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Cross-modal Deep Variational Hand Pose Estimation. [PDF] [Project] [Code] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature-mapping-for-learning-fast-and-accurate-3d-pose-inference-from-synthetic-images-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ganerated-hands-for-real-time-3d-hand-tracking-from-monocular-rgb-pdf-supp-project-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB. [PDF] [Supp] [Project] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#v2v-posenet-voxel-to-voxel-prediction-network-for-accurate-3d-hand-and-human-pose-estimation-from-a-single-depth-map-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#depth-based-3d-hand-pose-estimation-from-current-achievements-to-future-goals-pdf-spotlight" class="md-nav__link">
    <span class="md-ellipsis">
      • Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future Goals. [PDF] (Spotlight)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#augmented-skeleton-space-transfer-for-depth-based-hand-pose-estimation-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • Augmented skeleton space transfer for depth-based hand pose estimation. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-humans-workshop-monocular-rgb-hand-pose-inference-from-unsupervised-refinable-nets-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [3D HUMANS Workshop] Monocular RGB Hand Pose Inference From Unsupervised Refinable Nets. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-others" class="md-nav__link">
    <span class="md-ellipsis">
      2018 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2018 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2018-ismar-hybrid-3d-hand-articulations-tracking-guided-by-classification-and-search-space-adaptation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ISMAR] Hybrid 3D Hand Articulations Tracking Guided by Classification and Search Space Adaptation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-accv-hand-pose-estimation-based-on-3d-residual-network-with-data-padding-and-skeleton-steadying-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ACCV] Hand Pose Estimation based on 3D Residual Network with Data Padding and Skeleton Steadying. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-accv-partially-occluded-hands-a-challenging-new-dataset-for-single-image-hand-pose-estimation-pdf-project-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ACCV] Partially Occluded Hands: A challenging new dataset for single-image hand pose estimation. [PDF] [Project] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-accv-domain-transfer-for-3d-pose-estimation-from-color-images-without-manual-annotations-pdf-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ACCV] Domain Transfer for 3D Pose Estimation from Color Images without Manual Annotations. [PDF] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-pcm-hand-pose-estimation-with-attention-and-sequence-network-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 PCM] Hand Pose Estimation with Attention-and-Sequence Network. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-pcm-mutiple-transfer-net-with-region-ensemble-for-deep-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 PCM] Mutiple Transfer Net with Region Ensemble for Deep Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icpr-local-regression-based-hourglass-network-for-hand-pose-estimation-from-a-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICPR] Local Regression Based Hourglass Network for Hand Pose Estimation from a Single Depth Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icpr-dynamic-projected-segmentation-networks-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICPR] Dynamic Projected Segmentation Networks for Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-3dv-deephps-end-to-end-estimation-of-3d-hand-pose-and-shape-by-learning-from-synthetic-depth-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 3DV] DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning from Synthetic Depth. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icip-networks-effectively-utilizing-2d-spatial-information-for-accurate-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICIP] Networks Effectively Utilizing 2D Spatial Information for Accurate 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icip-on-the-fusion-of-rgb-and-depth-information-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICIP] On the Fusion of RGB and Depth Information For Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-icip-fast-lifting-for-3d-hand-pose-estimation-in-arvr-applications-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 ICIP] Fast Lifting for 3D Hand Pose Estimation in AR/VR Applications. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-bmvc-structure-aware-3d-hourglass-network-for-hand-pose-estimation-from-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 BMVC] Structure-Aware 3D Hourglass Network for Hand Pose Estimation from Single Depth Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-bmvc-3d-hand-pose-estimation-using-simulation-and-partial-supervision-with-a-shared-latent-space-pdf-code-oral" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 BMVC] 3D Hand Pose Estimation using Simulation and Partial-Supervision with a Shared Latent Space. [PDF] [Code] (Oral)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-fg-kinematic-constrained-cascaded-autoencoder-for-real-time-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 FG] Kinematic Constrained Cascaded Autoencoder for Real-time Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-wacv-using-a-single-rgb-frame-for-real-time-3d-hand-pose-estimation-in-the-wild-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018 WACV] Using a single RGB frame for real time 3D hand pose estimation in the wild. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2017 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2017 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning-to-estimate-3d-hand-pose-from-single-rgb-images-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning to Estimate 3D Hand Pose from Single RGB Images. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-time-hand-tracking-under-occlusion-from-an-egocentric-rgb-d-sensor-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#robust-hand-pose-estimation-during-the-interaction-with-an-unknown-object-pdf-supp-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Robust Hand Pose Estimation during the Interaction with an Unknown Object. [PDF] [Supp] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-hand-articulations-by-hallucinating-heat-distribution-pdf-supp-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning Hand Articulations by Hallucinating Heat Distribution. [PDF] [Supp] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#low-dimensionality-calibration-through-local-anisotropic-scaling-for-robust-hand-model-personalization-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Low-Dimensionality Calibration through Local Anisotropic Scaling for Robust Hand Model Personalization. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands17-workshop-back-to-rgb-3d-tracking-of-hands-and-hand-object-interactions-based-on-short-baseline-stereo-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands17 Workshop] Back to RGB: 3D tracking of hands and hand-object interactions based on short-baseline stereo. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands17-workshop-deepprior-improving-fast-and-accurate-3d-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands17 Workshop] DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hands17-workshop-hand-pose-estimation-using-deep-stereovision-and-markov-chain-monte-carlo-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [Hands17 Workshop] Hand Pose Estimation Using Deep Stereovision and Markov-chain Monte Carlo. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2017 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2017 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hand-keypoint-detection-in-single-images-using-multiview-bootstrapping-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Keypoint Detection in Single Images using Multiview Bootstrapping. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crossing-nets-combining-gans-and-vaes-with-a-shared-latent-space-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Crossing Nets: Combining GANs and VAEs with a Shared Latent Space for Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#big-hand-22m-benchmark-hand-pose-data-set-and-state-of-the-art-analysis-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Big Hand 2.2M Benchmark: Hand Pose Data Set and State of the Art Analysis. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-convolutional-neural-networks-for-efficient-and-robust-hand-pose-estimation-from-single-depth-images-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation from Single Depth Images. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-others" class="md-nav__link">
    <span class="md-ellipsis">
      2017 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2017 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2017-3dv-simultaneous-hand-pose-and-skeleton-bone-lengths-estimation-from-a-single-depth-image-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 3DV] Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a Single Depth Image. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-3dv-how-to-refine-3d-hand-pose-estimation-from-unlabelled-depth-data-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 3DV] How to Refine 3D Hand Pose Estimation from Unlabelled Depth Data? [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-icip-region-ensemble-network-improving-convolutional-network-for-hand-pose-estimation-pdf-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 ICIP] Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation. [PDF] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-icip-a-hand-pose-tracking-benchmark-from-stereo-matching-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 ICIP] A Hand Pose Tracking Benchmark from Stereo Matching. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-siggraph-asia-articulated-distance-fields-for-ultra-fast-tracking-of-hands-interacting-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 SIGGRAPH Asia] Articulated distance fields for ultra-fast tracking of hands interacting. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-siggraph-asia-online-generative-model-personalization-for-hand-tracking-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 SIGGRAPH Asia] Online Generative Model Personalization for Hand Tracking. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-siggraph-asia-embodied-hands-modeling-and-capturing-hands-and-bodies-together-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 SIGGRAPH Asia] Embodied Hands: Modeling and Capturing Hands and Bodies Together. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-bmvc-hand-pose-learning-combining-deep-learning-and-hierarchical-refinement-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 BMVC] Hand Pose Learning: Combining Deep Learning and Hierarchical Refinement for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-bmvc-generative-3d-hand-tracking-with-spatially-constrained-pose-sampling-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 BMVC] Generative 3D Hand Tracking with Spatially Constrained Pose Sampling. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-icra-learning-a-deep-network-with-spherical-part-model-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 ICRA] Learning a deep network with spherical part model for 3D hand pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-fg-occlusion-aware-hand-pose-recovery-from-sequences-of-depth-images-pdf-slide" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 FG] Occlusion aware hand pose recovery from sequences of depth images. [PDF] [Slide]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-fg-3d-hand-object-pose-estimation-from-depth-with-convolutional-neural-networks-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017 FG] 3D Hand-Object Pose Estimation from Depth with Convolutional Neural Networks. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-eccv" class="md-nav__link">
    <span class="md-ellipsis">
      2016 ECCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2016 ECCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#spatial-attention-deep-net-with-partial-pso-for-hierarchical-hybrid-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hand-pose-estimation-from-local-surface-normals-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Hand Pose Estimation from Local Surface Normals. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-time-joint-tracking-of-a-hand-manipulating-an-object-from-rgb-d-input-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Real-time Joint Tracking of a Hand Manipulating an Object from RGB-D Input. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2016 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2016 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#robust-3d-hand-pose-estimation-in-single-depth-images-from-single-view-cnn-to-multi-view-cnns-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Robust 3D Hand Pose Estimation in Single Depth Images: From Single-View CNN to Multi-View CNNs. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deephand-robust-hand-pose-estimation-by-completing-a-matrix-imputed-with-deep-features-pdfproject" class="md-nav__link">
    <span class="md-ellipsis">
      • DeepHand: Robust Hand Pose Estimation by Completing a Matrix Imputed With Deep Features. [PDF][Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#efficiently-creating-3d-training-data-for-fine-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Efficiently Creating 3D Training Data for Fine Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fits-like-a-glove-rapid-and-reliable-hand-shape-personalization-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Fits Like a Glove: Rapid and Reliable Hand Shape Personalization. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-others" class="md-nav__link">
    <span class="md-ellipsis">
      2016 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2016 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2016-nips-disco-nets-dissimilarity-coefficient-networks-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 NIPS] DISCO Nets : Dissimilarity Coefficient Networks. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-accv-hand-pose-regression-via-a-classification-guided-approach-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 ACCV] Hand Pose Regression via A Classification-guided Approach. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-icpr-deep-learning-for-integrated-hand-detection-and-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 ICPR] Deep learning for integrated hand detection and pose estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-icpr-depth-based-3d-hand-pose-tracking-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 ICPR] Depth-based 3D hand pose tracking. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-ijcai-model-based-deep-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 IJCAI] Model-based Deep Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-siggraph-efficient-and-precise-interactive-hand-tracking-through-joint-continuous-optimization-of-pose-and-correspondences-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 SIGGRAPH] Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-siggraph-asia-sphere-meshes-for-real-time-hand-modeling-and-tracking-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016 SIGGRAPH Asia] Sphere-Meshes for Real-Time Hand Modeling and Tracking. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-iccv" class="md-nav__link">
    <span class="md-ellipsis">
      2015 ICCV
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2015 ICCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-a-feedback-loop-for-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Training a Feedback Loop for Hand Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opening-the-black-box-hierarchical-sampling-optimization-for-estimating-human-hand-pose-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#depth-based-hand-pose-estimation-data-methods-and-challenges-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • Depth-based hand pose estimation: data, methods, and challenges. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d-hand-pose-estimation-using-randomized-decision-forest-with-segmentation-index-points-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • 3D Hand Pose Estimation Using Randomized Decision Forest with Segmentation Index Points. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-collaborative-filtering-approach-to-real-time-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • A collaborative filtering approach to real-time hand pose estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lending-a-hand-detecting-hands-and-recognizing-activities-in-complex-egocentric-interactions-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#understanding-everyday-hands-in-action-from-rgb-d-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Understanding Everyday Hands in Action from RGB-D Images. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2015 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2015 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cascaded-hand-pose-regression-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Cascaded Hand Pose Regression. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fast-and-robust-hand-tracking-using-detection-guided-optimization-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Fast and Robust Hand Tracking Using Detection-Guided Optimization. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-an-efficient-model-of-hand-shape-variation-from-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • Learning an Efficient Model of Hand Shape Variation from Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-others" class="md-nav__link">
    <span class="md-ellipsis">
      2015 Others
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2015 Others">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2015-bmvc-hybrid-one-shot-3d-hand-pose-estimation-by-exploiting-uncertainties-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 BMVC] Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-bmvc-rule-of-thumb-deep-derotation-for-improved-fingertip-detection-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 BMVC] Rule of Thumb: Deep Derotation for Improved Fingertip Detection. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-chi-accurate-robust-and-flexible-real-time-hand-tracking-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 CHI] Accurate, Robust, and Flexible Real-time Hand Tracking. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-cvww-hands-deep-in-deep-learning-for-hand-pose-estimation-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 CVWW] Hands Deep in Deep Learning for Hand Pose Estimation. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-fg-combining-discriminative-and-model-based-approaches-for-hand-pose-estimation-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 FG] Combining Discriminative and Model Based Approaches for Hand Pose Estimation. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-sgp-robust-articulated-icp-for-real-time-hand-tracking-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015 SGP] Robust Articulated-ICP for Real-Time Hand Tracking. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2014-cvpr" class="md-nav__link">
    <span class="md-ellipsis">
      2014 CVPR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2014 CVPR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#realtime-and-robust-hand-tracking-from-depth-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Realtime and robust hand tracking from depth. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-regression-forest-structured-estimation-of-3d-articulated-hand-posture-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Latent regression forest: Structured estimation of 3d articulated hand posture. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#user-specific-hand-modeling-from-monocular-depth-sequences-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • User-specific hand modeling from monocular depth sequences. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evolutionary-quasi-random-search-for-hand-articulations-tracking-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • Evolutionary Quasi-random Search for Hand Articulations Tracking. [PDF] [Project]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2014-others-before" class="md-nav__link">
    <span class="md-ellipsis">
      2014 Others &amp; Before
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2014 Others & Before">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2014-siggraph-real-time-continuous-pose-recovery-of-human-hands-using-convolutional-networks-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2014 SIGGRAPH] Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2013-iccv-real-time-articulated-hand-pose-estimation-using-semi-supervised-transductive-regression-forests-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2013 ICCV] Real-time Articulated Hand Pose Estimation using Semi-supervised Transductive Regression Forests. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2013-iccv-interactive-markerless-articulated-hand-motion-tracking-using-rgb-and-depth-data-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2013 ICCV] Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2013-iccv-efficient-hand-pose-estimation-from-a-single-depth-image-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2013 ICCV] Efficient Hand Pose Estimation from a Single Depth Image. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2012-eccv-motion-capture-of-hands-in-action-using-discriminative-salient-points-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2012 ECCV] Motion Capture of Hands in Action using Discriminative Salient Points. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2012-eccv-hand-pose-estimation-and-hand-shape-classification-using-multi-layered-randomized-decision-forests" class="md-nav__link">
    <span class="md-ellipsis">
      • [2012 ECCV] Hand pose estimation and hand shape classification using multi-layered randomized decision forests.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2011-cvprw-real-time-hand-pose-estimation-using-depth-sensors-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2011 CVPRW] Real Time Hand Pose Estimation using Depth Sensors. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2011-bmvc-efficient-model-based-3d-tracking-of-hand-articulations-using-kinect-pdf-project-code" class="md-nav__link">
    <span class="md-ellipsis">
      • [2011 BMVC] Efficient Model-based 3D Tracking of Hand Articulations using Kinect. [PDF] [Project] [Code]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#theses" class="md-nav__link">
    <span class="md-ellipsis">
      Theses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Theses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2022-hand-analysis-from-depth-images-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2022] Hand Analysis From Depth Images. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2020-learning-without-labeling-for-3d-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2020] Learning without Labeling for 3D Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-computational-learning-for-hand-pose-estimation-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] Computational Learning for Hand Pose Estimation. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-3d-hand-pose-estimation-methods-datasets-and-challenges-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] 3D hand pose estimation: methods, datasets, and challenges. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-3d-hand-pose-estimation-using-convolutional-neural-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] 3D hand pose estimation using convolutional neural networks. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-3d-hand-pose-estimation-from-images-for-interactive-applications-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] 3D Hand Pose Estimation from Images for Interactive Applications. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-articulated-human-pose-estimation-in-unconstrained-images-and-videos-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] Articulated Human Pose Estimation in Unconstrained Images and Videos. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-real-time-generative-hand-modeling-and-tracking-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] Real-Time Generative Hand Modeling and Tracking. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2018-recovery-of-the-3d-virtual-human-monocular-estimation-of-3d-shape-and-pose-with-data-driven-priors-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2018] Recovery of the 3D Virtual Human: Monocular Estimation of 3D Shape and Pose with Data Driven Priors. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-human-segmentation-pose-estimation-and-applications-pdf-slides" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017] Human Segmentation, Pose Estimation and Applications. [PDF] [Slides]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2017-capturing-hand-object-interaction-and-reconstruction-of-manipulated-objects-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2017] Capturing Hand-Object Interaction and Reconstruction of Manipulated Objects. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-tracking-hands-in-action-for-gesture-based-computer-input-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016] Tracking Hands in Action for Gesture-based Computer Input. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-3d-hand-pose-regression-with-variants-of-decision-forests-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016] 3D hand pose regression with variants of decision forests. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-deep-learning-for-human-motion-analysis-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016] Deep Learning for Human Motion Analysis. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2016-real-time-hand-pose-estimation-for-human-computer-interaction-pdf-project" class="md-nav__link">
    <span class="md-ellipsis">
      • [2016] Real time hand pose estimation for human computer interaction. [PDF] [Project]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-efficient-tracking-of-the-3d-articulated-motion-of-human-hands-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015] Efficient Tracking of the 3D Articulated Motion of Human Hands. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-vision-based-hand-pose-estimation-and-gesture-recognition-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015] Vision-based hand pose estimation and gesture recognition. [PDF]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2015-localization-of-humans-in-images-using-convolutional-networks-pdf" class="md-nav__link">
    <span class="md-ellipsis">
      • [2015] Localization of Humans in Images Using Convolutional Networks. [PDF]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#datasets" class="md-nav__link">
    <span class="md-ellipsis">
      Datasets
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="awesome-hand-pose-estimation">Awesome Hand Pose Estimation</h1>
<p>A curated list of related resources for hand pose estimation. Feel free to <a href="#Contribute">contribute</a>!</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#arxiv-papers">arXiv Papers</a></li>
<li><a href="#journal-papers">Journal Papers</a></li>
<li><a href="#tpami--ijcv">TPAMI / IJCV</a></li>
<li><a href="#other-journals">Others</a></li>
<li><a href="#conference-papers">Conference Papers</a></li>
<li>2024: <a href="#2024-cvpr">CVPR</a>, <a href="#2024-eccv">ECCV</a>, <a href="#2024-others">Others</a></li>
<li>2023: <a href="#2023-cvpr">CVPR</a>, <a href="#2023-iccv">ICCV</a>, <a href="#2023-others">Others</a></li>
<li>2022: <a href="#2022-cvpr">CVPR</a>, <a href="#2022-eccv">ECCV</a>, <a href="#2022-others">Others</a></li>
<li>2021: <a href="#2021-cvpr">CVPR</a>, <a href="#2021-iccv">ICCV</a>, <a href="#2021-others">Others</a></li>
<li>2020: <a href="#2020-cvpr">CVPR</a>, <a href="#2020-eccv">ECCV</a>, <a href="#2020-others">Others</a></li>
<li>2019: <a href="#2019-cvpr">CVPR</a>, <a href="#2019-iccv">ICCV</a>, <a href="#2019-others">Others</a></li>
<li>2018: <a href="#2018-cvpr">CVPR</a>, <a href="#2018-eccv">ECCV</a>, <a href="#2018-others">Others</a></li>
<li>2017: <a href="#2017-cvpr">CVPR</a>, <a href="#2017-iccv">ICCV</a>, <a href="#2017-others">Others</a></li>
<li>2016: <a href="#2016-cvpr">CVPR</a>, <a href="#2016-eccv">ECCV</a>, <a href="#2016-others">Others</a></li>
<li>2015: <a href="#2015-cvpr">CVPR</a>, <a href="#2015-iccv">ICCV</a>, <a href="#2015-others">Others</a></li>
<li>2014: <a href="#2014-cvpr">CVPR</a>, <a href="#2014-others--before">Others &amp; Before</a></li>
<li><a href="#theses">Theses</a></li>
<li><a href="#datasets">Datasets</a></li>
<li><a href="#depth">Depth</a></li>
<li><a href="#rgbdepth">RGB+Depth</a></li>
<li><a href="#rgb">RGB</a></li>
<li><a href="#workshops">Workshops</a></li>
<li><a href="#challenges">Challenges</a></li>
<li><a href="#other-related-papers">Other Related Papers</a></li>
</ul>
<p>* indicates equal contribution</p>
<h2 id="evaluation">Evaluation</h2>
<p>See folder <a href="./evaluation"><code>evaluation</code></a> to get more details about performance evaluation for hand pose estimation.</p>
<h2 id="arxiv-papers">arXiv Papers</h2>
<h5 id="arxiv220604927-pushing-the-envelope-for-depth-based-semi-supervised-3d-hand-pose-estimation-with-consistency-training-pdf">• <a href="https://arxiv.org/abs/2303.15147">[arXiv:2206.04927]</a> Pushing the Envelope for Depth-Based Semi-Supervised 3D Hand Pose Estimation with Consistency Training.  <a href="https://arxiv.org/abs/2303.15147">[PDF]</a></h5>
<p><em>Mohammad Rezaei, Farnaz Farahanipad, Alex Dillhoff, Vassilis Athitsos</em></p>
<h5 id="arxiv220604927-ego2handspose-a-dataset-for-egocentric-two-hand-3d-global-pose-estimation-pdf">• <a href="https://arxiv.org/abs/2206.04927">[arXiv:2206.04927]</a> Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global Pose Estimation.  <a href="https://arxiv.org/abs/2206.04927">[PDF]</a></h5>
<p><em>Fanqing Lin, Tony Martinez</em></p>
<h5 id="arxiv220607117-trihorn-net-a-model-for-accurate-depth-based-3d-hand-pose-estimation-pdf-code">• <a href="https://arxiv.org/abs/2206.07117">[arXiv:2206.07117]</a> TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation.  <a href="https://arxiv.org/abs/2206.07117">[PDF]</a>  <a href="https://github.com/mrezaei92/TriHorn-Net">[Code]</a></h5>
<p><em>Mohammad Rezaei, Razieh Rastgoo, Vassilis Athitsos</em></p>
<h5 id="arxiv220204533-nimble-a-non-rigid-hand-model-with-bones-and-muscles-pdf">• <a href="https://arxiv.org/abs/2202.04533">[arXiv:2202.04533]</a> NIMBLE: A Non-rigid Hand Model with Bones and Muscles.  <a href="https://arxiv.org/pdf/2202.04533">[PDF]</a></h5>
<p><em>Yuwei Li, Longwen Zhang, Zesong Qiu, Yingwenqi Jiang, Yuyao Zhang, Nianyi Li, Yuexin Ma, Lan Xu, Jingyi Yu</em></p>
<h5 id="arxiv220109548-consistent-3d-hand-reconstruction-in-video-via-self-supervised-learning-pdf">• <a href="https://arxiv.org/abs/2201.09548">[arXiv:2201.09548]</a> Consistent 3D Hand Reconstruction in Video via self-supervised Learning.  <a href="https://arxiv.org/pdf/2201.09548">[PDF]</a></h5>
<p><em>Zhigang Tu, Zhisheng Huang, Yujin Chen, Di Kang, Linchao Bao, Bisheng Yang, Junsong Yuan</em></p>
<h5 id="arxiv211106500-dynamic-iterative-refinement-for-efficient-3d-hand-pose-estimation-pdf">• <a href="https://arxiv.org/abs/2111.06500">[arXiv:2111.06500]</a> Dynamic Iterative Refinement for Efficient 3D Hand Pose Estimation. <a href="https://arxiv.org/pdf/2111.06500">[PDF]</a></h5>
<p><em>John Yang, Yash Bhalgat, Simyung Chang, Fatih Porikli, Nojun Kwak</em></p>
<h5 id="arxiv210914744-the-object-at-hand-automated-editing-for-mixed-reality-video-guidance-from-hand-object-interactions-pdf">• <a href="https://arxiv.org/abs/2109.14744">[arXiv:2109.14744]</a> The Object at Hand: Automated Editing for Mixed Reality Video Guidance from Hand-Object Interactions. <a href="https://arxiv.org/pdf/2109.14744">[PDF]</a></h5>
<p><em>Yao Lu, Walterio W. Mayol-Cuevas</em></p>
<h5 id="arxiv210914657-understanding-egocentric-hand-object-interactions-from-hand-pose-estimation-pdf">• <a href="https://arxiv.org/abs/2109.14657">[arXiv:2109.14657]</a> Understanding Egocentric Hand-Object Interactions from Hand Pose Estimation. <a href="https://arxiv.org/pdf/2109.14657">[PDF]</a></h5>
<p><em>Yao Lu, Walterio W. Mayol-Cuevas</em></p>
<h5 id="arxiv210911747-a-multi-view-video-based-3d-hand-pose-estimation-pdf">• <a href="https://arxiv.org/abs/2109.11747">[arXiv:2109.11747]</a> A Multi-View Video-Based 3D Hand Pose Estimation. <a href="https://arxiv.org/pdf/2109.11747">[PDF]</a></h5>
<p><em>Leyla Khaleghi, Alireza Sepas Moghaddam, Joshua Marshall, Ali Etemad</em></p>
<h5 id="arxiv210813995-realistic-hands-a-hybrid-model-for-3d-hand-reconstruction-pdf-project">• <a href="https://arxiv.org/abs/2108.13995">[arXiv:2108.13995]</a> Realistic Hands: A Hybrid Model for 3D Hand Reconstruction. <a href="https://arxiv.org/pdf/2108.13995">[PDF]</a> <a href="https://hassony2.github.io/homan.html">[Project]</a></h5>
<p><em>Michael Seeber, Martin R. Oswald, Roi Poranne</em></p>
<h5 id="arxiv210807044-towards-unconstrained-joint-hand-object-reconstruction-from-rgb-videos-pdf-project-code">• <a href="https://arxiv.org/abs/2108.07044">[arXiv:2108.07044]</a> Towards unconstrained joint hand-object reconstruction from RGB videos. <a href="https://arxiv.org/pdf/2108.07044">[PDF]</a> <a href="https://hassony2.github.io/homan.html">[Project]</a>  <a href="https://github.com/hassony2/homan">[Code]</a></h5>
<p><em>Yana Hasson, Gül Varol, Ivan Laptev, Cordelia Schmid</em></p>
<h5 id="arxiv210700887-ho-3d_v3-improving-the-accuracy-of-hand-object-annotations-of-the-ho-3d-dataset-pdf">• <a href="https://arxiv.org/abs/2107.00887">[arXiv:2107.00887]</a> HO-3D_v3: Improving the Accuracy of Hand-Object Annotations of the HO-3D Dataset. <a href="https://arxiv.org/abs/2107.00887">[PDF]</a></h5>
<p><em>Shreyas Hampali, Sayan Deb Sarkar, Vincent Lepetit</em></p>
<h5 id="arxiv210605954-adversarial-motion-modelling-helps-semi-supervised-hand-pose-estimation-pdf">• <a href="https://arxiv.org/abs/2106.05954">[arXiv:2106.05954]</a> Adversarial Motion Modelling helps Semi-supervised Hand Pose Estimation. <a href="https://arxiv.org/pdf/2106.05954">[PDF]</a></h5>
<p><em>Adrian Spurr, Pavlo Molchanov, Umar Iqbal, Jan Kautz, Otmar Hilliges</em></p>
<h5 id="arxiv210604324-contrastive-representation-learning-for-hand-shape-estimation-pdf-project-code-data">• <a href="https://arxiv.org/abs/2106.04324">[arXiv:2106.04324]</a> Contrastive Representation Learning for Hand Shape Estimation. <a href="https://arxiv.org/pdf/2106.04324">[PDF]</a>  <a href="https://lmb.informatik.uni-freiburg.de/projects/contra-hand/">[Project]</a>  <a href="https://github.com/lmb-freiburg/contra-hand">[Code]</a>  <a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/HanCo.en.html">[Data]</a></h5>
<p><em>Christian Zimmermann, Max Argus, Thomas Brox</em></p>
<h5 id="arxiv210414639-handsformer-keypoint-transformer-for-monocular-3d-pose-estimation-ofhands-and-object-in-interaction-pdf">• <a href="https://arxiv.org/abs/2104.14639">[arXiv:2104.14639]</a> HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation ofHands and Object in Interaction. <a href="https://arxiv.org/pdf/2104.14639">[PDF]</a></h5>
<p><em>Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, Vincent Lepetit</em></p>
<h5 id="arxiv210207067-fasthand-fast-hand-pose-estimation-from-a-monocular-camera-pdf">• <a href="https://arxiv.org/abs/2102.07067">[arXiv:2102.07067]</a> FastHand: Fast Hand Pose Estimation From A Monocular Camera. <a href="https://arxiv.org/pdf/2102.07067">[PDF]</a></h5>
<p><em>Shan An, Xiajie Zhang, Dong Wei, Haogang Zhu, Jianyu Yang, Konstantinos A. Tsintotas</em></p>
<h5 id="arxiv201211260-unsupervised-domain-adaptation-with-temporal-consistent-self-training-for-3d-hand-object-joint-reconstruction-pdf">• <a href="https://arxiv.org/abs/2012.11260">[arXiv:2012.11260]</a> Unsupervised Domain Adaptation with Temporal-Consistent Self-Training for 3D Hand-Object Joint Reconstruction. <a href="https://arxiv.org/pdf/2012.11260.pdf">[PDF]</a></h5>
<p><em>Mengshi Qi, Edoardo Remelli, Mathieu Salzmann, Pascal Fua</em></p>
<h5 id="arxiv200808324-frankmocap-fast-monocular-3d-hand-and-body-motion-capture-by-regression-and-integration-pdf-project">• <a href="https://arxiv.org/abs/2008.08324">[arXiv:2008.08324]</a> FrankMocap: Fast Monocular 3D Hand and Body Motion Capture by Regression and Integration. <a href="https://arxiv.org/pdf/2008.08324.pdf">[PDF]</a>  <a href="https://penincillin.github.io/frank_mocap">[Project]</a></h5>
<p><em>Yu Rong, Takaaki Shiratori, Hanbyul Joo</em></p>
<h5 id="arxiv200605927-recent-advances-in-3d-object-and-hand-pose-estimation-pdf">• <a href="https://arxiv.org/abs/2006.05927">[arXiv:2006.05927]</a> Recent Advances in 3D Object and Hand Pose Estimation. <a href="https://arxiv.org/pdf/2006.05927.pdf">[PDF]</a></h5>
<p><em>Vincent Lepetit</em></p>
<h5 id="arxiv200108047-attention-a-lightweight-2d-hand-pose-estimation-approach-pdf">• <a href="https://arxiv.org/abs/2001.08047">[arXiv:2001.08047]</a> Attention! A Lightweight 2D Hand Pose Estimation Approach. <a href="https://arxiv.org/pdf/2001.08047.pdf">[PDF]</a></h5>
<p><em>Nicholas Santavas, Ioannis Kansizoglou, Loukas Bampis, Evangelos Karakasis, Antonios Gasteratos</em></p>
<h5 id="arxiv200100702-handaugment-a-simple-data-augmentation-method-for-depth-based-3d-hand-pose-estimation-pdf-code">• <a href="https://arxiv.org/abs/2001.00702">[arXiv:2001.00702]</a> HandAugment: A Simple Data Augmentation Method for Depth-Based 3D Hand Pose Estimation. <a href="https://arxiv.org/pdf/2001.00702.pdf">[PDF]</a> <a href="https://github.com/wozhangzhaohui/HandAugment">[Code]</a></h5>
<p><em>Zhaohui Zhang, Shipeng Xie, Mingxiu Chen, Haichao Zhu</em></p>
<h5 id="arxiv191212436-silhouette-net-3d-hand-pose-estimation-from-silhouettes-pdf">• <a href="https://arxiv.org/abs/1912.12436">[arXiv:1912.12436]</a> Silhouette-Net: 3D Hand Pose Estimation from Silhouettes. <a href="https://arxiv.org/pdf/1912.12436.pdf">[PDF]</a></h5>
<p><em>Kuo-Wei Lee, Shih-Hung Liu, Hwann-Tzong Chen, Koichi Ito</em></p>
<h5 id="arxiv191112501-an-end-to-end-framework-for-unconstrained-monocular-3d-hand-pose-estimation-pdf">• <a href="https://arxiv.org/abs/1911.12501">[arXiv:1911.12501]</a> An End-to-end Framework for Unconstrained Monocular 3D Hand Pose Estimation. <a href="https://arxiv.org/pdf/1911.12501.pdf">[PDF]</a></h5>
<p><em>Sanjeev Sharma, Shaoli Huang, Dacheng Tao</em></p>
<h5 id="arxiv191201875-graphposegan-3d-hand-pose-estimation-from-a-monocular-rgb-image-via-adversarial-learning-on-graphs-pdf">• <a href="https://arxiv.org/abs/1912.01875">[arXiv:1912.01875]</a> GraphPoseGAN: 3D Hand Pose Estimation from a Monocular RGB Image via Adversarial Learning on Graphs. <a href="https://arxiv.org/pdf/1912.01875.pdf">[PDF]</a></h5>
<p><em>Yiming He, Wei Hu, Siyuan Yang, Xiaochao Qu, Pengfei Wan, Zongming Guo</em></p>
<h5 id="arxiv191107424-capturing-hand-articulations-using-recurrent-neural-network-for-3d-hand-pose-estimation-pdf">• <a href="https://arxiv.org/abs/1911.07424">[arXiv:1911.07424]</a> Capturing Hand Articulations using Recurrent Neural Network for 3D Hand Pose Estimation. <a href="https://arxiv.org/pdf/1911.07424.pdf">[PDF]</a></h5>
<p><em>Cheol-hwan Yoo, Seung-wook Kim, Seo-won Ji, Yong-goo Shin, Sung-jea Ko</em></p>
<h5 id="arxiv180700898-model-based-hand-pose-estimation-for-generalized-hand-shape-with-appearance-normalization-pdf">• <a href="https://arxiv.org/abs/1807.00898">[arXiv:1807.00898]</a> Model-based Hand Pose Estimation for Generalized Hand Shape with Appearance Normalization. <a href="https://arxiv.org/pdf/1807.00898.pdf">[PDF]</a></h5>
<p><em>Jan Wöhlke, Shile Li, Dongheui Lee</em></p>
<h5 id="arxiv170509606-end-to-end-global-to-local-cnn-learning-for-hand-pose-recovery-in-depth-data-pdf">• <a href="https://arxiv.org/abs/1705.09606">[arXiv:1705.09606]</a> End-to-end Global to Local CNN Learning for Hand Pose Recovery in Depth data. <a href="https://arxiv.org/pdf/1705.09606.pdf">[PDF]</a></h5>
<p><em>Meysam Madadi, Sergio Escalera, Xavier Baro, Jordi Gonzalez</em></p>
<h5 id="arxiv170402224-hand3d-hand-pose-estimation-using-3d-neural-network-pdf-project">• <a href="https://arxiv.org/abs/1704.02224">[arXiv:1704.02224]</a> Hand3D: Hand Pose Estimation using 3D Neural Network. <a href="https://arxiv.org/pdf/1704.02224.pdf">[PDF]</a>  <a href="http://www.idengxm.com/hand3d/index.html">[Project]</a></h5>
<p><em>Xiaoming Deng*, Shuo Yang*, Yinda Zhang*, Ping Tan, Liang Chang, Hongan Wang</em></p>
<h5 id="arxiv161200596-learning-to-search-on-manifolds-for-3d-pose-estimation-of-articulated-objects-pdf">• <a href="https://arxiv.org/abs/1612.00596">[arXiv:1612.00596]</a> Learning to Search on Manifolds for 3D Pose Estimation of Articulated Objects. <a href="https://arxiv.org/pdf/1612.00596.pdf">[PDF]</a></h5>
<p><em>Yu Zhang, Chi Xu, Li Cheng</em></p>
<p><a href="#contents">[back to top]</a></p>
<h2 id="journal-papers">Journal Papers</h2>
<h3 id="tpami-ijcv">TPAMI / IJCV</h3>
<h5 id="2024-tpami-evhandpose-event-based-3d-hand-pose-estimation-with-sparse-supervision-pdf">• [2024 TPAMI] EvHandPose: Event-Based 3D Hand Pose Estimation With Sparse Supervision. <a href="https://ieeexplore.ieee.org/document/10478195/">[PDF]</a></h5>
<p><em>Jianping Jiang, Jiahe Li, Baowen Zhang, Xiaoming Deng, Boxin Shi</em></p>
<h5 id="2024-tpami-learning-a-contact-potential-field-for-modeling-the-hand-object-interaction-pdf">• [2024 TPAMI] Learning a Contact Potential Field for Modeling the Hand-Object Interaction. <a href="https://ieeexplore.ieee.org/document/10478277/">[PDF]</a></h5>
<p><em>Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Junming Zhang, Jiefeng Li, Cewu Lu</em></p>
<h5 id="2023-tpami-consistent-3d-hand-reconstruction-in-video-via-self-supervised-learning-pdf">• [2023 TPAMI] Consistent 3D Hand Reconstruction in Video via self-supervised Learning. <a href="https://arxiv.org/pdf/2201.09548">[PDF]</a></h5>
<p><em>Zhigang Tu, Zhisheng Huang, Yujin Chen, Di Kang, Linchao Bao, Bisheng Yang, Junsong Yuan</em></p>
<h5 id="2022-tpami-recurrent-3d-hand-pose-estimation-using-cascaded-pose-guided-3d-alignments-pdf">• [2022 TPAMI] Recurrent 3D Hand Pose Estimation Using Cascaded Pose-guided 3D Alignments. <a href="https://ieeexplore.ieee.org/document/9736619/">[PDF]</a></h5>
<p><em>Xiaoming Deng, Dexin Zuo, Yinda Zhang, Zhaopeng Cui, Jian Cheng, Ping Tan, Liang Chang, Marc Pollefeys, Sean Fanello, Hongan Wang</em></p>
<h5 id="2021-tpami-handvoxnet-3d-hand-shape-and-pose-estimation-using-voxel-based-neural-networks-pdf">• [2021 TPAMI] HandVoxNet++: 3D Hand Shape and Pose Estimation using Voxel-Based Neural Networks. <a href="https://arxiv.org/abs/2107.01205">[PDF]</a></h5>
<p><em>Jameel Malik, Soshi Shimada, Ahmed Elhayek, Sk Aziz Ali, Christian Theobalt, Vladislav Golyanik, Didier Stricker</em></p>
<h5 id="2020-tpami-3d-hand-pose-estimation-using-synthetic-data-and-weakly-labeled-rgb-images-pdf">• [2020 TPAMI] 3D Hand Pose Estimation Using Synthetic Data and Weakly Labeled RGB Images. <a href="https://ieeexplore.ieee.org/document/9091090">[PDF]</a></h5>
<p><em>Yujun Cai, Liuhao Ge, Jianfei Cai, Nadia Magnenat-Thalmann, Junsong Yuan</em></p>
<h5 id="2019-tpami-generalized-feedback-loop-for-joint-hand-object-pose-estimation-pdf-project">• [2019 TPAMI] Generalized Feedback Loop for Joint Hand-Object Pose Estimation. <a href="https://arxiv.org/pdf/1903.10883">[PDF]</a> <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/research-projects/joint-3d-hand-object-pose-estimation/">[Project]</a></h5>
<p><em>Markus Oberweger, Paul Wohlhart, Vincent Lepetit</em></p>
<h5 id="2019-tpami-feature-boosting-network-for-3d-pose-estimation-pdf">• [2019 TPAMI] Feature Boosting Network For 3D Pose Estimation. <a href="https://ieeexplore.ieee.org/document/8621059">[PDF]</a></h5>
<p><em>Jun Liu, Henghui Ding, Amir Shahroudy, Ling-Yu Duan, Xudong Jiang, Gang Wang, Alex C. Kot</em></p>
<h5 id="2018-tpami-opening-the-black-box-hierarchical-sampling-optimization-for-hand-pose-estimation-pdf">• [2018 TPAMI] Opening the Black Box: Hierarchical Sampling Optimization for Hand Pose Estimation. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8386667">[PDF]</a></h5>
<p><em>Danhang Tang*, Qi Ye*, Shanxin Yuan, Jonathan Taylor, Pushmeet Kohli, Cem Keskin, Tae-Kyun Kim, Jamie Shotton</em></p>
<h5 id="2018-ijcv-depth-based-hand-pose-estimation-methods-data-and-challenges-pdf-project-code">• [2018 IJCV] Depth-Based Hand Pose Estimation: Methods, Data, and Challenges. <a href="https://link.springer.com/content/pdf/10.1007%2Fs11263-018-1081-7.pdf">[PDF]</a>  <a href="http://arrummzen.net/#HandData">[Project]</a> <a href="https://github.com/jsupancic/deep_hand_pose">[Code]</a></h5>
<p><em>James Steven Supančič III, Grégory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan</em></p>
<h5 id="2018-tpami-real-time-3d-hand-pose-estimation-with-3d-convolutional-neural-networks-pdf">• [2018 TPAMI] Real-time 3D Hand Pose Estimation with 3D Convolutional Neural Networks. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8338122">[PDF]</a></h5>
<p><em>Liuhao Ge, Hui Liang, Junsong Yuan and Daniel Thalmann</em></p>
<h5 id="2016-ijcv-lie-x-depth-image-based-articulated-object-pose-estimation-tracking-and-action-recognition-on-lie-groups-pdf-project">• [2016 IJCV] Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups. <a href="https://arxiv.org/pdf/1609.03773.pdf">[PDF]</a> <a href="https://web.bii.a-star.edu.sg/archive/machine_learning/Projects/behaviorAnalysis/Lie-X/Lie-X.html">[Project]</a></h5>
<p><em>Chi Xu, Lakshmi Narasimhan Govindarajan, Yu Zhang, Li Cheng</em></p>
<h5 id="2016-tpami-latent-regression-forest-structured-estimation-of-3d-hand-poses-pdf">• [2016 TPAMI] Latent Regression Forest: Structured Estimation of 3D Hand Poses. <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7539555">[PDF]</a></h5>
<p><em>Danhang Tang, Hyung Chang, Alykhan Tejani, Tae-Kyun Kim</em></p>
<h5 id="2016-ijcv-capturing-hands-in-action-using-discriminative-salient-points-and-physics-simulation-pdf-project">• [2016 IJCV] Capturing Hands in Action using Discriminative Salient Points and Physics Simulation. <a href="http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/IJCV_Hand_Object_Capture.pdf">[PDF]</a> <a href="http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/">[Project]</a></h5>
<p><em>Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo Aponte, Marc Pollefeys, Juergen Gall</em></p>
<h5 id="2015-ijcv-estimate-hand-poses-efficiently-from-single-depth-images-pdf-project-code">• [2015 IJCV] Estimate Hand Poses Efficiently from Single Depth Images. <a href="https://web.bii.a-star.edu.sg/~xuchi/pdf/XuEtAl_IJCV15.pdf">[PDF]</a> <a href="http://web.bii.a-star.edu.sg/~xuchi/dhand.htm">[Project]</a>  <a href="https://github.com/lzddzh/HandPoseEstimation">[Code]</a></h5>
<p><em>Chi Xu, Ashwin Nanjappa, Xiaowei Zhang, Li Cheng</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="other-journals">Other Journals</h3>
<h5 id="2023-eswa-trihorn-net-a-model-for-accurate-depth-based-3d-hand-pose-estimation-pdf-code">• [2023 ESWA] TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation.  <a href="https://www.sciencedirect.com/science/article/pii/S0957417423004232">[PDF]</a>  <a href="https://github.com/mrezaei92/TriHorn-Net">[Code]</a></h5>
<p><em>Mohammad Rezaei, Razieh Rastgoo, Vassilis Athitsos</em></p>
<h5 id="2022-tip-a-dual-branch-self-boosting-framework-for-self-supervised-3d-hand-pose-estimation-pdf-code">• [2022 TIP] A Dual-Branch Self-Boosting Framework for Self-Supervised 3D Hand Pose Estimation. <a href="https://ieeexplore.ieee.org/document/9841448">[PDF]</a> <a href="https://github.com/RenFeiTemp/DSF">[Code]</a></h5>
<p><em>Pengfei Ren, Haifeng Sun, Jiachang Hao, Qi Qi, Jingyu Wang, Jianxin Liao</em></p>
<h5 id="2022-technologies-a-survey-on-gan-based-data-augmentation-for-hand-pose-estimation-problem-pdf">• [2022 Technologies] A Survey on GAN-Based Data Augmentation for Hand Pose Estimation Problem. <a href="https://www.mdpi.com/2227-7080/10/2/43/pdf?version=1647826385">[PDF]</a></h5>
<p><em>Farnaz Farahanipad, Mohammad Rezaei, Mohammad Sadegh Nasr, Farhad Kamangar, Vassilis Athitsos</em></p>
<h5 id="2022-tcsvt-3d-hand-pose-estimation-from-monocular-rgb-with-feature-interaction-module-pdf">• [2022 TCSVT] 3D Hand Pose Estimation from Monocular RGB with Feature Interaction Module. <a href="https://ieeexplore.ieee.org/document/9680673/">[PDF]</a></h5>
<p><em>Shaoxiang Guo, Eric Rigall, Yakun Ju, Junyu Dong</em></p>
<h5 id="2021-tip-hand-pose-understanding-with-large-scale-photo-realistic-rendering-dataset-pdf">• [2021 TIP] Hand Pose Understanding with Large-Scale Photo-Realistic Rendering Dataset. <a href="https://ieeexplore.ieee.org/document/9398571">[PDF]</a></h5>
<p><em>Xiaoming Deng, Yinda Zhang, Jian Shi, Yuying Zhu, Dachuan Cheng, Dexin Zuo, Zhaopeng Cui, Ping Tan, Liang Chang, Hongan Wang</em></p>
<h5 id="2021-tip-joint-hand-object-3d-reconstruction-from-a-single-image-with-cross-branch-feature-fusion-pdf">• [2021 TIP] Joint Hand-object 3D Reconstruction from a Single Image with Cross-branch Feature Fusion. <a href="https://arxiv.org/pdf/2006.15561.pdf">[PDF]</a></h5>
<p><em>Yujin Chen, Zhigang Tu, Di Kang, Ruizhi Chen, Linchao Bao, Zhengyou Zhang, Junsong Yuan</em></p>
<h5 id="2021-neurocomputing-spatial-aware-stacked-regression-network-for-real-time-3d-hand-pose-estimation-pdf">• [2021 Neurocomputing] Spatial-aware Stacked Regression Network for Real-time 3D Hand Pose Estimation. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231221000667">[PDF]</a></h5>
<p><em>Pengfei Ren, Haifeng Sun, Weiting Huang, Jiachang hao, Daixuan Cheng, Qi Qi, Jingyu Wang, Jianxin Liao</em></p>
<h5 id="2021-tmm-differentiable-spatial-regression-a-novel-method-for-3d-hand-pose-estimation-pdf-code">• [2021 TMM] Differentiable Spatial Regression: A Novel Method for 3D Hand Pose Estimation. <a href="https://drive.google.com/file/d/1kuhBSA4nzmJnIPeiTOTqC4w1YKQgjPBR/view?usp=share_link">[PDF]</a> <a href="https://github.com/IcarusWizard/PixelwiseRegression">[Code]</a></h5>
<p><em>Xingyuan Zhang, Fuhai Zhang</em></p>
<h5 id="2020-tip-weakly-supervised-learning-for-single-depth-based-hand-shape-recovery-pdf">• [2020 TIP] Weakly-supervised Learning for Single Depth based Hand Shape Recovery. <a href="https://ieeexplore.ieee.org/document/9262071">[PDF]</a></h5>
<p><em>Xiaoming Deng, Yuying Zhu, Yinda Zhang, Zhaopeng Cui, Ping Tan, Wentian Qu, Cuixia Ma, Hongan Wang</em></p>
<h5 id="2020-signal-process-image-commun-accurate-3d-hand-pose-estimation-network-utilizing-joints-information-pdf">• [2020 Signal Process Image Commun] Accurate 3D Hand Pose Estimation Network Utilizing Joints Information. <a href="https://www.sciencedirect.com/science/article/pii/S0923596520301831">[PDF]</a></h5>
<p><em>Xiongquan Zhang; Shiliang Huang; Zhongfu Ye</em></p>
<h5 id="2020-tcsvt-improve-regression-network-on-depth-hand-pose-estimation-with-auxiliary-variable-pdf">• [2020 TCSVT] Improve Regression Network on Depth Hand Pose Estimation with Auxiliary Variable. <a href="https://ieeexplore.ieee.org/abstract/document/9085372">[PDF]</a></h5>
<p><em>Lu Xu, Chen Hu, Ji’an Tao, Jianru Xue, Kuizhi Mei</em></p>
<h5 id="2020-tvcg-3d-hand-tracking-in-the-presence-of-excessive-motion-blur-pdf">• [2020 TVCG] 3D Hand Tracking in the Presence of Excessive Motion Blur. <a href="https://ieeexplore.ieee.org/document/8998145">[PDF]</a></h5>
<p><em>Gabyong Park, Antonis Argyros, Juyoung Lee, Woontack Woo</em></p>
<h5 id="2019-computers-graphics-simple-and-effective-deep-hand-shape-and-pose-regression-from-a-single-depth-image-pdf">• [2019 Computers &amp; Graphics] Simple and effective deep hand shape and pose regression from a single depth image. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0097849319301591">[PDF]</a></h5>
<p><em>Jameel Malik, Ahmed Elhayek, Fabrizio Nunnari, Didier Stricker</em></p>
<h5 id="2019-tip-srhandnet-real-time-2d-hand-pose-estimation-with-simultaneous-region-localization-pdf-project">• [2019 TIP] SRHandNet: Real-time 2D Hand Pose Estimation with Simultaneous Region Localization. <a href="https://yangangwang.com/papers/WANG-SRH-2019-11.pdf">[PDF]</a> <a href="https://yangangwang.com/papers/WANG-SRH-2019-07.html">[Project]</a></h5>
<p><em>Yangang Wang, Baowen Zhang, Cong Peng</em></p>
<h5 id="2019-sensors-whsp-net-a-weakly-supervised-approach-for-3d-hand-shape-and-pose-recovery-from-a-single-depth-image-pdf">• [2019 Sensors] WHSP-Net: A Weakly-Supervised Approach for 3D Hand Shape and Pose Recovery from a Single Depth Image. <a href="https://www.mdpi.com/1424-8220/19/17/3784">[PDF]</a></h5>
<p><em>Jameel Malik*, Ahmed Elhayek*, Didier Stricker</em></p>
<h5 id="2019-ra-l-variational-object-aware-3d-hand-pose-from-a-single-rgb-image-pdf-code">• [2019 <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7083369">RA-L</a>] Variational Object-aware 3D Hand Pose from a Single RGB Image. <a href="https://www.researchgate.net/profile/Yida_Wang/publication/334639748_Variational_Object-aware_3D_Hand_Pose_from_a_Single_RGB_Image/links/5d3a1a41a6fdcc370a6048df/Variational-Object-aware-3D-Hand-Pose-from-a-Single-RGB-Image.pdf">[PDF]</a>  <a href="https://github.com/wangyida/VO-handpose">[Code]</a></h5>
<p><em>Yafei Gao*, Yida Wang*, Pietro Falco, Nassir Navab, Federico Tombari</em></p>
<h5 id="2018-pr-a-survey-on-3d-hand-pose-estimation-cameras-methods-and-datasets-pdf">• [2018 PR] A Survey on 3D Hand Pose Estimation: Cameras, Methods, and Datasets. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320319301724">[PDF]</a></h5>
<p><em>Rui Li, Zhenyu Liu, Jianrong Tan</em></p>
<h5 id="2018-neurocomputing-a-crnn-module-for-hand-pose-estimation-pdf">• [2018 Neurocomputing] A CRNN module for hand pose estimation. <a href="https://www.sciencedirect.com/science/article/pii/S0925231218315273#!">[PDF]</a></h5>
<p><em>Zhongxu Hu, Youmin Hu, Jie Liu, Bo Wu, Dongmin Han, Thomas Kurfess</em></p>
<h5 id="2018-ivc-large-scale-multiview-3d-hand-pose-dataset-pdf-project">• [2018 IVC] Large-scale Multiview 3D Hand Pose Dataset. <a href="https://arxiv.org/pdf/1707.03742.pdf">[PDF]</a>  <a href="http://www.rovit.ua.es/dataset/mhpdataset/">[Project]</a></h5>
<p><em>Francisco Gomez-Donoso, Sergio Orts-Escolano and Miguel Cazorla</em></p>
<h5 id="2018-tcsvt-mask-pose-cascaded-cnn-for-2d-hand-pose-estimation-from-single-color-image-pdf-project-code">• [2018 TCSVT] Mask-pose Cascaded CNN for 2D Hand Pose Estimation from Single Color Image. <a href="https://www.yangangwang.com/papers/WANG-MCC-2018-10.pdf">[PDF]</a>  <a href="https://www.yangangwang.com/papers/WANG-MCC-2018-10.html">[Project]</a>  <a href="https://www.yangangwang.com/papers/WANG-MCC-2018-10.html">[Code]</a></h5>
<p><em>Yangang Wang, Cong Peng and Yebin Liu</em></p>
<h5 id="2018-ivc-top-down-model-fitting-for-hand-pose-recovery-in-sequences-of-depth-images-pdf">• [2018 IVC] Top-down model fitting for hand pose recovery in sequences of depth images. <a href="https://www.sciencedirect.com/science/article/pii/S0262885618301513#aep-article-footnote-id1">[PDF]</a></h5>
<p><em>Meysam Madadi, Sergio Escalera, Alex Carruesco, Carlos Andujar, Xavier Baró, Jordi Gonzàlez</em></p>
<h5 id="2018-tcyb-context-aware-deep-spatio-temporal-network-for-hand-pose-estimation-from-depth-images-pdf">• [2018 TCYB] Context-Aware Deep Spatio-Temporal Network for Hand Pose Estimation from Depth Images. <a href="https://arxiv.org/pdf/1810.02994.pdf">[PDF]</a></h5>
<p><em>Yiming Wu, Wei Ji, Xi Li, Gang Wang, Jianwei Yin, Fei Wu</em></p>
<h5 id="2018-ieee-access-shpr-net-deep-semantic-hand-pose-regression-from-point-clouds-pdf-project">• [2018 IEEE Access] SHPR-Net: Deep Semantic Hand Pose Regression From Point Clouds. <a href="https://ieeexplore.ieee.org/document/8425735/">[PDF]</a>  <a href="https://sites.google.com/view/xinghaochen/projects/SHPR-Net">[Project]</a></h5>
<p><em>Xinghao Chen, Guijin Wang, Cairong Zhang, Tae-Kyun Kim, Xiangyang Ji</em></p>
<h5 id="2018-neurocomputing-pose-guided-structured-region-ensemble-network-for-cascaded-hand-pose-estimation-pdf-project-code">• [2018 Neurocomputing]  Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation. <a href="https://arxiv.org/pdf/1708.03416.pdf">[PDF]</a>  <a href="https://sites.google.com/view/xinghaochen/projects/Pose-REN">[Project]</a>  <a href="https://github.com/xinghaochen/Pose-REN">[Code]</a></h5>
<p><em>Xinghao Chen, Guijin Wang, Hengkai Guo, Cairong Zhang</em></p>
<h5 id="2018-pr-learning-a-deep-network-with-spherical-part-model-for-3d-hand-pose-estimation-pdf">• [2018 PR]  Learning a deep network with spherical part model for 3D hand pose estimation. <a href="https://www.sciencedirect.com/science/article/pii/S0031320318300839">[PDF]</a></h5>
<p><em>Tzu-Yang Chen, Pai-Wen Ting, Min-Yu Wu, Li-Chen Fu</em></p>
<h5 id="2018-tip-robust-3d-hand-pose-estimation-from-single-depth-images-using-multi-view-cnns-pdf">• [2018 TIP] Robust 3D Hand Pose Estimation from Single Depth Images using Multi-View CNNs. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8357595">[PDF]</a></h5>
<p><em>Liuhao Ge, Hui Liang, Junsong Yuan and Daniel Thalmann</em></p>
<h5 id="2018-jvci-region-ensemble-network-towards-good-practices-for-deep-3d-hand-pose-estimation-pdf-code">• [2018 JVCI] Region Ensemble Network: Towards Good Practices for Deep 3D Hand Pose Estimation. <a href="https://www.sciencedirect.com/science/article/pii/S1047320318300816">[PDF]</a> <a href="https://github.com/guohengkai/region-ensemble-network">[Code]</a></h5>
<p><em>Guijin Wang, Xinghao Chen*, Hengkai Guo*, Cairong Zhang</em></p>
<h5 id="2017-tcyb-hough-forest-with-optimized-leaves-for-global-hand-pose-estimation-with-arbitrary-postures-pdf">• [2017 TCYB] Hough Forest with Optimized Leaves for Global Hand Pose Estimation with Arbitrary Postures. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8237190">[PDF]</a></h5>
<p><em>Hui Liang, Junsong Yuan, J. Lee, Liuhao Ge and Daniel Thalmann</em></p>
<h5 id="2017-tcsvt-robust-rgb-d-hand-tracking-using-deep-learning-priors-pdf">• [2017 TCSVT] Robust RGB-D Hand Tracking Using Deep Learning Priors. <a href="http://ieeexplore.ieee.org/abstract/document/7955084/">[PDF]</a></h5>
<p><em>Jordi Sanchez-Riera, Kathiravan Srinivasan, Kai-Lung Hua, Wen-Huang Cheng, M. Anwar Hossain, and Mohammed F. Alhamid</em></p>
<h5 id="2017-cviu-hand-pose-estimation-through-semi-supervised-and-weakly-supervised-learning-pdf">• [2017 CVIU] Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning. <a href="https://arxiv.org/pdf/1511.06728.pdf">[PDF]</a></h5>
<p><em>Natalia Neverova, Christian Wolf, Florian Nebout, Graham Taylor</em></p>
<h5 id="2017-neurocomputing-multi-task-multi-domain-learning-application-to-semantic-segmentation-and-pose-regression-pdf">• [2017 Neurocomputing] Multi-task, Multi-domain Learning: application to semantic segmentation and pose regression. <a href="http://liris.cnrs.fr/christian.wolf/papers/neurocomputing2017.pdf">[PDF]</a></h5>
<p><em>Damien Foururea, Rémi Emonet, Elisa Fromont, Damien Muselet, Natalia Neverova, Alain Trémeaua, Christian Wolf</em></p>
<h5 id="2016-cviu-guided-optimisation-through-classification-and-regression-for-hand-pose-estimation-pdf-project">• [2016 CVIU] Guided Optimisation through Classification and Regression for Hand Pose Estimation. <a href="http://www.krejov.com/uploads/2/4/0/5/24053627/1-s2.0-s107731421630193x-main.pdf">[PDF]</a> <a href="http://www.krejov.com/hand-pose-estimation.html">[Project]</a></h5>
<p><em>Philip Krejov, Andrew Gilbert, Richard Bowden</em></p>
<h5 id="2015-tcsvt-resolving-ambiguous-hand-pose-predictions-by-exploiting-part-correlations-pdf">• [2015 TCSVT] Resolving Ambiguous Hand Pose Predictions by Exploiting Part Correlations. <a href="https://ieeexplore.ieee.org/document/6926804/">[PDF]</a></h5>
<p><em>Hui Liang, Junsong Yuan, Daniel Thalmann</em></p>
<h5 id="2014-tmm-parsing-the-hand-in-depth-images-pdf-project-code">• [2014 TMM] Parsing the Hand in Depth Images. <a href="https://ieeexplore.ieee.org/document/6740010">[PDF]</a> <a href="https://sites.google.com/site/seraphlh/projects">[Project]</a>  <a href="https://github.com/shrekei/RandomDecisionForest">[Code]</a></h5>
<p><em>Hui Liang, Junsong Yuan, Daniel Thalmann</em></p>
<p><a href="#contents">[back to top]</a></p>
<h2 id="conference-papers">Conference Papers</h2>
<h3 id="2024-eccv">2024 ECCV</h3>
<h5 id="handdagt-a-denoising-adaptive-graph-transformer-for-3d-hand-pose-estimation-pdf">• HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/12018.pdf">[PDF]</a></h5>
<p><em>Wencan Cheng, Eunji Kim, Jong Hwan Ko</em></p>
<h5 id="dense-hand-objectho-graspnet-with-full-grasping-taxonomy-and-dynamics-pdf">• Dense Hand-Object(HO) GraspNet with Full Grasping Taxonomy and Dynamics. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10784.pdf">[PDF]</a></h5>
<p><em>Woojin Cho, Jihyun Lee, Minjae Yi, Minje Kim, Taeyun Woo, Donghwan Kim, Taewook Ha, Hyokeun Lee, Je-Hwan Ryu, Woontack Woo, Tae-Kyun (T-K) Kim</em></p>
<h5 id="3d-hand-pose-estimation-in-everyday-egocentric-images-pdf">• 3D Hand Pose Estimation in Everyday Egocentric Images. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10034.pdf">[PDF]</a></h5>
<p><em>Aditya Prakash, Ruisen Tu, Matthew Chang, Saurabh Gupta</em></p>
<h5 id="3d-reconstruction-of-objects-in-hands-without-real-world-3d-supervision-pdf">• 3D Reconstruction of Objects in Hands without Real World 3D Supervision. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10029.pdf">[PDF]</a></h5>
<p><em>Aditya Prakash, Matthew Chang, Matthew Jin, Ruisen Tu, Saurabh Gupta</em></p>
<h5 id="weakly-supervised-3d-hand-reconstruction-with-knowledge-prior-and-uncertainty-guidance-pdf">• Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10017.pdf">[PDF]</a></h5>
<p><em>Yufei Zhang, Jeffrey Kephart, Qiang Ji</em></p>
<h5 id="mlphand-real-time-multi-view-3d-hand-reconstruction-via-mlp-modeling-pdf">• MLPHand: Real Time Multi-View 3D Hand Reconstruction via MLP Modeling. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/09503.pdf">[PDF]</a></h5>
<p><em>Jian Yang, Jiakun Li, Guoming Li, Huaiyu Wu, Zhen Shen, Zhaoxin Fan</em></p>
<h5 id="are-synthetic-data-useful-for-egocentric-hand-object-interaction-detection-pdf">• Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08953.pdf">[PDF]</a></h5>
<p><em>Rosario Leonardi, Antonino Furnari, Francesco Ragusa, Giovanni Maria Farinella</em></p>
<h5 id="3d-hand-sequence-recovery-from-real-blurry-images-and-event-stream-pdf">• 3D Hand Sequence Recovery from Real Blurry Images and Event Stream. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07674.pdf">[PDF]</a></h5>
<p><em>JoonKyu Park, Gyeongsik Moon, Weipeng Xu, Evan Kaseman, Takaaki Shiratori, Kyoung Mu Lee</em></p>
<h5 id="coarse-to-fine-implicit-representation-learning-for-3d-hand-object-reconstruction-from-a-single-rgb-d-image-pdf">• Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06748.pdf">[PDF]</a></h5>
<p><em>Xingyu Liu, Pengfei Ren, Jingyu Wang, Qi Qi, Haifeng Sun, Zirui Zhuang, Jianxin Liao</em></p>
<h5 id="handdgp-camera-space-hand-mesh-prediction-with-differentiable-global-positioning-pdf-project-code">• HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05563.pdf">[PDF]</a> <a href="https://nianticlabs.github.io/handdgp/">[Project]</a> <a href="https://github.com/nianticlabs/HandDGP">[Code]</a></h5>
<p><em>Eugene Valassakis, Guillermo Garcia-Hernando</em></p>
<h5 id="controlling-the-world-by-sleight-of-hand-pdf">• Controlling the World by Sleight of Hand. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04442.pdf">[PDF]</a></h5>
<p><em>Sruthi Sudhakar, Ruoshi Liu, Basile Van Hoorick, Carl Vondrick, Richard Zemel</em></p>
<h5 id="learning-cross-hand-policies-of-high-dof-reaching-and-grasping-pdf">• Learning Cross-hand Policies of High-DOF Reaching and Grasping. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04377.pdf">[PDF]</a></h5>
<p><em>Qijin She, Shishun Zhang, Yunfan Ye, Ruizhen Hu, Kai Xu</em></p>
<h5 id="d-sco-dual-stream-conditional-diffusion-for-monocular-hand-held-object-reconstruction-pdf">• D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object Reconstruction. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04261.pdf">[PDF]</a></h5>
<p><em>Bowen Fu, Gu Wang, Chenyangguang Zhang, Yan Di, Ziqin Huang, Zhiying Leng, Fabian Manhardt, Xiangyang Ji, Federico Tombari</em></p>
<h5 id="nl2contact-natural-language-guided-3d-hand-object-contact-modeling-with-diffusion-model-pdf">• NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04090.pdf">[PDF]</a></h5>
<p><em>Zhongqun Zhang, Hengfei Wang, Ziwei Yu, Yihua Cheng, Angela Yao, Hyung Jin Chang</em></p>
<h5 id="benchmarks-and-challenges-in-pose-estimation-for-egocentric-hand-interactions-with-objects-pdf">• Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03682.pdf">[PDF]</a></h5>
<p><em>Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao</em></p>
<h5 id="on-the-utility-of-3d-hand-poses-for-action-recognition-pdf">• On the Utility of 3D Hand Poses for Action Recognition. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01025.pdf">[PDF]</a></h5>
<p><em>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</em></p>
<h5 id="attentionhand-text-driven-controllable-hand-image-generation-for-3d-hand-reconstruction-in-the-wild-pdf-project-code-oral">• AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild. <a href="https://arxiv.org/abs/2407.18034">[PDF]</a> <a href="https://redorangeyellowy.github.io/AttentionHand/">[Project]</a> <a href="https://github.com/redorangeyellowy/AttentionHand">[Code]</a> <em>(Oral)</em></h5>
<p><em>Junho Park, Kyeongbo Kong, Suk-Ju Kang</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2024-cvpr">2024 CVPR</h3>
<h5 id="physics-aware-hand-object-interaction-denoising">• Physics-aware Hand-object Interaction Denoising</h5>
<p><em>Haowen Luo, Yunze Liu, Li Yi</em></p>
<h5 id="hoidiffusion-generating-realistic-3d-hand-object-interaction-data">• HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data</h5>
<p><em>Mengqi Zhang, Yang Fu, Zheng Ding, Sifei Liu, Zhuowen Tu, Xiaolong Wang</em></p>
<h5 id="urhand-universal-relightable-hands-pdf">• URHand: Universal Relightable Hands. <a href="https://arxiv.org/pdf/2401.05334">[PDF]</a></h5>
<p><em>Zhaoxi Chen, Gyeongsik Moon, Kaiwen Guo, Chen Cao, Stanislav Pidhorskyi, Tomas Simon, Rohan Joshi, Yuan Dong, Yichen Xu, Bernardo Pires, He Wen, Lucas Evans, Bo Peng, Julia Buffalini, Autumn Trimble, Kevyn McPhail, Melissa Schoeller, Shoou-I Yu, Javier Romero, Michael Zollhöfer, Yaser Sheikh, Ziwei Liu, Shunsuke Saito</em></p>
<h5 id="oakink2-a-dataset-of-embodied-hands-object-manipulation-in-long-horizon-complex-task-completion">• OakInk2: A Dataset of Embodied Hands-Object Manipulation in Long-Horizon Complex Task Completion</h5>
<p><em>Xinyu Zhan, Lixin Yang, Yifei Zhao, Kangrui Mao, Hanlin Xu, Zenan Lin, Kailin Li, Cewu Lu</em></p>
<h5 id="interhandgen-two-hand-interaction-generation-via-cascaded-reverse-diffusion">• InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion</h5>
<p><em>Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun (T-K) Kim</em></p>
<h5 id="moho-learning-single-view-hand-held-object-reconstruction-with-multi-view-occlusion-aware-supervision-pdf">• MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision. <a href="https://arxiv.org/pdf/2310.11696">[PDF]</a></h5>
<p><em>Chenyangguang Zhang, Guanlong Jiao, Yan Di, Ziqin Huang, Gu Wang, Ruida Zhang, Bowen Fu, Federico Tombari, Xiangyang Ji</em></p>
<h5 id="ohta-one-shot-hand-avatar-via-data-driven-implicit-priors-pdf">• OHTA: One-shot Hand Avatar via Data-driven Implicit Priors. <a href="https://arxiv.org/pdf/2402.18969">[PDF]</a></h5>
<p><em>Xiaozheng Zheng, Chao Wen, Zhuo Su, Zeran Xu, Zhaohu Li, Yang Zhao, Zhou Xue</em></p>
<h5 id="handbooster-boosting-3d-hand-mesh-reconstruction-by-conditional-synthesis-and-sampling-of-hand-object-interactions">• HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions.</h5>
<p><em>Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu</em></p>
<h5 id="handdiff-3d-hand-pose-estimation-with-diffusion-on-image-point-cloud">• HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud.</h5>
<p><em>Wencan Cheng, Hao Tang, Luc Van Gool, Jong Hwan Ko</em> </p>
<h5 id="text2hoi-text-guided-3d-motion-generation-for-hand-object-interaction">• Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction.</h5>
<p><em>Junuk Cha, Jihyeon Kim, Jae Shin Yoon, Seungryul Baek</em></p>
<h5 id="both2hands-inferring-3d-hands-from-both-text-prompts-and-body-dynamics-pdf">• BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics. <a href="https://arxiv.org/pdf/2312.07937">[PDF]</a></h5>
<p><em>Wenqian Zhang, Molin Huang, Yuxuan Zhou, Juze Zhang, Jingyi Yu, Jingya Wang, Lan Xu</em></p>
<h5 id="gears-local-geometry-aware-hand-object-interaction-synthesis">• GEARS: Local Geometry-aware Hand-object Interaction Synthesis.</h5>
<p><em>Keyang Zhou, Bharat Bhatnagar, Jan Eric Lenssen, Gerard Pons-Moll</em></p>
<h5 id="a-simple-baseline-for-efficient-hand-mesh-reconstruction-pdf">• A Simple Baseline for Efficient Hand Mesh Reconstruction. <a href="https://arxiv.org/pdf/2403.01813">[PDF]</a></h5>
<p><em>Zhishan Zhou, Shihao.zhou, Zhi Lv, Minqiang Zou, Yao Tang, Jiajun Liang</em></p>
<h5 id="hold-category-agnostic-3d-reconstruction-of-interacting-hands-and-objects-from-video-pdf-code">• HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video. <a href="https://arxiv.org/pdf/2311.18448">[PDF]</a> <a href="https://github.com/zc-alexfan/hold">[Code]</a></h5>
<p><em>Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen, Michael J. Black, Otmar Hilliges</em></p>
<h5 id="ms-mano-enabling-hand-pose-tracking-with-biomechanical-constraints">• MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints.</h5>
<p><em>Pengfei Xie, Wenqiang Xu, Tutian Tang, Zhenjun Yu, Cewu Lu</em></p>
<h5 id="hoist-former-hand-held-objects-identification-segmentation-and-tracking-in-the-wild">• HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild.</h5>
<p><em>Supreeth Narasimhaswamy, Huy Nguyen, Lihan Huang, Minh Hoai</em></p>
<h5 id="bitt-bi-directional-texture-reconstruction-of-interacting-two-hands-from-a-single-image">• BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image.</h5>
<p><em>Minje Kim, Tae-Kyun Kim</em></p>
<h5 id="authentic-hand-avatar-from-a-phone-scan-via-universal-hand-model">• Authentic Hand Avatar from a Phone Scan via Universal Hand Model.</h5>
<p><em>Gyeongsik Moon, Weipeng Xu, Rohan Joshi, Chenglei Wu, Takaaki Shiratori</em></p>
<h5 id="reconstructing-hands-in-3d-with-transformers-pdf">• Reconstructing Hands in 3D with Transformers. <a href="https://arxiv.org/pdf/2312.05251">[PDF]</a></h5>
<p><em>Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, Jitendra Malik</em></p>
<h5 id="complementing-event-streams-and-rgb-frames-for-hand-mesh-reconstruction">• Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction.</h5>
<p><em>Jianping Jiang, Xinyu Zhou, Bingxuan Wang, Xiaoming Deng, Chao Xu, Boxin Shi</em></p>
<h5 id="single-to-dual-view-adaptation-for-egocentric-3d-hand-pose-estimation-pdf">• Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation. <a href="https://arxiv.org/pdf/2403.04381">[PDF]</a></h5>
<p><em>Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato</em></p>
<h5 id="hoisdf-constraining-3d-hand-object-pose-estimation-with-global-signed-distance-fields-pdf">• HOISDF: Constraining 3D Hand Object Pose Estimation with Global Signed Distance Fields. <a href="https://arxiv.org/pdf/2402.17062">[PDF]</a></h5>
<p><em>Haozhe Qi, Chen Zhao, Mathieu Salzmann, Alexander Mathis</em></p>
<h5 id="g-hop-generative-hand-object-prior-for-interaction-reconstruction-and-grasp-synthesis">• G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis.</h5>
<p><em>Yufei Ye, Abhinav Gupta, Kris Kitani, Shubham Tulsiani</em></p>
<h5 id="hhmr-holistic-hand-mesh-recovery-by-enhancing-the-multimodal-controllability-of-graph-diffusion-models">• HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models.</h5>
<p><em>Mengcheng Li, Hongwen Zhang, Yuxiang Zhang, Ruizhi Shao, Tao Yu, Yebin Liu</em></p>
<p><a href="#contents">[back to top]</a></p>
<h2 id="conference-papers_1">Conference Papers</h2>
<h3 id="2023-cvpr">2023 CVPR</h3>
<h5 id="a-probabilistic-attention-model-with-occlusion-aware-texture-regression-for-3d-hand-reconstruction-from-a-single-rgb-image">• A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image</h5>
<p><em>Zheheng Jiang, Hossein Rahmani, Sue Black, Bryan M. Williams</em></p>
<h5 id="a2j-transformer-anchor-to-joint-transformer-network-for-3d-interacting-hand-pose-estimation-from-a-single-rgb-image-pdf-code">• A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image. <a href="https://arxiv.org/pdf/2304.03635.pdf">[PDF]</a>  <a href="https://github.com/ChanglongJiangGit/A2J-Transformer">[Code]</a></h5>
<p><em>Changlong Jiang, Yang Xiao, Cunlin Wu, Mingyang Zhang, Jinghong Zheng, Zhiguo Cao, Joey Tianyi Zhou</em></p>
<h5 id="memahand-exploiting-mesh-mano-interaction-for-single-image-two-hand-reconstruction-pdf">• MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction. <a href="https://arxiv.org/pdf/2303.15718.pdf">[PDF]</a></h5>
<p><em>Congyi Wang, Feida Zhu, Shilei Wen</em></p>
<h5 id="arctic-a-dataset-for-dexterous-bimanual-hand-object-manipulation-project">• ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation. <a href="https://arctic.is.tue.mpg.de/">[Project]</a></h5>
<p><em>Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J. Black, Otmar Hilliges</em></p>
<h5 id="assemblyhands-towards-egocentric-activity-understanding-via-3d-hand-pose-estimation-project">• AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation. <a href="https://assemblyhands.github.io/">[Project]</a></h5>
<p><em>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, and Cem Keskin</em></p>
<h5 id="high-fidelity-3d-hand-shape-reconstruction-via-scalable-graph-frequency-decomposition">• High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition.</h5>
<p><em>Tianyu Luan, Yuanhao Zhai, Jingjing Meng, Zhong Li, Zhang Chen, Yi Xu, and Junsong Yuan.</em></p>
<h5 id="handnerf-neural-radiance-fields-for-animatable-interacting-hands-pdf">• HandNeRF: Neural Radiance Fields for Animatable Interacting Hands. <a href="https://arxiv.org/pdf/2303.13825.pdf">[PDF]</a></h5>
<p><em>Zhiyang Guo, Wengang Zhou, Min Wang, Li Li, Houqiang Li</em></p>
<h5 id="poem-reconstructing-hand-in-a-point-embedded-multi-view-stereo-pdf-code">• POEM: Reconstructing Hand in a Point Embedded Multi-view Stereo. <a href="https://arxiv.org/pdf/2304.04038.pdf">[PDF]</a> <a href="https://github.com/lixiny/POEM">[Code]</a></h5>
<p><em>Lixin Yang, Jian Xu, Licheng Zhong, Xinyu Zhan, Zhicheng Wang, Kejian Wu, Cewu Lu</em></p>
<h5 id="harp-personalized-hand-reconstruction-from-a-monocular-rgb-video-pdf-project">• HARP: Personalized Hand Reconstruction from a Monocular RGB Video. <a href="https://arxiv.org/pdf/2212.09530.pdf">[PDF]</a> <a href="https://korrawe.github.io/harp-project/">[Project]</a></h5>
<p><em>Korrawe Karunratanakul, Sergey Prokudin, Otmar Hilliges, Siyu Tang</em></p>
<h5 id="relightablehands-efficient-neural-relighting-of-articulated-hand-models-pdf-project">• RelightableHands: Efficient Neural Relighting of Articulated Hand Models. <a href="https://arxiv.org/pdf/2303.04866.pdf">[PDF]</a> <a href="https://sh8.io/#/relightable_hands">[Project]</a></h5>
<p><em>Shun Iwase, Shunsuke Saito, Tomas Simon, Stephen Lombardi, Timur Bagautdinov, Rohan Joshi, Fabian Prada, Takaaki Shiratori, Yaser Sheikh, Jason Saragih</em></p>
<h5 id="h2onet-hand-occlusion-and-orientation-aware-network-for-real-time-3d-hand-mesh-reconstruction">• H2ONet: Hand-Occlusion-and-Orientation-aware Network for Real-time 3D Hand Mesh Reconstruction.</h5>
<p><em>Hao Xu, Tianyu Wang, Xiao Tang, Chi-Wing Fu</em></p>
<h5 id="affordance-diffusion-synthesizing-hand-object-interactions-pdf-project">• Affordance Diffusion: Synthesizing Hand-Object Interactions. <a href="https://arxiv.org/pdf/2303.12538.pdf">[PDF]</a> <a href="https://judyye.github.io/affordiffusion-www/">[Project]</a></h5>
<p><em>Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello, Stan Birchfield, Jiaming Song, Shubham Tulsiani, Sifei Liu</em></p>
<h5 id="gsdf-geometry-driven-signed-distance-functions-for-3d-hand-object-reconstruction-pdf-project">• gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction. <a href="https://zerchen.github.io/contents/CVPR_gSDF_Paper.pdf">[PDF]</a> <a href="https://zerchen.github.io/projects/gsdf.html">[Project]</a></h5>
<p><em>Zerui Chen, Shizhe Chen, Cordelia Schmid, Ivan Laptev</em></p>
<h5 id="harmonious-feature-learning-for-interactive-hand-object-pose-estimation">• Harmonious Feature Learning for Interactive Hand-Object Pose Estimation.</h5>
<p><em>Zhifeng Lin, Changxing Ding, Huan Yao, Zengsheng Kuang, Shaoli Huang</em></p>
<h5 id="handy-towards-a-high-fidelity-3d-hand-shape-and-appearance-model">• Handy: Towards a high fidelity 3D hand shape and appearance model.</h5>
<p><em>Rolandos Potamias Potamias, Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou, Stefanos Zafeiriou</em></p>
<h5 id="hand-avatar-free-pose-hand-animation-and-rendering-from-monocular-video-pdf-project">• Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video. <a href="https://arxiv.org/pdf/2211.12782.pdf">[PDF]</a> <a href="https://seanchenxy.github.io/HandAvatarWeb/">[Project]</a></h5>
<p><em>Xingyu Chen, Baoyuan Wang Heung-Yeung, Shum</em></p>
<h5 id="cross-domain-3d-hand-pose-estimation-with-dual-modalities-pdf">• Cross-domain 3D Hand Pose Estimation with Dual Modalities. [PDF]</h5>
<p><em>Qiuxia Lin, Linlin Yang, Angela Yao</em></p>
<h5 id="overcoming-the-tradeoff-in-accuracy-and-plausibility-for-3d-hand-shape-reconstruction-pdf">• Overcoming the Tradeoff in Accuracy and Plausibility for 3D Hand Shape Reconstruction. [PDF]</h5>
<p><em>Ziwei Yu, Chen Li, Linlin Yang, Xiaoxu Zheng, Michael Bi Mi, Gim Hee Lee, Angela Yao</em></p>
<h5 id="hierarchical-temporal-transformer-for-3d-hand-pose-estimation-and-action-recognition-from-egocentric-rgb-videos-pdf-project-code">• Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos. <a href="https://arxiv.org/pdf/2209.09484">[PDF]</a>  <a href="https://fylwen.github.io/htt.html">[Project]</a> <a href="https://github.com/fylwen/HTT">[Code]</a></h5>
<p><em>Yilin Wen, Hao Pan, Lei Yang, Jia Pan, Taku Komura, Wenping Wang</em></p>
<h5 id="im2hands-learning-attentive-implicit-representation-of-interacting-two-hand-shapes-pdf-project-code">• Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes. <a href="https://arxiv.org/pdf/2302.14348">[PDF]</a>  <a href="https://jyunlee.github.io/projects/implicit-two-hands/">[Project]</a> <a href="https://github.com/jyunlee/Im2Hands">[Code]</a></h5>
<p><em>Jihyun Lee, Minhyuk Sung, Honggyu Choi, Tae-Kyun Kim</em></p>
<h5 id="neural-voting-field-for-camera-space-3d-hand-pose-estimation-pdf">• Neural Voting Field for Camera-Space 3D Hand Pose Estimation. [PDF]</h5>
<p><em>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</em></p>
<h5 id="bringing-inputs-to-shared-domains-for-3d-interacting-hands-recovery-in-the-wild-pdf">• Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild. [PDF]</h5>
<p><em>Gyeongsik Moon</em></p>
<h5 id="recovering-3d-hand-mesh-sequence-from-a-single-blurry-image-a-new-dataset-and-temporal-unfolding-pdf">• Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding. [PDF]</h5>
<p><em>JoonKyu Park<em>, Yeonguk Oh</em>, Jaeha Kim*, Gyeongsik Moon, Kyoung Mu Lee</em></p>
<h5 id="semi-supervised-hand-appearance-recovery-via-structure-disentanglement-and-dual-adversarial-discrimination-pdf">• Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination. [PDF]</h5>
<p><em>Zimeng Zhao, Binghui Zuo, Zhiyu Long and Yangang Wang</em></p>
<h5 id="transformer-based-unified-recognition-of-two-hands-manipulating-objects-pdf">• Transformer-based Unified Recognition of Two Hands Manipulating Objects. [PDF]</h5>
<p><em>Hoseong Cho, Chanwoo Kim, Jihyeon Kim, Seongyeong Lee, Elkhan Ismayilzada, Seungryul Baek</em></p>
<h5 id="acr-attention-collaboration-based-regressor-for-arbitrary-two-hand-reconstruction-pdf-project-code">• ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction. <a href="https://arxiv.org/pdf/2303.05938">[PDF]</a>  <a href="https://zhengdiyu.github.io/ACR-page/">[Project]</a> <a href="https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction">[Code]</a></h5>
<p><em>Zhengdi Yu, Shaoli Huang, Chen Fang, Toby P. Breckon, Jue Wang</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2023-iccv">2023 ICCV</h3>
<h5 id="novel-view-synthesis-and-pose-estimation-for-hand-object-interaction-from-sparse-views-pdf-project-code-data">• Novel-view Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qu_Novel-View_Synthesis_and_Pose_Estimation_for_Hand-Object_Interaction_from_Sparse_ICCV_2023_paper.pdf">[PDF]</a> <a href="https://iscas3dv.github.io/HO-NeRF/index.html">Project]</a> <a href="https://github.com/iscas3dv/HO-NeRF">[Code]</a> <a href="https://pan.baidu.com/s/1t7oRCtJe0qBYazbC7CZoiA?pwd=8rc4">[Data]</a></h5>
<p><em>Wentian  Qu, Zhaopeng Cui, Yinda Zhang, Chenyu Meng, Cuixia Ma, Xiaoming Deng, Hongan Wang</em></p>
<h5 id="contactgen-generative-contact-modeling-for-grasp-generation-pdf-project-code">• ContactGen: Generative Contact Modeling for Grasp Generation <a href="https://arxiv.org/pdf/2310.03740.pdf">[PDF]</a> <a href="https://stevenlsw.github.io/contactgen/">[Project]</a> <a href="https://github.com/stevenlsw/contactgen">[Code]</a></h5>
<p><em>Shaowei Liu, Yang Zhou, Jimei Yang, Saurabh Gupta, Shenlong Wang</em></p>
<h5 id="diffusion-guided-reconstruction-of-everyday-hand-object-interaction-clips-pdf">• Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Diffusion-Guided_Reconstruction_of_Everyday_Hand-Object_Interaction_Clips_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Yufei Ye, Poorvi Hebbar, Abhinav Gupta, Shubham Tulsiani</em></p>
<h5 id="handr2n2-iterative-3d-hand-pose-estimation-using-a-residual-recurrent-neural-network-pdf">• HandR2N2: Iterative 3D Hand Pose Estimation Using a Residual Recurrent Neural Network. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_HandR2N2_Iterative_3D_Hand_Pose_Estimation_Using_a_Residual_Recurrent_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Wencan Cheng, Jong Hwan Ko</em></p>
<h5 id="hamuco-hand-pose-estimation-via-multiview-collaborative-self-supervised-learning-pdf-project-code">• HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_HaMuCo_Hand_Pose_Estimation_via_Multiview_Collaborative_Self-Supervised_Learning_ICCV_2023_paper.pdf">[PDF]</a> <a href="https://zxz267.github.io/HaMuCo/">[Project]</a> <a href="https://github.com/zxz267/HaMuCo">[Code]</a></h5>
<p><em>Xiaozheng Zheng, Chao Wen, Zhou Xue, Pengfei Ren, Jingyu Wang</em></p>
<h5 id="deformer-dynamic-fusion-transformer-for-robust-hand-pose-estimation-pdf">• Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_Deformer_Dynamic_Fusion_Transformer_for_Robust_Hand_Pose_Estimation_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Qichen Fu, Xingyu Liu, Ran Xu, Juan Carlos Niebles, Kris M. Kitani</em></p>
<h5 id="phrit-parametric-hand-representation-with-implicit-template-pdf">• PHRIT: Parametric Hand Representation with Implicit Template. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_PHRIT_Parametric_Hand_Representation_with_Implicit_Template_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Zhisheng Huang, Yujin Chen, Di Kang, Jinlu Zhang, Zhigang Tu</em></p>
<h5 id="chord-category-level-hand-held-object-reconstruction-via-shape-deformation-pdf">• CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_CHORD_Category-level_Hand-held_Object_Reconstruction_via_Shape_Deformation_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Kailin Li, Lixin Yang, Haoyu Zhen, Zenan Lin, Xinyu Zhan, Licheng Zhong, Jian Xu, Kejian Wu, Cewu Lu</em></p>
<h5 id="uncertainty-aware-state-space-transformer-for-egocentric-3d-hand-trajectory-forecasting-pdf">• Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bao_Uncertainty-aware_State_Space_Transformer_for_Egocentric_3D_Hand_Trajectory_Forecasting_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Wentao Bao, Lele Chen, Libing Zeng, Zhong Li, Yi Xu, Junsong Yuan, Yu Kong</em></p>
<h5 id="spectral-graphormer-spectral-graph-based-transformer-for-egocentric-two-hand-reconstruction-using-multi-view-color-images-pdf-project-code">• Spectral Graphormer: Spectral Graph-Based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tse_Spectral_Graphormer_Spectral_Graph-Based_Transformer_for_Egocentric_Two-Hand_Reconstruction_using_ICCV_2023_paper.pdf">[PDF]</a> <a href="https://eldentse.github.io/Spectral-Graphormer/">[Project]</a> <a href="https://github.com/google-research/google-research/tree/master/spectral_graphormer">[Code]</a></h5>
<p><em>Tze Ho Elden Tse, Franziska Mueller, Zhengyang Shen, Danhang Tang, Thabo Beeler, Mingsong Dou, Yinda Zhang, Sasa Petrovic, Hyung Jin Chang, Jonathan Taylor, Bardia Doosti</em></p>
<h5 id="reconstructing-interacting-hands-with-interaction-prior-from-monocular-images-pdf">• Reconstructing Interacting Hands with Interaction Prior from Monocular Images. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zuo_Reconstructing_Interacting_Hands_with_Interaction_Prior_from_Monocular_Images_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Binghui Zuo, Zimeng Zhao, Wenqian Sun, Wei Xie, Zhou Xue, Yangang Wang</em></p>
<h5 id="ochid-fi-occlusion-robust-hand-pose-estimation-in-3d-via-rf-vision-pdf">• OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_OCHID-Fi_Occlusion-Robust_Hand_Pose_Estimation_in_3D_via_RF-Vision_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Shujie Zhang, Tianyue Zheng, Zhe Chen, Jingzhi Hu, Abdelwahed Khamis, Jiajun Liu, Jun Luo</em></p>
<h5 id="dynamic-hyperbolic-attention-network-for-fine-hand-object-reconstruction-pdf">• Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Leng_Dynamic_Hyperbolic_Attention_Network_for_Fine_Hand-object_Reconstruction_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Zhiying Leng, Shun-Cheng Wu, Mahdi Saleh, Antonio Montanaro, Hao Yu, Yin Wang, Nassir Navab, Xiaohui Liang, Federico Tombari</em></p>
<h5 id="decoupled-iterative-refinement-framework-for-interacting-hands-reconstruction-from-a-single-rgb-image-pdf-project-code">• Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_Decoupled_Iterative_Refinement_Framework_for_Interacting_Hands_Reconstruction_from_a_ICCV_2023_paper.pdf">[PDF]</a> <a href="https://pengfeiren96.github.io/DIR/">[Project]</a> <a href="https://github.com/PengfeiRen96/DIR">[Code]</a></h5>
<p><em>Pengfei Ren, Chao Wen, Xiaozheng Zheng, Zhou Xue, Haifeng Sun, Qi Qi, Jingyu Wang, Jianxin Liao</em></p>
<h5 id="affordpose-a-large-scale-dataset-of-hand-object-interactions-with-affordance-driven-hand-pose-pdf">• AffordPose: A Large-Scale Dataset of Hand-Object Interactions with Affordance-Driven Hand Pose. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jian_AffordPose_A_Large-Scale_Dataset_of_Hand-Object_Interactions_with_Affordance-Driven_Hand_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Juntao Jian, Xiuping Liu, Manyi Li, Ruizhen Hu, Jian Liu</em></p>
<h5 id="egopca-a-new-framework-for-egocentric-hand-object-interaction-understanding-pdf">• EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_EgoPCA_A_New_Framework_for_Egocentric_Hand-Object_Interaction_Understanding_ICCV_2023_paper.pdf">[PDF]</a></h5>
<p><em>Yue Xu, Yong-Lu Li, Zhemin Huang, Michael Xu Liu, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang</em></p>
<h5 id="renderih-a-large-scale-synthetic-dataset-for-3d-interacting-hand-pose-estimation-pdf-project-code">• RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_RenderIH_A_Large-Scale_Synthetic_Dataset_for_3D_Interacting_Hand_Pose_ICCV_2023_paper.pdf">[PDF]</a> <a href="https://github.com/adwardlee/RenderIH/tree/main">[Project]</a> <a href="https://github.com/adwardlee/RenderIH/tree/main">[Code]</a></h5>
<p><em>Lijun Li, Linrui Tian, Xindi Zhang, Qi Wang, Bang Zhang, Liefeng Bo, Mengyuan Liu, Chen Chen</em></p>
<h5 id="source-free-domain-adaptive-human-pose-estimation-pdf-project-code">• Source-free Domain Adaptive Human Pose Estimation <a href="https://arxiv.org/abs/2308.03202">[PDF]</a> <a href="https://github.com/davidpengucf/SFDAHPE">[Project]</a> <a href="https://github.com/davidpengucf/SFDAHPE">[Code]</a></h5>
<p><em>Qucheng Peng, Ce Zheng, Chen Chen</em></p>
<h5 id="livehand-real-time-and-photorealistic-neural-hand-rendering-pdf-project-code">• LiveHand: Real-time and Photorealistic Neural Hand Rendering <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Mundra_LiveHand_Real-time_and_Photorealistic_Neural_Hand_Rendering_ICCV_2023_paper.pdf">[PDF]</a> <a href="https://vcai.mpi-inf.mpg.de/projects/LiveHand/">[Project]</a> <a href="https://github.com/amundra15/livehand">[Code]</a></h5>
<p><em>Akshay Mundra, Mallikarjun B R, Jiayi Wang, Marc Habermann, Christian Theobalt, Mohamed Elgharib</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2023-others">2023 Others</h3>
<h5 id="2023-neurips-fourierhandflow-neural-4d-hand-representation-using-fourier-query-flow-pdf-project">• [2023 NeurIPS] FourierHandFlow: Neural 4D Hand Representation Using Fourier Query Flow. <a href="https://arxiv.org/pdf/2307.08100.pdf">[PDF]</a>  <a href="https://jyunlee.github.io/projects/fourier-hand-flow/">[Project]</a></h5>
<p><em>Jihyun Lee, Junbong Jang, Donghwan Kim, Minhyuk Sung, Tae-Kyun (T-K) Kim</em></p>
<h5 id="2023-iccvw-showme-benchmarking-object-agnostic-hand-object-3d-reconstruction-pdf-project-code-data">• [2023 ICCVW] SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction. <a href="https://arxiv.org/pdf/2309.10748.pdf">[PDF]</a>  <a href="https://europe.naverlabs.com/research/showme">[Project]</a> <a href="https://github.com/naver/showme/tree/main">[Code]</a> <a href="https://download.europe.naverlabs.com/showme/">[Data]</a></h5>
<p><em>Anilkumar Swamy, Vincent Leroy, Philippe Weinzaepfel, Fabien Baradel, Salma Galaaoui, Romain Bregier, Matthieu Armando, Jean-Sebastien Franco, Gregory Rogez</em></p>
<h5 id="2023-aaai-two-heads-are-better-than-one-image-point-cloud-network-for-depth-based-3d-hand-pose-estimation-pdf-aaai-23-distinguished-papers">• [2023 AAAI] Two Heads are Better than One: Image-Point Cloud Network for Depth-Based 3D Hand Pose Estimation. [[PDF]]  <em>(AAAI-23 Distinguished Papers)</em></h5>
<p><em>Pengfei Ren, Yuchen Chen, Jiachang Hao, Haifeng Sun, Qi Qi, Jingyu Wang, Jianxin Liao</em></p>
<h5 id="2023-aaai-tracking-and-reconstructing-hand-object-interactions-from-point-cloud-sequences-in-the-wild-pdf">• [2023 AAAI] Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild. <a href="https://arxiv.org/pdf/2209.12009">[PDF]</a></h5>
<p><em>Jiayi Chen, Mi Yan, Jiazhao Zhang, Yinzhen Xu, Xiaolong Li, Yijia Weng, Li Yi, Shuran Song, He Wang</em></p>
<h5 id="2023-wacv-thor-net-end-to-end-graformer-based-realistic-two-hands-and-object-reconstruction-with-self-supervision-pdf-code">• [2023 WACV] THOR-Net: End-to-End Graformer-Based Realistic Two Hands and Object Reconstruction With Self-Supervision. <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Aboukhadra_THOR-Net_End-to-End_Graformer-Based_Realistic_Two_Hands_and_Object_Reconstruction_With_WACV_2023_paper.pdf">[PDF]</a> <a href="https://github.com/ATAboukhadra/THOR-Net">[Code]</a></h5>
<p><em>Ahmed Tawfik Aboukhadra, Jameel Malik, Ahmed Elhayek, Nadia Robertini, Didier Stricker</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2022-eccv">2022 ECCV</h3>
<h5 id="identity-aware-hand-mesh-estimation-and-personalization-from-rgb-images-pdf-code">• Identity-aware Hand Mesh Estimation and Personalization from RGB Images    . [[PDF]] <a href="https://github.com/deyingk/PersonalizedHandMeshEstimation">[Code]</a></h5>
<p><em>Deying Kong, Linguang Zhang, Liangjian Chen, Haoyu Ma, Xiangyi Yan, Shanlin Sun, Xingwei Liu, Kun Han, Xiaohui Xie</em></p>
<h5 id="alignsdf-pose-aligned-signed-distance-fields-for-hand-object-reconstruction-pdf-project-code">• AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction. <a href="https://arxiv.org/pdf/2207.12909.pdf">[PDF]</a> <a href="https://zerchen.github.io/projects/alignsdf.html">[Project]</a> <a href="https://github.com/zerchen/alignsdf">[Code]</a></h5>
<p><em>Zerui Chen, Yana Hasson, Cordelia Schmid, Ivan Laptev</em></p>
<h5 id="s2contact-graph-based-network-for-3d-hand-object-contact-estimation-with-semi-supervised-learning-pdf-project-code">• S<sup>2</sup>Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning. <a href="https://arxiv.org/pdf/2208.00874.pdf">[PDF]</a> <a href="https://eldentse.github.io/s2contact/">[Project]</a>  <a href="https://github.com/eldentse/s2contact">[Code]</a></h5>
<p><em>Tze Ho Elden Tse, Zhongqun Zhang, Kwang In Kim, Ales Leonardis, Feng Zheng, Hyung Jin Chang</em></p>
<h5 id="domain-adaptive-hand-keypoint-and-pixel-localization-in-the-wild-pdf-project">• Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. <a href="https://arxiv.org/pdf/2203.08344.pdf">[PDF]</a> <a href="https://tkhkaeio.github.io/projects/22-hand-ps-da/">[Project]</a></h5>
<p><em>Takehiko Ohkawa, Yu-Jhe Li, Qichen Fu, Ryosuke Furuta, Kris M. Kitani, and Yoichi Sato</em></p>
<h5 id="3d-interacting-hand-pose-estimation-by-hand-de-occlusion-and-removal-pdf-projectdataset">• 3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal. <a href="https://arxiv.org/abs/2207.11061">[PDF]</a> <a href="https://menghao666.github.io/HDR/">[Project]</a><a href="https://connecthkuhk-my.sharepoint.com/personal/js20_connect_hku_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fjs20%5Fconnect%5Fhku%5Fhk%2FDocuments%2FAIH%5Fdataset&amp;ga=1">[Dataset]</a></h5>
<p>Hao Meng, Sheng Jin, Wentao Liu, Chen Qian, Mengxiang Lin, Wanli Ouyang, Ping Luo</p>
<h5 id="cross-attention-of-disentangled-modalities-for-3d-human-mesh-recovery-with-transformers-pdf-project-code">• Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers. <a href="https://arxiv.org/abs/2207.13820">[PDF]</a> <a href="https://fastmetro.github.io/">[Project]</a> <a href="https://github.com/postech-ami/FastMETRO">[Code]</a></h5>
<p><em>Junhyeong Cho, Kim Youwang, Tae-Hyun Oh</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2022-cvpr">2022 CVPR</h3>
<h5 id="whats-in-your-hands-3d-reconstruction-of-generic-objects-in-hands-pdf-project-code">• What's in your hands? 3D Reconstruction of Generic Objects in Hands. <a href="https://arxiv.org/pdf/2204.07153.pdf">[PDF]</a> <a href="https://judyye.github.io/ihoi/">[Project]</a> <a href="https://github.com/JudyYe/ihoi">[Code]</a></h5>
<p><em>Yufei Ye, Abhinav Gupta, Shubham Tulsiani</em></p>
<h5 id="mining-multi-view-information-a-strong-self-supervised-framework-for-depth-based-3d-hand-pose-and-mesh-estimation-pdf-code">• Mining Multi-View Information: A Strong Self-Supervised Framework for Depth-based 3D Hand Pose and Mesh Estimation. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_Mining_Multi-View_Information_A_Strong_Self-Supervised_Framework_for_Depth-Based_3D_CVPR_2022_paper.pdf">[PDF]</a> [Code]</h5>
<p><em>Pengfei Ren, Haifeng Sun, Jiachang Hao, Jingyu Wang, Qi Qi,Jianxin Liao</em></p>
<h5 id="handoccnet-occlusion-robust-3d-hand-mesh-estimation-network-pdf-code">• HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network. <a href="https://arxiv.org/pdf/2203.14564">[PDF]</a> <a href="https://github.com/namepllet/HandOccNet">[Code]</a></h5>
<p><em>JoonKyu Park, Yeonguk Oh, Gyeongsik Moon, Hongsuk Choi, Kyoung Mu Lee</em></p>
<h5 id="keypoint-transformer-solving-joint-identification-in-challenging-hands-and-object-interactions-for-accurate-3d-pose-estimation-pdf">• Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation. [PDF]</h5>
<p><em>Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, Vincent Lepetit</em></p>
<h5 id="collaborative-learning-for-hand-and-object-reconstruction-with-attention-guided-graph-convolution-pdf-project">• Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tse_Collaborative_Learning_for_Hand_and_Object_Reconstruction_With_Attention-Guided_Graph_CVPR_2022_paper.pdf">[PDF]</a> <a href="https://eldentse.github.io/collab-hand-object/">[Project]</a></h5>
<p><em>Tze Ho Elden Tse, Kwang In Kim, Ales Leonardis, and Hyung Jin Chang</em></p>
<h5 id="spatial-temporal-parallel-transformer-for-arm-hand-dynamic-estimation-pdf">• Spatial-Temporal Parallel Transformer for Arm-Hand Dynamic Estimation. <a href="https://arxiv.org/pdf/2203.16202.pdf">[PDF]</a></h5>
<p><em>Shuying Liu, Wenbin Wu, Jiaxian Wu, Yue Lin</em></p>
<h5 id="d-grasp-physically-plausible-dynamic-grasp-synthesis-for-hand-object-interactions-pdf-project">• D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions. <a href="https://arxiv.org/pdf/2112.03028.pdf">[PDF]</a> <a href="https://eth-ait.github.io/d-grasp/">[Project]</a></h5>
<p><em>Sammy Christen, Muhammed Kocabas, Emre Aksan, Jemin Hwangbo, Jie Song, Otmar Hilliges</em></p>
<h5 id="goal-generating-4d-whole-body-motion-for-hand-object-grasping-pdf-project">• GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping. <a href="https://arxiv.org/pdf/2112.11454.pdf">[PDF]</a> <a href="https://goal.is.tuebingen.mpg.de/">[Project]</a></h5>
<p><em>Omid Taheri, Vasileios Choutas, Michael J. Black, Dimitrios Tzionas</em></p>
<h5 id="oakink-a-large-scale-knowledge-repository-for-understanding-hand-object-interaction-pdf-code">• OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction. <a href="https://arxiv.org/pdf/2203.15709">[PDF]</a> <a href="https://github.com/lixiny/OakInk">[Code]</a></h5>
<p><em>Lixin Yang, Kailin Li Xinyu Zhan, Fei Wu, Anran Xu, Liu Liu, Cewu Lu</em></p>
<h5 id="artiboost-boosting-articulated-3d-hand-object-pose-estimation-via-online-exploration-and-synthesis-pdf-code">• ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. <a href="https://arxiv.org/pdf/2109.05488">[PDF]</a> <a href="https://github.com/MVIG-SJTU/ArtiBoost">[Code]</a></h5>
<p><em>Kailin Li, Lixin Yang, Xinyu Zhan, Jun Lv, Wenqiang Xu, Jiefeng Li, Cewu Lu</em></p>
<h5 id="interacting-attention-graph-for-single-image-two-hand-reconstruction-pdf-project-code">• Interacting Attention Graph for Single Image Two-Hand Reconstruction. <a href="https://arxiv.org/pdf/2203.09364">[PDF]</a>  <a href="http://www.liuyebin.com/IntagHand/Intaghand.html">[Project]</a>  <a href="https://github.com/Dw1010/IntagHand">[Code]</a></h5>
<p><em>Mengcheng Li，Liang An, Hongwen Zhang, Lianpeng Wu, Feng Chen, Tao Yu, Yebin Liu</em></p>
<h5 id="mobrecon-mobile-friendly-hand-mesh-reconstruction-from-monocular-image-pdf-code">• MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image.  <a href="https://arxiv.org/pdf/2112.02753">[PDF]</a> <a href="https://github.com/SeanChenxy/HandMesh">[Code]</a></h5>
<p><em>Xingyu Chen, Yufeng Liu, Yajiao Dong, Xiong Zhang, Chongyang Ma, Yanmin Xiong, Yuan Zhang, Xiaoyan Guo</em></p>
<h5 id="lisa-learning-implicit-shape-and-appearance-of-hands-pdf-project">• LISA: Learning Implicit Shape and Appearance of Hands.  <a href="https://arxiv.org/pdf/2204.01695">[PDF]</a> <a href="http://www.iri.upc.edu/people/ecorona/lisa/">[Project]</a></h5>
<p><em>Enric Corona, Tomas Hodan, Minh Vo, Francesc Moreno-Noguer, Chris Sweeney, Richard Newcombe, Lingni Ma</em></p>
<h3 id="2022-others">2022 Others</h3>
<h5 id="2022-aaai-efficient-virtual-view-selection-for-3d-hand-pose-estimation-pdf-project-code">• [2022 AAAI] Efficient Virtual View Selection for 3D Hand Pose Estimation. <a href="https://me495.github.io/handpose-virtualview/resources/paper.pdf">[PDF]</a> <a href="https://me495.github.io/handpose-virtualview/">[Project]</a> <a href="https://github.com/iscas3dv/handpose-virtualview">[Code]</a></h5>
<p><em>Jian Cheng, Yanguang Wan, Dexin Zuo, Cuixia Ma, Jian Gu, Ping Tan, Hongan Wang, Xiaoming Deng, Yinda Zhang</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2021-iccv">2021 ICCV</h3>
<h5 id="toward-human-like-grasp-dexterous-grasping-via-semantic-representation-of-object-hand-pdf">• Toward Human-Like Grasp: Dexterous Grasping via Semantic Representation of Object-Hand. <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Toward_Human-Like_Grasp_Dexterous_Grasping_via_Semantic_Representation_of_Object-Hand_ICCV_2021_paper.pdf">[PDF]</a></h5>
<p><em>Tianqiang Zhu, Rina Wu, Xiangbo Lin, Yi Sun</em></p>
<h5 id="self-supervised-transfer-learning-for-hand-mesh-recovery-from-binocular-images-pdf">• Self-Supervised Transfer Learning for Hand Mesh Recovery From Binocular Images. <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Self-Supervised_Transfer_Learning_for_Hand_Mesh_Recovery_From_Binocular_Images_ICCV_2021_paper.pdf">[PDF]</a></h5>
<p><em>Zheng Chen, Sihan Wang, Yi Sun, Xiaohong Ma</em></p>
<h5 id="self-supervised-3d-hand-pose-estimation-from-monocular-rgb-via-contrastive-learning-pdf-code-oral">• Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. <a href="https://arxiv.org/pdf/2106.05953">[PDF]</a> <a href="https://github.com/dahiyaaneesh/peclr">[Code]</a> (Oral)</h5>
<p><em>Adrian Spurr, Aneesh Dahiya, Xucong Zhang, Xi Wang, Otmar Hilliges</em></p>
<h5 id="towards-accurate-alignment-in-real-time-3d-hand-mesh-reconstruction-pdf-code">• Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction. <a href="https://arxiv.org/pdf/2109.01723.pdf">[PDF]</a>  <a href="https://github.com/wbstx/handAR">[Code]</a></h5>
<p><em>Xiao Tang, Tianyu Wang, Chi-Wing Fu</em></p>
<h5 id="eventhands-real-time-neural-3d-hand-reconstruction-from-an-event-stream-pdf-project">• EventHands: Real-Time Neural 3D Hand Reconstruction from an Event Stream. <a href="https://arxiv.org/pdf/2012.06475.pdf">[PDF]</a>  <a href="https://gvv.mpi-inf.mpg.de/projects/EventHands/">[Project]</a></h5>
<p><em>Viktor Rudnev, Vladislav Golyanik, Jiayi Wang, Hans-Peter Seidel, Franziska Mueller, Mohamed Elgharib, Christian Theobalt</em></p>
<h5 id="reconstructing-hand-object-interactions-in-the-wild-pdf-project">• Reconstructing Hand-Object Interactions in the Wild. <a href="https://arxiv.org/pdf/2012.09856.pdf">[PDF]</a>  <a href="https://people.eecs.berkeley.edu/~zhecao/rhoi/">[Project]</a></h5>
<p><em>Zhe Cao<em>, Ilija Radosavovic</em>, Angjoo Kanazawa, Jitendra Malik</em></p>
<h5 id="handfoldingnet-a-3d-hand-pose-estimation-network-using-multiscale-feature-guided-folding-of-a-2d-hand-skeleton-pdf-code">• HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton. <a href="https://arxiv.org/pdf/2108.05545">[PDF]</a> <a href="https://github.com/cwc1260/HandFold">[Code]</a></h5>
<p><em>Wencan Cheng, Jae Hyun Park, Jong Hwan Ko</em></p>
<h5 id="h2o-two-hands-manipulating-objects-for-first-person-interaction-recognition-pdf-project-code">• H2O: Two Hands Manipulating Objects for First Person Interaction Recognition. <a href="https://arxiv.org/pdf/2104.11181">[PDF]</a>  <a href="https://www.taeinkwon.com/projects/h2o">[Project]</a>  <a href="https://github.com/taeinkwon/h2odataset">[Code]</a></h5>
<p><em>Taein Kwon, Bugra Tekin, Jan Stuhmer, Federica Bogo, Marc Pollefeys</em></p>
<h5 id="i2uv-handnet-image-to-uv-prediction-network-for-accurate-and-high-fidelity-3d-hand-mesh-modeling-pdf">• I2UV-HandNet: Image-to-UV Prediction Network for Accurate and High-fidelity 3D Hand Mesh Modeling. <a href="https://arxiv.org/pdf/2102.03725">[PDF]</a></h5>
<p><em>Ping Chen, Yujin Chen, Dong Yang, Fangyin Wu, Qin Li, Qingpei Xia, Yong Tan</em></p>
<h5 id="semihand-semi-supervised-hand-pose-estimation-with-consistency-pdf">• SemiHand: Semi-supervised Hand Pose Estimation with Consistency. <a href="https://www.mu4yang.com/files/project/semihand/semihand.pdf">[PDF]</a></h5>
<p><em>Linlin Yang, Shicheng Chen, Angela Yao</em></p>
<h5 id="end-to-end-detection-and-pose-estimation-of-two-interacting-hands-pdf">• End-to-End Detection and Pose Estimation of Two Interacting Hands. <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_End-to-End_Detection_and_Pose_Estimation_of_Two_Interacting_Hands_ICCV_2021_paper.pdf">[PDF]</a></h5>
<p><em>Donguk Kim, Kwang In Kim, Seungryul Baek</em></p>
<h5 id="hand-object-contact-consistency-reasoning-for-human-grasps-generation-pdf-project-oral">• Hand-Object Contact Consistency Reasoning for Human Grasps Generation. <a href="https://arxiv.org/pdf/2104.03304">[PDF]</a> <a href="https://hwjiang1510.github.io/GraspTTA/">[Project]</a> <em>(Oral)</em></h5>
<p><em>Hanwen Jiang, Shaowei Liu, Jiashun Wang, Xiaolong Wang</em></p>
<h5 id="hand-image-understanding-via-deep-multi-task-learning-pdf-code">• Hand Image Understanding via Deep Multi-Task Learning. <a href="https://arxiv.org/pdf/2107.11646">[PDF]</a> <a href="https://github.com/MandyMo/HIU-DMTL">[Code]</a></h5>
<p><em>Xiong Zhang, Hongsheng Huang, Jianchao Tan, Hongmin Xu, Cheng Yang, Guozhu Peng, Lei Wang, Ji Liu</em></p>
<h5 id="cpf-learning-a-contact-potential-field-to-model-the-hand-object-interaction-pdf-code">• CPF: Learning a Contact Potential Field to Model the Hand-object Interaction. <a href="https://arxiv.org/pdf/2012.00924.pdf">[PDF]</a>  <a href="https://github.com/lixiny/CPF">[Code]</a></h5>
<p><em>Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Jiefeng Li, Cewu Lu</em></p>
<h5 id="travelnet-self-supervised-physically-plausible-hand-motion-learning-from-monocular-color-images-pdf">• TravelNet: Self-supervised Physically Plausible Hand Motion Learning from Monocular Color Images.  <a href="https://www.yangangwang.com/papers/ZHAO-TRAVEL-2021-08.pdf">[PDF]</a></h5>
<p><em>Zimeng Zhao, Xi Zhao and Yangang Wang</em></p>
<h5 id="interacting-two-hand-3d-pose-and-shape-reconstruction-from-single-color-image-pdf-project-code">• Interacting Two-Hand 3D Pose and Shape Reconstruction from Single Color Image.  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Interacting_Two-Hand_3D_Pose_and_Shape_Reconstruction_From_Single_Color_ICCV_2021_paper.pdf">[PDF]</a> <a href="https://baowenz.github.io/Intershape/">[Project]</a> <a href="https://github.com/BaowenZ/Two-Hand-Shape-Pose">[Code]</a></h5>
<p><em>Baowen Zhang, Yangang Wang, Xiaoming Deng, Yinda Zhang, Ping Tan, Cuixia Ma and Hongan Wang</em></p>
<h5 id="removing-the-bias-of-integral-pose-regression-pdf">• Removing the Bias of Integral Pose Regression. <a href="https://www.mu4yang.com/files/papers/Removing%20the%20Bias%20of%20Integral%20Pose%20Regression.pdf">[PDF]</a></h5>
<p><em>Kerui Gu, Linlin Yang, Angela Yao</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2021-cvpr">2021 CVPR</h3>
<h5 id="monocular-real-time-full-body-capture-with-inter-part-correlations-pdf">• Monocular Real-time Full Body Capture with Inter-part Correlations. <a href="https://arxiv.org/pdf/2012.06087">[PDF]</a></h5>
<p><em>Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush Tewari, Christian Theobalt, Feng Xu</em></p>
<h5 id="end-to-end-human-pose-and-mesh-reconstruction-with-transformers-pdf-code">• End-to-End Human Pose and Mesh Reconstruction with Transformers. <a href="https://arxiv.org/pdf/2012.09760.pdf">[PDF]</a> <a href="https://github.com/microsoft/MeshTransformer">[Code]</a></h5>
<p><em>Kevin Lin, Lijuan Wang, Zicheng Liu</em></p>
<h5 id="dexycb-a-benchmark-for-capturing-hand-grasping-of-objects-pdf-project-code">• DexYCB: A Benchmark for Capturing Hand Grasping of Objects. <a href="https://arxiv.org/pdf/2104.04631.pdf">[PDF]</a> <a href="https://dex-ycb.github.io/">[Project]</a> <a href="https://github.com/NVlabs/dex-ycb-toolkit">[Code]</a></h5>
<p><em>Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S. Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, Jan Kautz, Dieter Fox</em></p>
<h5 id="body2hands-learning-to-infer-3d-hands-from-conversational-gesture-body-dynamics-pdf-project">• Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics. <a href="https://arxiv.org/pdf/2007.12287.pdf">[PDF]</a> <a href="http://people.eecs.berkeley.edu/~evonne_ng/projects/body2hands/">[Project]</a></h5>
<p><em>Evonne Ng, Hanbyul Joo, Shiry Ginosar, Trevor Darrell</em></p>
<h5 id="camera-space-hand-mesh-recovery-via-semantic-aggregation-and-adaptive-2d-1d-registration-pdf">• Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration. <a href="https://arxiv.org/pdf/2103.02845.pdf">[PDF]</a></h5>
<p><em>Xingyu Chen, Yufeng Liu, Chongyang Ma, Jianlong Chang, Huayan Wang, Tian Chen, Xiaoyan Guo, Pengfei Wan, Wen Zheng</em></p>
<h5 id="model-based-3d-hand-reconstruction-via-self-supervised-learning-pdf">• Model-based 3D Hand Reconstruction via Self-Supervised Learning. <a href="https://arxiv.org/pdf/2103.11703">[PDF]</a></h5>
<p><em>Yujin Chen, Zhigang Tu, Di Kang, Linchao Bao, Ying Zhang, Xuefei Zhe, Ruizhi Chen, Junsong Yuan</em></p>
<h5 id="semi-supervised-3d-hand-object-poses-estimation-with-interactions-in-time-pdf-project-code">• Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time. <a href="https://arxiv.org/pdf/2106.05266.pdf">[PDF]</a> <a href="https://stevenlsw.github.io/Semi-Hand-Object/">[Project]</a> <a href="https://github.com/stevenlsw/Semi-Hand-Object">[Code]</a></h5>
<p><em>Shaowei Liu<em>, Hanwen Jiang</em>, Jiarui Xu, Sifei Liu, Xiaolong Wang</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2021-others">2021 Others</h3>
<h5 id="2021-3dv-a-skeleton-driven-neural-occupancy-representation-for-articulated-hands-pdf-project-code-oral">• [2021 3DV] A Skeleton-Driven Neural Occupancy Representation for Articulated Hands. <a href="https://arxiv.org/pdf/2109.11399">[PDF]</a> <a href="https://korrawe.github.io/HALO/HALO.html">[Project]</a> <a href="https://github.com/korrawe/halo">[Code]</a> <em>(Oral)</em></h5>
<p><em>Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar Hilliges, Siyu Tang</em></p>
<h5 id="2021-3dv-learning-to-disambiguate-strongly-interacting-hands-via-probabilistic-per-pixel-part-segmentation-pdf-project-code-oral">• [2021 3DV] Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation. <a href="https://arxiv.org/abs/2107.00434">[PDF]</a> <a href="https://zc-alexfan.github.io/digit">[Project]</a> <a href="https://github.com/zc-alexfan/digit-interacting">[Code]</a> <em>(Oral)</em></h5>
<p><em>Zicong Fan, Adrian Spurr, Muhammed Kocabas, Siyu Tang, Michael J. Black, Otmar Hilliges</em></p>
<h5 id="2021-dicta-semi-supervised-3d-hand-shape-and-pose-estimation-with-label-propagation-pdf">• [2021 DICTA] Semi-Supervised 3D Hand Shape and Pose Estimation with Label Propagation. <a href="https://arxiv.org/pdf/2111.15199">[PDF]</a></h5>
<p><em>Samira Kaviani, Amir Rahimi, Richard Hartley</em></p>
<h5 id="2021-bmvc-joint-aware-regression-rethinking-regression-based-method-for-3d-hand-pose-estimation-pdf">• [2021 BMVC] Joint-Aware Regression: Rethinking Regression-Based Method for 3D Hand Pose Estimation. [PDF]</h5>
<p><em>Xiaozheng Zheng, Pengfei Ren, Haifeng Sun, Jingyu Wang, Qi Qi and Jianxin Liao</em></p>
<h5 id="2021-bmvc-local-and-global-point-cloud-reconstruction-for-3d-hand-pose-estimation-pdf-data">• [2021 BMVC] Local and Global Point Cloud Reconstruction for 3D Hand Pose Estimation. <a href="https://arxiv.org/pdf/2112.06389.pdf">[PDF]</a> <a href="https://github.com/ShichengChen/multiviewDataset">[Data]</a></h5>
<p><em>Ziwei Yu, Linlin Yang, Shicheng Chen and Angela Yao</em></p>
<h5 id="2021-bmvc-handtailor-towards-high-precision-monocular-3d-hand-recovery-pdf-code">• [2021 BMVC] HandTailor: Towards High-Precision Monocular 3D Hand Recovery. <a href="https://arxiv.org/pdf/2102.09244">[PDF]</a> <a href="https://github.com/LyuJ1998/HandTailor">[Code]</a></h5>
<p><em>Jun Lv, Wenqiang Xu, Lixin Yang, Sucheng Qian, Chongzhao Mao, Cewu Lu</em></p>
<h5 id="2021-bmvc-multi-view-image-based-hand-geometry-refinement-using-differentiable-monte-carlo-ray-tracing-pdf">• [2021 BMVC] Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing. <a href="https://arxiv.org/pdf/2107.05509">[PDF]</a></h5>
<p><em>Giorgos Karvounas, Nikolaos Kyriazis, Iason Oikonomidis, Aggeliki Tsoli, Antonis A. Argyros</em></p>
<h5 id="2021-siggraph-manipnet-neural-manipulation-synthesis-with-a-hand-object-spatial-representation-pdf-code">• [2021 SIGGRAPH] ManipNet: Neural Manipulation Synthesis with a Hand-Object Spatial Representation. <a href="http://www.ipab.inf.ed.ac.uk/cgvu/zhang2021.pdf">[PDF]</a>  <a href="https://github.com/cghezhang/ManipNet">[Code]</a></h5>
<p><em>He Zhang, Yuting Ye, Takaaki Shiratori, Taku Komura</em></p>
<h5 id="2021-siggraph-single-depth-view-based-real-time-reconstruction-of-hand-object-interactions-pdf">• [2021 SIGGRAPH] Single Depth View-Based Real-time Reconstruction of Hand-object Interactions. <a href="http://xufeng.site/publications/2021/Single%20Depth%20View%20Based%20Real-time%20Reconstruction%20of%20Hand-objectInteractions.pdf">[PDF]</a></h5>
<p><em>Hao Zhang, Yuxiao Zhou, Yifei Tian, Jun-Hai Yong, Feng Xu</em></p>
<h5 id="2021-iros-dynamic-modeling-of-hand-object-interactions-via-tactile-sensing-pdf-project">• [2021 IROS] Dynamic Modeling of Hand-Object Interactions via Tactile Sensing. <a href="https://arxiv.org/pdf/2109.04378">[PDF]</a>  <a href="http://phystouch.csail.mit.edu/">[Project]</a></h5>
<p><em>Qiang Zhang, Yunzhu Li, Yiyue Luo, Wan Shou, Michael Foshey, Junchi Yan, Joshua B. Tenenbaum, Wojciech Matusik, Antonio Torralba</em></p>
<h5 id="2021-aaai-exploiting-learnable-joint-groups-for-hand-pose-estimation-pdf-code">• [2021 AAAI] Exploiting Learnable Joint Groups for Hand Pose Estimation. <a href="https://arxiv.org/pdf/2012.09496">[PDF]</a>  <a href="https://github.com/moranli-aca/LearnableGroups-Hand">[Code]</a></h5>
<p><em>Moran Li, Yuan Gao, Nong Sang</em></p>
<h5 id="2021-wacv-active-learning-for-bayesian-3d-hand-pose-estimation-pdf-code">• [2021 WACV] Active Learning for Bayesian 3D Hand Pose Estimation. <a href="https://arxiv.org/pdf/2010.00694">[PDF]</a>  <a href="https://github.com/razvancaramalau/al_bhpe">[Code]</a></h5>
<p><em>Razvan Caramalau, Binod Bhattarai, Tae-Kyun Kim</em></p>
<h5 id="2021-wacv-two-hand-global-3d-pose-estimation-using-monocular-rgb-pdf-code">• [2021 WACV] Two-hand Global 3D Pose Estimation Using Monocular RGB. <a href="https://arxiv.org/pdf/2006.01320.pdf">[PDF]</a>  <a href="https://github.com/AlextheEngineer/Ego3DHands">[Code]</a></h5>
<p><em>Fanqing Lin, Connor Wilhelm, Tony Martinez</em></p>
<h5 id="2021-wacv-mvhm-a-large-scale-multi-view-hand-mesh-benchmark-for-accurate-3d-hand-pose-estimation-pdf">• [2021 WACV] MVHM: A Large-Scale Multi-View Hand Mesh Benchmark for Accurate 3D Hand Pose Estimation. <a href="https://arxiv.org/abs/2012.03206">[PDF]</a></h5>
<p><em>Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Xiaohui Xie</em></p>
<h5 id="2021-wacv-temporal-aware-self-supervised-learning-for-3d-hand-pose-and-mesh-estimation-in-videos-pdf">• [2021 WACV] Temporal-Aware Self-Supervised Learning for 3D Hand Pose and Mesh Estimation in Videos. <a href="https://arxiv.org/abs/2012.03205">[PDF]</a></h5>
<p><em>Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Xiaohui Xie</em></p>
<h5 id="2021-petra-weakly-supervised-hand-part-segmentation-from-depth-images-pdf">• [2021 PETRA] Weakly-supervised hand part segmentation from depth images. <a href="https://dl.acm.org/doi/10.1145/3453892.3453902">[PDF]</a></h5>
<p><em>Mohammad Rezaei, Farnaz Farahanipad, Alex Dillhoff, Ramez Elmasri, Vassilis Athitsos</em></p>
<h5 id="2021-petra-a-pipeline-for-hand-2-d-keypoint-localization-using-unpaired-image-to-image-translation-pdf">• [2021 PETRA] A Pipeline for Hand 2-D Keypoint Localization Using Unpaired Image to Image Translation. <a href="https://dl.acm.org/doi/10.1145/3453892.3453904">[PDF]</a></h5>
<p><em>Farnaz Farahanipad, Mohammad Rezaei, Alex Dillhoff, Farhad Kamangar, Vassilis Athitsos</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2020-eccv">2020 ECCV</h3>
<h5 id="grab-a-dataset-of-whole-body-human-grasping-of-objects-pdf-project-code">• GRAB: A Dataset of Whole-Body Human Grasping of Objects. <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490562.pdf">[PDF]</a> <a href="https://grab.is.tue.mpg.de/">[Project]</a> <a href="https://github.com/otaheri/GrabNet">[Code]</a></h5>
<p><em>Omid Taheri, Nima Ghorbani, Michael J. Black, Dimitrios Tzionas</em></p>
<h5 id="monocular-expressive-body-regression-through-body-driven-attention-pdf-project-code">• Monocular Expressive Body Regression through Body-Driven Attention. <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550018.pdf">[PDF]</a> <a href="https://expose.is.tue.mpg.de/en">[Project]</a> <a href="https://github.com/vchoutas/expose">[Code]</a></h5>
<p><em>Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas , Michael J. Black</em></p>
<h5 id="the-phong-surface-efficient-3d-model-fitting-using-lifted-optimization-pdf-oral">• The Phong Surface: Efficient 3D Model Fitting using Lifted Optimization. <a href="https://arxiv.org/pdf/2007.04940">[PDF]</a> <em>(Oral)</em></h5>
<p><em>Jingjing Shen, Thomas J. Cashman, Qi Ye, Tim Hutton, Toby Sharp, Federica Bogo, Andrew William Fitzgibbon, Jamie Shotton</em></p>
<h5 id="whole-body-human-pose-estimation-in-the-wild-pdf-code">• Whole-Body Human Pose Estimation in the Wild. <a href="https://arxiv.org/pdf/2007.11858.pdf">[PDF]</a> <a href="https://github.com/jin-s13/COCO-WholeBody">[Code]</a></h5>
<p><em>Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo</em></p>
<h5 id="dual-grid-net-hand-mesh-vertex-regression-from-single-depth-maps-pdf">• Dual Grid Net: hand mesh vertex regression from single depth maps. <a href="https://arxiv.org/pdf/1907.10695.pdf">[PDF]</a></h5>
<p><em>Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao</em></p>
<h5 id="hand-transformer-non-autoregressive-structured-modeling-for-3d-hand-pose-estimation-pdf">• Hand-Transformer: Non-Autoregressive Structured Modeling for 3D Hand Pose Estimation. <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700018.pdf">[PDF]</a></h5>
<p><em>Lin Huang, Jianchao Tan, Ji Liu, and Junsong Yuan</em></p>
<h5 id="contactpose-a-dataset-of-grasps-with-object-contact-and-hand-pose-pdf-project-code">• ContactPose: A Dataset of Grasps with Object Contact and Hand Pose. <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580358.pdf">[PDF]</a>  <a href="https://contactpose.cc.gatech.edu/">[Project]</a> <a href="https://github.com/facebookresearch/ContactPose">[Code]</a></h5>
<p><em>Samarth Brahmbhatt, Chengcheng Tang, Chris Twigg, Charles C. Kemp, and James Hays</em></p>
<h5 id="seqhand-rgb-sequence-based-3d-hand-pose-and-shape-estimation-pdf">• SeqHAND: RGB-Sequence-Based 3D Hand Pose and Shape Estimation. <a href="http://arxiv.org/pdf/2007.05168">[PDF]</a></h5>
<p><em>John Yang, Hyung Jin Chang, Seungeui Lee, Nojun Kwak</em></p>
<h5 id="html-a-parametric-hand-texture-model-for-3d-hand-reconstruction-and-personalizationm-pdf-project">• HTML: A Parametric Hand Texture Model for 3D Hand Reconstruction and Personalizationm. <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560052.pdf">[PDF]</a> <a href="https://handtracker.mpi-inf.mpg.de/projects/HandTextureModel/">[Project]</a></h5>
<p><em>Neng Qian, Jiayi Wang, Franziska Mueller, Florian Bernard, Vladislav Golyanik, Christian Theobalt</em></p>
<h5 id="jgr-p2o-joint-graph-reasoning-based-pixel-to-offset-prediction-network-for-3d-hand-pose-estimation-from-a-single-depth-image-pdf-code-spotlight">• JGR-P2O: Joint Graph Reasoning based Pixel-to-Offset Prediction Network for 3D Hand Pose Estimation from a Single Depth Image. <a href="https://arxiv.org/pdf/2007.04646">[PDF]</a>  <a href="https://github.com/fanglinpu/JGR-P2O">[Code]</a> <em>(Spotlight)</em></h5>
<p><em>Linpu Fang, Xingyan Liu, Li Liu, Hang Xu, Wenxiong Kang</em></p>
<h5 id="adaptive-computationally-efficient-network-for-monocular-3d-hand-pose-estimation-pdf-spotlight">• Adaptive Computationally Efficient Network for Monocular 3D Hand Pose Estimation. <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490120.pdf">[PDF]</a> <em>(Spotlight)</em></h5>
<p><em>Zhipeng Fan, Jun Liu, Yao Wang</em></p>
<h5 id="collaborative-learning-of-gesture-recognition-and-3d-hand-pose-estimation-with-multi-order-feature-analysis-pdf-spotlight">• Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-Order Feature Analysis.  <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480766.pdf">[PDF]</a> <em>(Spotlight)</em></h5>
<p><em>Siyuan Yang, Jun Liu, Shijian Lu, Meng Hwa Er, Alex C. Kot</em></p>
<h5 id="deephandmesh-weakly-supervised-deep-encoder-decoder-framework-for-high-fidelity-hand-mesh-modeling-from-a-single-rgb-image-pdf-project-oral">• DeepHandMesh: Weakly-supervised Deep Encoder-Decoder Framework for High-fidelity Hand Mesh Modeling from a Single RGB Image. <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470426.pdf">[PDF]</a> <a href="https://mks0601.github.io/DeepHandMesh/">[Project]</a> <em>(Oral)</em></h5>
<p><em>Gyeongsik Moon, Takaaki Shiratori, Kyoung Mu Lee</em></p>
<h5 id="interhand26m-a-new-large-scale-dataset-and-baseline-for-3d-single-and-interacting-hand-pose-estimation-from-a-single-rgb-image-pdf-project-code">• InterHand2.6M: A New Large-scale Dataset and Baseline for 3D Single and Interacting Hand Pose Estimation from a Single RGB Image. <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650545.pdf">[PDF]</a> <a href="https://mks0601.github.io/InterHand2.6M/">[Project]</a> [<a href="https://github.com/facebookresearch/InterHand2.6M">Code]</a></h5>
<p><em>Gyeongsik Moon, Shoou-i Yu, He Wen, Takaaki Shiratori, Kyoung Mu Lee</em></p>
<h5 id="i2l-meshnet-image-to-lixel-prediction-network-for-accurate-3d-human-pose-and-mesh-estimation-from-a-single-rgb-image-pdf-code">• I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image. <a href="https://arxiv.org/abs/2008.03713">[PDF]</a> [<a href="https://github.com/mks0601/I2L-MeshNet_RELEASE">Code]</a></h5>
<p><em>Gyeongsik Moon, Kyoung Mu Lee</em></p>
<h5 id="weakly-supervised-3d-hand-pose-estimation-via-biomechanical-constraints-pdf">• Weakly-Supervised 3D Hand Pose Estimation via Biomechanical Constraints. <a href="https://arxiv.org/pdf/2003.09282.pdf">[PDF]</a></h5>
<p><em>Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Otmar Hilliges, Jan Kautz</em></p>
<h5 id="measuring-generalisation-to-unseen-viewpoints-articulations-shapes-and-objects-for-3d-hand-pose-estimation-under-hand-object-interaction-pdf-code">• Measuring Generalisation to Unseen Viewpoints, Articulations, Shapes and Objects for 3D Hand Pose Estimation under Hand-Object Interaction. <a href="https://arxiv.org/pdf/2003.13764.pdf">[PDF]</a> [<a href="https://github.com/anilarmagan/HANDS19-Challenge-Toolbox">Code]</a></h5>
<p><em>Anil Armagan, Guillermo Garcia-Hernando, Seungryul Baek, Shreyas Hampali, Mahdi Rad, Zhaohui Zhang, Shipeng Xie, MingXiu Chen, Boshen Zhang, Fu Xiong, Yang Xiao, Zhiguo Cao, Junsong Yuan, Pengfei Ren, Weiting Huang, Haifeng Sun, Marek Hrúz, Jakub Kanis, Zdeněk Krňoul, Qingfu Wan, Shile Li, Linlin Yang, Dongheui Lee, Angela Yao, Weiguo Zhou, Sijia Mei, Yunhui Liu, Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Philippe Weinzaepfel, Romain Brégier, Gregory Rogez, Vincent Lepetit, Tae-Kyun Kim</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2020-cvpr">2020 CVPR</h3>
<h5 id="weakly-supervised-mesh-convolutional-hand-reconstruction-in-the-wild-pdf-project-oral-paper-award-nominees">• Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild. <a href="https://arxiv.org/pdf/2004.01946.pdf">[PDF]</a> <a href="https://www.arielai.com/mesh_hands/">[Project]</a>  <em>(Oral)</em> <em>(<a href="http://cvpr2020.thecvf.com/node/817">Paper Award Nominees</a>)</em></h5>
<p><em>Dominik Kulon, Riza Alp Güler, Iasonas Kokkinos, Michael Bronstein, Stefanos Zafeiriou</em></p>
<h5 id="weakly-supervised-domain-adaptation-via-gan-and-mesh-model-for-estimating-3d-hand-poses-interacting-objects-pdf-code-oral-paper-award-nominees">• Weakly-supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects.  <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Baek_Weakly-Supervised_Domain_Adaptation_via_GAN_and_Mesh_Model_for_Estimating_CVPR_2020_paper.pdf">[PDF]</a> <a href="https://github.com/bsrvision/weak_da_hands">[Code]</a> <em>(Oral)</em> <em>(<a href="http://cvpr2020.thecvf.com/node/817">Paper Award Nominees</a>)</em></h5>
<p><em>Seungryul Baek, Kwang In Kim, Tae-Kyun Kim</em></p>
<h5 id="ganhand-predicting-human-grasp-affordances-in-multi-object-scenes-pdf">• GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes. <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Corona_GanHand_Predicting_Human_Grasp_Affordances_in_Multi-Object_Scenes_CVPR_2020_paper.pdf">[PDF]</a></h5>
<p><em>Enric Corona, Albert Pumarola, Guillem Alenya, Francesc Moreno-Noguer, Gregory Rogez</em></p>
<h5 id="cross-modal-variational-alignment-of-latent-spaces-pdf">• Cross-Modal Variational Alignment of Latent Spaces. <a href="http://openaccess.thecvf.com/content_CVPRW_2020/papers/w56/Theodoridis_Cross-Modal_Variational_Alignment_of_Latent_Spaces_CVPRW_2020_paper.pdf">[PDF]</a></h5>
<p><em>Thomas Theodoridis, Theocharis Chatzis, Vassilios Solachidis, Kosmas Dimitropoulos, Petros Daras</em></p>
<h5 id="humbi-a-large-multiview-dataset-of-human-body-expressions-pdf-project">• HUMBI: A Large Multiview Dataset of Human Body Expressions. <a href="https://arxiv.org/pdf/1812.00281.pdf">[PDF]</a> <a href="https://humbi-data.net/hand/">[Project]</a></h5>
<p><em>Zhixuan Yu*, Jae Shin Yoon*, Prashanth Venkatesh, Jaesik Park, Jihun Yu, Hyun Soo Park</em></p>
<h5 id="epipolar-transformers-pdf-code">• Epipolar Transformers. <a href="https://arxiv.org/pdf/2005.04551.pdf">[PDF]</a> <a href="https://github.com/yihui-he/epipolar-transformers">[Code]</a></h5>
<p><em>Yihui He*, Rui Yan*, Shoou-I Yu, Katerina Fragkiadaki</em></p>
<h5 id="handvoxnet-deep-voxel-based-network-for-3d-hand-shape-and-pose-estimation-from-a-single-depth-map-pdf">• HandVoxNet: Deep Voxel-Based Network for 3D Hand Shape and Pose Estimation from a Single Depth Map. <a href="https://arxiv.org/pdf/2004.01588.pdf">[PDF]</a></h5>
<p><em>Jameel Malik, Ibrahim Abdelaziz, Ahmed Elhayek, Soshi Shimada, Sk Aziz Ali, Vladislav Golyanik, Christian Theobalt, Didier Stricker</em></p>
<h5 id="knowledge-as-priors-cross-modal-knowledge-generalization-for-datasets-without-superior-knowledge-pdf">• Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge. <a href="https://arxiv.org/pdf/2004.00176.pdf">[PDF]</a></h5>
<p><em>Long Zhao, Xi Peng, Yuxiao Chen, Mubbasir Kapadia, Dimitris N. Metaxas</em></p>
<h5 id="leveraging-photometric-consistency-over-time-for-sparsely-supervised-hand-object-reconstruction-pdf-project-code">• Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction. <a href="http://arxiv.org/pdf/2004.13449.pdf">[PDF]</a> <a href="https://hassony2.github.io/handobjectconsist.html">[Project]</a> <a href="https://github.com/hassony2/handobjectconsist">[Code]</a></h5>
<p><em>Yana Hasson, Bugra Tekin, Federica Bogo, Ivan Laptev, Marc Pollefeys, Cordelia Schmid</em></p>
<h5 id="monocular-real-time-hand-shape-and-motion-capture-using-multi-modal-data-pdf-project-code">• Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data. <a href="https://arxiv.org/pdf/2003.09572.pdf">[PDF]</a> <a href="https://calciferzh.github.io/publications/zhou2020monocular">[Project]</a> <a href="https://github.com/CalciferZh/minimal-hand">[Code]</a></h5>
<p><em>Yuxiao Zhou, Marc Habermann, Weipeng Xu, Ikhsanul Habibie, Christian Theobalt, Feng Xu</em></p>
<h5 id="hope-net-a-graph-based-model-for-hand-object-pose-estimation-pdf-code">• HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation. <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Doosti_HOPE-Net_A_Graph-Based_Model_for_Hand-Object_Pose_Estimation_CVPR_2020_paper.pdf">[PDF]</a> <a href="https://github.com/bardiadoosti/HOPE">[Code]</a></h5>
<p><em>Bardia Doosti, Shujon Naha, David Crandall, Majid Mirbagheri</em></p>
<h5 id="honnotate-a-method-for-3d-annotation-of-hand-and-objects-poses-pdf-project-code">• HOnnotate: A method for 3D Annotation of Hand and Objects Poses. <a href="https://arxiv.org/pdf/1907.01481.pdf">[PDF]</a> <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/research-projects/hand-object-3d-pose-annotation/">[Project]</a> <a href="https://github.com/shreyashampali/ho3d">[Code]</a></h5>
<p><em>Shreyas Hampali, Mahdi Rad, Markus Oberweger, Vincent Lepetit</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2020-others">2020 Others</h3>
<h5 id="2020-icra-robust-occlusion-aware-pose-estimation-for-objects-grasped-by-adaptive-hands-pdf-code">• [2020 ICRA] Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands . <a href="https://arxiv.org/pdf/2003.03518.pdf">[PDF]</a> <a href="https://github.com/wenbowen123/icra20-hand-object-pose">[Code]</a></h5>
<p><em>Bowen Wen, Chaitanya Mitash, Sruthi Soorian, Andrew Kimmel, Avishai Sintov, Kostas E. Bekris</em></p>
<h5 id="2020-cvprw-mediapipe-hands-on-device-real-time-hand-tracking-pdf">• [2020 CVPRW] MediaPipe Hands: On-device Real-time Hand Tracking. <a href="https://arxiv.org/pdf/2006.10214.pdf">[PDF]</a></h5>
<p><em>Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George Sung, Chuo-Ling Chang, Matthias Grundmann</em></p>
<h5 id="2020-ismar-3d-hand-pose-estimation-with-a-single-infrared-camera-via-domain-transfer-learning-pdf">• [2020 ISMAR] 3D Hand Pose Estimation with a Single Infrared Camera via Domain Transfer Learning. <a href="https://drive.google.com/file/d/1rBmsblA2YY4qDUxY4GRgrLQ0EyA3wbFs/view">[PDF]</a></h5>
<p><em>Gabyong Park, Tae-Kyun Kim, Woontack Woo</em></p>
<h5 id="2020-ismar-bare-hand-depth-inpainting-for-3d-tracking-of-hand-interacting-with-object-pdf">• [2020 ISMAR] Bare-hand Depth Inpainting for 3D Tracking of Hand Interacting with Object. <a href="https://drive.google.com/file/d/1hwOATe-2Sr5CdQD98SlIYlpCAGRQ109N/view">[PDF]</a></h5>
<p><em>Woojin Cho, Gabyong Park, Woontack Woo</em></p>
<h5 id="2020-3dv-grasping-field-learning-implicit-representations-for-human-grasps-pdf-code-best-paper-award">• [2020 3DV] Grasping Field: Learning Implicit Representations for Human Grasps. <a href="https://arxiv.org/pdf/2008.04451.pdf">[PDF]</a> <a href="https://github.com/korrawe/grasping_field">[Code]</a> <em>(Best Paper Award)</em></h5>
<p><em>Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael Black, Krikamol Muandet, Siyu Tang</em></p>
<h5 id="2020-uist-deepfisheye-near-surface-multi-finger-tracking-technology-using-fisheye-camera-pdf-project-code">• [2020 UIST] DeepFisheye: Near-Surface Multi-Finger Tracking Technology Using Fisheye Camera. <a href="https://dl.acm.org/doi/abs/10.1145/3379337.3415818">[PDF]</a>  <a href="http://kwpark.io/deepfisheye">[Project]</a> <a href="https://github.com/KAIST-HCIL/DeepFisheyeNet">[Code]</a></h5>
<p><em>Keunwoo Park, Sunbum Kim, Youngwoo Yoon, Tae-Kyun Kim, Geehyuk Lee</em></p>
<h5 id="2020-siggraph-asia-constraining-dense-hand-surface-tracking-with-elasticity-pdf-project">• [2020 SIGGRAPH Asia] Constraining Dense Hand Surface Tracking With Elasticity. <a href="https://research.fb.com/wp-content/uploads/2020/11/Constraining-Dense-Hand-Surface-Tracking-with-Elasticity.pdf">[PDF]</a>  <a href="https://research.fb.com/publications/constraining-dense-hand-surface-tracking-with-elasticity/">[Project]</a></h5>
<p><em>Breannan Smith, Chenglei Wu, He Wen, Patrick Peluse, Yaser Sheikh, Jessica Hodgins, Takaaki Shiratori</em></p>
<h5 id="2020-siggraph-asia-rgb2hands-real-time-tracking-of-3d-hand-interactions-from-monocular-rgb-video-pdf-project">• [2020 SIGGRAPH Asia] RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB Video. <a href="https://handtracker.mpi-inf.mpg.de/projects/RGB2Hands/content/RGB2Hands_author_version.pdf">[PDF]</a>  <a href="https://handtracker.mpi-inf.mpg.de/projects/RGB2Hands/">[Project]</a></h5>
<p><em>Jiayi Wang, Franziska Mueller, Florian Bernard, Suzanne Sorli, Oleksandr Sotnychenko, Neng Qian, Miguel A. Otaduy, Dan Casas, and Christian Theobalt</em></p>
<h5 id="2020-siggraph-megatrack-monochrome-egocentric-articulated-hand-tracking-for-virtual-reality-pdf">• [2020 SIGGRAPH] MEgATrack: Monochrome Egocentric Articulated Hand-Tracking for Virtual Reality. <a href="https://dl.acm.org/doi/abs/10.1145/3386569.3392452">[PDF]</a></h5>
<p><em>Shangchen Han, Beibei Liu, Randi Cabezas, Christopher D. Twigg, Peizhao Zhang, Jeff Petkau, Tsz-Ho Yu, Chun-Jung Tai, Muzaffer Akbay, Zheng Wang, Asaf Nitzan, Gang Dong, Yuting Ye, Lingling Tao, Chengde Wan, Robert Wang</em></p>
<h5 id="2020-mm-mm-hand-3d-aware-multi-modal-guided-hand-generation-for-3d-hand-pose-synthesis-pdf-code">• [2020 MM] MM-Hand: 3D-Aware Multi-Modal Guided Hand Generation for 3D Hand Pose Synthesis. <a href="http://vllab.cs.nctu.edu.tw/images/paper/mm-wu20.pdf">[PDF]</a> <a href="https://github.com/ScottHoang/mm-hand">[Code]</a></h5>
<p><em>Zhenyu Wu, Duc Hoang, Shih-Yao Lin, Yusheng Xie, Liangjian Chen, Yen-Yu Lin, Zhangyang Wang, Wei Fan</em></p>
<h5 id="2020-mm-adaptive-wasserstein-hourglass-for-weakly-supervised-hand-pose-estimation-from-monocular-rgb-pdf">• [2020 MM] Adaptive Wasserstein Hourglass for Weakly Supervised Hand Pose Estimation from Monocular RGB. <a href="https://arxiv.org/pdf/1909.05666.pdf">[PDF]</a></h5>
<p><em>Yumeng Zhang, Li Chen, Yufeng Liu, Junhai Yong, Wen Zheng</em></p>
<h5 id="2020-mm-hot-net-non-autoregressive-transformer-for-3d-hand-object-pose-estimation-pdf">• [2020 MM] HOT-Net: Non-Autoregressive Transformer for 3D Hand-Object Pose Estimation. <a href="https://cse.buffalo.edu/~jsyuan/papers/2020/lin_mm20.pdf">[PDF]</a></h5>
<p><em>Lin Huang, Jianchao Tan, Jingjing Meng, Ji Liu, and Junsong Yuan</em></p>
<h5 id="2020-bmvc-pmd-net-privileged-modality-distillation-network-for-3d-hand-pose-estimation-from-a-single-rgb-image-pdf">• [2020 BMVC] PMD-Net: Privileged Modality Distillation Network for 3D Hand Pose Estimation from a Single RGB Image. <a href="https://www.bmvc2020-conference.com/assets/papers/0413.pdf">[PDF]</a></h5>
<p><em>Kewen Wang and Xilin Chen</em></p>
<h5 id="2020-bmvc-sia-gcn-a-spatial-information-aware-graph-neural-network-with-2d-convolutions-for-hand-pose-estimation-pdf">• [2020 BMVC] SIA-GCN: A Spatial Information Aware Graph Neural Network with 2D Convolutions for Hand Pose Estimation. <a href="https://www.bmvc2020-conference.com/assets/papers/0066.pdf">[PDF]</a></h5>
<p><em>Deying Kong, Haoyu Ma and Xiaohui Xie</em></p>
<h5 id="2020-bmvc-explicit-knowledge-distillation-for-3d-hand-pose-estimation-from-monocular-rgb-pdf">• [2020 BMVC] Explicit Knowledge Distillation for 3D Hand Pose Estimation from Monocular RGB. <a href="https://www.bmvc2020-conference.com/assets/papers/0242.pdf">[PDF]</a></h5>
<p><em>Yumeng Zhang, Li Chen, Yufeng Liu, Wen Zheng and JunHai Yong</em></p>
<h5 id="2020-bmvc-bihand-recovering-hand-mesh-with-multi-stage-bisected-hourglass-networks-pdf-code">• [2020 BMVC] BiHand: Recovering Hand Mesh with Multi-stage Bisected Hourglass Networks. <a href="https://arxiv.org/pdf/2008.05079.pdf">[PDF]</a> <a href="https://github.com/lixiny/bihand">[Code]</a></h5>
<p><em>Lixin Yang, Jiasen Li, Wenqiang Xu, Yiqun Diao, Cewu Lu</em></p>
<h5 id="2020-ubicomp-fingertrak-continuous-3d-hand-pose-tracking-by-deep-learning-hand-silhouettes-captured-by-miniature-thermal-cameras-on-wrist-pdf-eccv-2020-demo-award-nominee">• [2020 Ubicomp] FingerTrak: Continuous 3D Hand Pose Tracking by Deep Learning Hand Silhouettes Captured by Miniature Thermal Cameras on Wrist. <a href="https://dl.acm.org/doi/10.1145/3397306">[PDF]</a> (<em>ECCV 2020 Demo Award Nominee</em>)</h5>
<p><em>Fang Hu, Peng He, Songlin Xu, Yin Li, Cheng Zhang</em></p>
<h5 id="2020-fg-hand-tracking-from-monocular-rgb-with-dense-semantic-labels-pdf">• [2020 FG] Hand tracking from monocular RGB with dense semantic labels. <a href="https://www.computer.org/csdl/pds/api/csdl/proceedings/download-article/1kecISCOYgw/pdf">[PDF]</a></h5>
<p><em>Peter Thompson, Aphrodite Galata</em></p>
<h5 id="2020-fg-generative-model-based-loss-to-the-rescue-a-method-to-overcome-annotation-errors-for-depth-based-hand-pose-estimation-pdf">• [2020 FG] Generative Model-Based Loss to the Rescue: A Method to Overcome Annotation Errors for Depth-Based Hand Pose Estimation. <a href="https://arxiv.org/pdf/2007.03073.pdf">[PDF]</a></h5>
<p><em>Jiayi Wang, Franziska Mueller, Florian Bernard, Christian Theobalt</em></p>
<h5 id="2020-iros-physics-based-dexterous-manipulations-with-estimated-hand-poses-and-residual-reinforcement-learning-pdf">• [2020 IROS] Physics-Based Dexterous Manipulations with Estimated Hand Poses and Residual Reinforcement Learning. <a href="https://arxiv.org/pdf/2008.03285">[PDF]</a></h5>
<p><em>Guillermo Garcia-Hernando, Edward Johns, Tae-Kyun Kim</em></p>
<h5 id="2020-chi-evaluation-of-machine-learning-techniques-for-hand-pose-estimation-on-handheld-device-with-proximity-sensor-pdf">• [2020 CHI] Evaluation of Machine Learning Techniques for Hand Pose Estimation on Handheld Device with Proximity Sensor. <a href="https://dl.acm.org/doi/pdf/10.1145/3313831.3376712">[PDF]</a></h5>
<p><em>Kazuyuki Arimatsu, Hideki Mori</em></p>
<h5 id="2020-aaai-awr-adaptive-weighting-regression-for-3d-hand-pose-estimation-pdf-code">• [2020 AAAI] AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation. <a href="https://www.aaai.org//Papers//AAAI//2020GB//AAAI-HuangW.4059.pdf">[PDF]</a>  <a href="https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression">[Code]</a></h5>
<p><em>Weiting Huang, Pengfei Ren, Jingyu Wang, Qi Qi, Haifeng Sun</em></p>
<h5 id="2020-wacv-nonparametric-structure-regularization-machine-for-2d-hand-pose-estimation-pdf-code">• [2020 WACV] Nonparametric Structure Regularization Machine for 2D Hand Pose Estimation. <a href="https://arxiv.org/pdf/2001.08869.pdf">[PDF]</a> <a href="https://github.com/HowieMa/NSRMhand">[Code]</a></h5>
<p><em>Yifei Chen*, Haoyu Ma*, Deying Kong, Xiangyi Yan, Jianbao Wu, Wei Fan, Xiaohui Xie</em></p>
<h5 id="2020-wacv-3d-hand-pose-estimation-with-disentangled-cross-modal-latent-space-pdf">• [2020 WACV] 3D Hand Pose Estimation with Disentangled Cross-Modal Latent Space. <a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Gu_3D_Hand_Pose_Estimation_with_Disentangled_Cross-Modal_Latent_Space_WACV_2020_paper.pdf">[PDF]</a></h5>
<p><em>Jiajun Gu, Zhiyong Wang, Wanli Ouyang, Weichen Zhang, Jiafeng Li, Li Zhuo</em></p>
<h5 id="2020-wacv-dggan-depth-image-guided-generative-adversarial-networks-for-disentangling-rgb-and-depth-images-in-3d-hand-pose-estimation-pdf">• [2020 WACV] DGGAN: Depth-image Guided Generative Adversarial Networks for Disentangling RGB and Depth Images in 3D Hand Pose Estimation. <a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_DGGAN_Depth-image_Guided_Generative_Adversarial_Networks_forDisentangling_RGB_and_Depth_WACV_2020_paper.pdf">[PDF]</a></h5>
<p><em>Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Wei Fan, Xiaohui Xie</em></p>
<h5 id="2020-wacv-rotation-invariant-mixed-graphical-model-network-for-2d-hand-pose-estimation-pdf">• [2020 WACV] Rotation-invariant Mixed Graphical Model Network for 2D Hand Pose Estimation. <a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Kong_Rotation-invariant_Mixed_Graphical_Model_Network_for_2D_Hand_Pose_Estimation_WACV_2020_paper.pdf">[PDF]</a></h5>
<p><em>Deying Kong, Haoyu Ma, Yifei Chen, Xiaohui Xie</em></p>
<h5 id="2020-icassp-weakly-supervised-segmentation-guided-hand-pose-estimation-during-interaction-with-unknown-objects-pdf">• [2020 ICASSP] Weakly Supervised Segmentation Guided Hand Pose Estimation During Interaction With Unknown Objects. <a href="https://ieeexplore.ieee.org/abstract/document/9053082">[PDF]</a></h5>
<p><em>Cairong Zhang, Guijin Wang, Xinghao Chen, Pengwei Xie, Toshihiko Yamasaki</em></p>
<h5 id="2020-icassp-hand-3d-studio-a-new-multi-view-system-for-3d-hand-reconstruction-pdf-project">• [2020 ICASSP] Hand-3D-Studio: A New Multi-view System for 3D Hand Reconstruction. <a href="https://www.yangangwang.com/papers/ZHAO-H3D-2020-02.pdf">[PDF]</a> <a href="https://www.yangangwang.com/papers/ZHAO-H3S-2020-02.html">[Project]</a></h5>
<p><em>Zhengyi Zhao, Tianyao Wang, Siyu Xia, Yangang Wang</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2019-iccv">2019 ICCV</h3>
<h5 id="freihand-a-dataset-for-markerless-capture-of-hand-pose-and-shape-from-single-rgb-images-pdf-project-code">• FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from Single RGB Images. <a href="https://arxiv.org/pdf/1909.04349.pdf">[PDF]</a>  <a href="https://lmb.informatik.uni-freiburg.de/projects/freihand/">[Project]</a> <a href="https://github.com/lmb-freiburg/freihand">[Code]</a></h5>
<p><em>Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, Thomas Brox</em></p>
<h5 id="a2j-anchor-to-joint-regression-network-for-3d-articulated-pose-estimation-from-a-single-depth-image-pdf-code">• A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image. <a href="https://cse.buffalo.edu/~jsyuan/papers/2019/A2J.pdf">[PDF]</a> <a href="https://github.com/zhangboshen/A2J">[Code]</a></h5>
<p><em>Fu Xiong*, Boshen Zhang*, Yang Xiao, Zhiguo Cao, Taidong Yu, Joey Tianyi Zhou, and Junsong Yuan</em></p>
<h5 id="exploiting-spatial-temporal-relationships-for-3d-pose-estimation-via-graph-convolutional-networks-pdf">• Exploiting Spatial-temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks. <a href="https://cse.buffalo.edu/~jsyuan/papers/2019/Exploiting_Spatial-temporal_Relationships_for_3D_Pose_Estimation_via_Graph_Convolutional_Networks.pdf">[PDF]</a></h5>
<p><em>Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat Thalmann</em></p>
<h5 id="resolving-3d-human-pose-ambiguities-with-3d-scene-constraints-pdf-project">• Resolving 3D Human Pose Ambiguities with 3D Scene Constraints. <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/530/ICCV_2019___PROX.pdf">[PDF]</a>  <a href="https://prox.is.tue.mpg.de/">[Project]</a></h5>
<p><em>Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas and Michael J. Black</em></p>
<h5 id="so-handnet-self-organizing-network-for-3d-hand-pose-estimation-with-semi-supervised-learning-pdf-code">• SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning. <a href="https://drive.google.com/file/d/11GJzouV6jt_aOpvrJ8l3J5x_R_-m-Lg8/view">[PDF]</a>  <a href="https://github.com/TerenceCYJ/SO-HandNet">[Code]</a></h5>
<p><em>Yujin Chen, Zhigang Tu, Liuhao Ge, Dejun Zhang, Ruizhi Chen, Junsong Yuan</em></p>
<h5 id="end-to-end-hand-mesh-recovery-from-a-monocular-rgb-image-pdf-code">• End-to-end Hand Mesh Recovery from a Monocular RGB Image. <a href="https://arxiv.org/pdf/1902.09305.pdf">[PDF]</a>  <a href="https://github.com/MandyMo/HAMR">[Code]</a></h5>
<p><em>Xiong Zhang*, Qiang Li*, Wenbo Zhang, Wen Zheng</em></p>
<h5 id="aligning-latent-spaces-for-3d-hand-pose-estimation-pdf">• Aligning Latent Spaces for 3D Hand Pose Estimation. <a href="https://www.mu4yang.com/files/papers/Aligning%20Latent%20Spaces%20for%203D%20Hand%20Pose%20Estimation.pdf">[PDF]</a></h5>
<p><em>Linlin Yang*, Shile Li*, Dongheui Lee, Angela Yao</em></p>
<h5 id="hands19-workshop-disentangling-pose-from-appearance-in-monochrome-hand-images-pdf">• [HANDS19 Workshop] Disentangling Pose from Appearance in Monochrome Hand Images. <a href="https://arxiv.org/pdf/1904.07528.pdf">[PDF]</a></h5>
<p><em>Yikang Li, Chris Twigg, Yuting Ye, Lingling Tao, Xiaogang Wang</em></p>
<h5 id="hands19-workshop-rgb-based-3d-hand-pose-estimation-via-privileged-learning-with-depth-images-pdf">• [HANDS19 Workshop] RGB-based 3D Hand Pose Estimation via Privileged Learning with Depth Images. <a href="https://arxiv.org/pdf/1811.07376.pdf">[PDF]</a></h5>
<p><em>Shanxin Yuan, Bjorn Stenger, Tae-Kyun Kim</em></p>
<h5 id="hands19-workshop-explicit-pose-deformation-learning-for-tracking-human-poses-pdf">• [HANDS19 Workshop] Explicit Pose Deformation Learning for Tracking Human Poses. <a href="https://arxiv.org/pdf/1811.07123.pdf">[PDF]</a></h5>
<p><em>Xiao Sun, Chuankang Li, Stephen Lin</em></p>
<h5 id="hands19-workshop-hand-pose-ensemble-learning-based-on-grouping-features-of-hand-point-sets-pdf">• [HANDS19 Workshop] Hand Pose Ensemble Learning based on Grouping Features of Hand Point Sets. [PDF]</h5>
<p><em>Tianqiang Zhu, Yi Sun, Xiaohong Ma, Xiangbo Lin</em></p>
<h3 id="2019-cvpr">2019 CVPR</h3>
<h5 id="disentangling-latent-hands-for-image-synthesis-and-pose-estimation-pdf">• Disentangling Latent Hands for Image Synthesis and Pose Estimation. <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Disentangling_Latent_Hands_for_Image_Synthesis_and_Pose_Estimation_CVPR_2019_paper.pdf">[PDF]</a></h5>
<p><em>Linlin Yang, Angela Yao</em></p>
<h5 id="point-to-pose-voting-based-hand-pose-estimation-using-residual-permutation-equivariant-layer-pdf">• Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation Equivariant Layer. <a href="https://arxiv.org/pdf/1812.02050.pdf">[PDF]</a></h5>
<p><em>Shile Li, Dongheui Lee</em></p>
<h5 id="ho-unified-egocentric-recognition-of-3d-hand-object-poses-and-interactions-pdf-oral">• H•O: Unified Egocentric Recognition of 3D Hand-Object Poses and Interactions. <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Tekin_HO_Unified_Egocentric_Recognition_of_3D_Hand-Object_Poses_and_Interactions_CVPR_2019_paper.pdf">[PDF]</a>  <em>(Oral)</em></h5>
<p><em>Bugra Tekin, Federica Bogo, Marc Pollefeys</em></p>
<h5 id="self-supervised-3d-hand-pose-estimation-pdf-code-oral-best-paper-finalists">• Self supervised 3D hand pose estimation. <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wan_Self-Supervised_3D_Hand_Pose_Estimation_Through_Training_by_Fitting_CVPR_2019_paper.pdf">[PDF]</a> <a href="https://github.com/melonwan/sphereHand">[Code]</a> <em>(Oral)</em> <em>(<a href="http://cvpr2019.thecvf.com/files/CVPR%202019%20-%20Welcome%20Slides%20Final.pdf">Best Paper Finalists</a>)</em></h5>
<p><em>Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao</em></p>
<h5 id="crossinfonet-multi-task-information-sharing-based-hand-pose-estimation-pdf-code">• CrossInfoNet: Multi-Task Information Sharing Based Hand Pose Estimation. <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Du_CrossInfoNet_Multi-Task_Information_Sharing_Based_Hand_Pose_Estimation_CVPR_2019_paper.pdf">[PDF]</a> <a href="https://github.com/dumyy/handpose">[Code]</a></h5>
<p><em>Kuo Du, Xiangbo Lin, Yi Sun, Xiaohong Ma</em></p>
<h5 id="expressive-body-capture-3d-hands-face-and-body-from-a-single-image-pdf-project-code-oral">• Expressive Body Capture: 3D Hands, Face, and Body from a Single Image.  <a href="https://arxiv.org/pdf/1904.05866">[PDF]</a>  <a href="https://smpl-x.is.tue.mpg.de/">[Project]</a>  <a href="https://github.com/vchoutas/smplify-x">[Code]</a> <em>(Oral)</em></h5>
<p><em>Georgios Pavlakos*, Vasileios Choutas*, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, Michael J. Black</em></p>
<h5 id="learning-joint-reconstruction-of-hands-and-manipulated-objects-pdf-code-code-project">• Learning joint reconstruction of hands and manipulated objects. <a href="https://arxiv.org/pdf/1904.05767.pdf">[PDF]</a> <a href="https://github.com/hassony2/manopth">[Code]</a> <a href="https://github.com/hassony2/obman_train">[Code]</a> <a href="https://www.di.ens.fr/willow/research/obman/">[Project]</a></h5>
<p><em>Yana Hasson, Gül Varol, Dimitris Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, and Cordelia Schmid</em></p>
<h5 id="3d-hand-shape-and-pose-estimation-from-a-single-rgb-image-pdf-project-code-oral">• 3D Hand Shape and Pose Estimation from a Single RGB Image. <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Ge_3D_Hand_Shape_and_Pose_Estimation_From_a_Single_RGB_CVPR_2019_paper.pdf">[PDF]</a> <a href="https://sites.google.com/site/geliuhaontu/home/cvpr2019">[Project]</a> <a href="https://github.com/3d-hand-shape/hand-graph-cnn">[Code]</a> <em>(Oral)</em></h5>
<p><em>Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying Wang, Jianfei Cai, Junsong Yuan</em></p>
<h5 id="3d-hand-shape-and-pose-from-images-in-the-wild-pdf-code-oral">• 3D Hand Shape and Pose from Images in the Wild. <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Boukhayma_3D_Hand_Shape_and_Pose_From_Images_in_the_Wild_CVPR_2019_paper.pdf">[PDF]</a>  <a href="https://github.com/boukhayma/3dhand">[Code]</a> <em>(Oral)</em></h5>
<p><em>Adnane Boukhayma, Rodrigo de Bem, Philip H.S. Torr</em></p>
<h5 id="pushing-the-envelope-for-rgb-based-dense-3d-hand-pose-estimation-via-neural-rendering-pdf">• Pushing the Envelope for RGB-based Dense 3D Hand Pose Estimation via Neural Rendering. <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Baek_Pushing_the_Envelope_for_RGB-Based_Dense_3D_Hand_Pose_Estimation_CVPR_2019_paper.pdf">[PDF]</a></h5>
<p><em>Seungryul Baek, Kwang In Kim, Tae-Kyun Kim</em></p>
<h5 id="monocular-total-capture-posing-face-body-and-hands-in-the-wild-pdf-project-code-oral">• Monocular Total Capture: Posing Face, Body, and Hands in the Wild. <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xiang_Monocular_Total_Capture_Posing_Face_Body_and_Hands_in_the_CVPR_2019_paper.pdf">[PDF]</a> <a href="http://domedb.perception.cs.cmu.edu/monototalcapture.html">[Project]</a> <a href="https://github.com/CMU-Perceptual-Computing-Lab/MonocularTotalCapture">[Code]</a> <em>(Oral)</em></h5>
<p><em>Donglai Xiang, Hanbyul Joo, Yaser Sheikh</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2019-others">2019 Others</h3>
<h5 id="2019-siggraph-interactive-hand-pose-estimation-using-a-stretch-sensing-soft-glove-pdf-project">• [2019 SIGGRAPH] Interactive Hand Pose Estimation using a Stretch-Sensing Soft Glove. <a href="https://cims.nyu.edu/gcl/papers/2019-Capacitive.pdf">[PDF]</a>  <a href="https://igl.ethz.ch/projects/stretch-glove/">[Project]</a></h5>
<p><em>Oliver Glauser, Shihao Wu, Daniele Panozzo, Otmar Hilliges, Olga Sorkine-Hornung</em></p>
<h5 id="2019-siggraph-interactionfusion-real-time-reconstruction-of-hand-poses-and-deformable-objects-in-hand-object-interactions-pdf">• [2019 SIGGRAPH] InteractionFusion: Real-time Reconstruction of Hand Poses and Deformable Objects in Hand-object Interactions. <a href="https://dl.acm.org/citation.cfm?id=3322998">[PDF]</a></h5>
<p><em>Hao Zhang, Zi-Hao Bo, Jun-Hai Yong, Feng Xu</em></p>
<h5 id="2019-siggraph-real-time-pose-and-shape-reconstruction-of-two-interacting-hands-with-a-single-depth-camera-pdf-project">• [2019 SIGGRAPH] Real-time pose and shape reconstruction of two interacting hands with a single depth camera. <a href="https://handtracker.mpi-inf.mpg.de/projects/TwoHands/content/TwoHands_SIGGRAPH2019.pdf">[PDF]</a> <a href="https://handtracker.mpi-inf.mpg.de/projects/TwoHands/">[Project]</a></h5>
<p><em>Franziska Mueller, Micah Davis, Florian Bernard, Oleksandr Sotnychenko, Mickeal Verschoor, Miguel A. Otaduy, Dan Casas, Christian Theobalt</em></p>
<h5 id="2019-fg-deep-conditional-variational-estimation-for-depth-based-hand-poses-pdf">• [2019 FG] Deep Conditional Variational Estimation for Depth-Based Hand Poses. <a href="https://ieeexplore.ieee.org/abstract/document/8756559">[PDF]</a></h5>
<p><em>Lu Xu, Chen Hu, Yinqi Li, Ji’an Tao, Jianru Xue, Kuizhi Mei</em></p>
<h5 id="2019-bmvc-unified-2d-and-3d-hand-pose-estimation-from-a-single-visible-or-x-ray-image-pdf">• [2019 BMVC] Unified 2D and 3D Hand Pose Estimation from a Single Visible or X-ray Image. <a href="https://bmvc2019.org/wp-content/uploads/papers/0931-paper.pdf">[PDF]</a></h5>
<p><em>Akila Pemasiri, Kien Nguyen Thanh, Sridha Sridharan, Clinton Fookes</em></p>
<h5 id="2019-bmvc-tagan-tonality-aligned-generative-adversarial-networks-for-realistic-handpose-synthesis-pdf">• [2019 BMVC] TAGAN: Tonality Aligned Generative Adversarial Networks for Realistic HandPose Synthesis. <a href="https://bmvc2019.org/wp-content/uploads/papers/0408-paper.pdf">[PDF]</a></h5>
<p><em>Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Hui Tang, Yufan Xue, Xiaohui Xie, Yen-Yu Lin, Wei Fan</em></p>
<h5 id="2019-bmvc-single-image-3d-hand-reconstruction-with-mesh-convolutions-pdf-code">• [2019 BMVC] Single Image 3D Hand Reconstruction with Mesh Convolutions. <a href="https://bmvc2019.org/wp-content/uploads/papers/0653-paper.pdf">[PDF]</a> <a href="https://github.com/dkulon/hand-reconstruction">[Code]</a></h5>
<p><em>Dominik Kulon, Haoyang Wang, Riza Alp Güler, Michael Bronstein, Stefanos Zafeiriou</em></p>
<h5 id="2019-bmvc-adaptive-graphical-model-network-for-2d-handpose-estimation-pdf">• [2019 BMVC] Adaptive Graphical Model Network for 2D Handpose Estimation. <a href="https://bmvc2019.org/wp-content/uploads/papers/0907-paper.pdf">[PDF]</a></h5>
<p><em>Deying Kong, Yifei Chen, Haoyu Ma, Xiangyi Yan, Xiaohui Xie</em></p>
<h5 id="2019-bmvc-srn-stacked-regression-network-for-real-time-3d-hand-pose-estimation-pdf">• [2019 BMVC] SRN: Stacked Regression Network for Real-time 3D Hand Pose Estimation. <a href="https://bmvc2019.org/wp-content/uploads/papers/0918-paper.pdf">[PDF]</a></h5>
<p><em>Pengfei Ren, Haifeng Sun, Jingyu Wang, Qi Qi, Weiting Huang</em></p>
<h5 id="2019-bmvc-end-to-end-3d-hand-pose-estimation-from-stereo-cameras-pdf-oral">• [2019 BMVC] End-to-End 3D Hand Pose Estimation from Stereo Cameras. <a href="https://bmvc2019.org/wp-content/uploads/papers/0219-paper.pdf">[PDF]</a>  <em>(Oral)</em></h5>
<p><em>Yuncheng Li, Zehao Xue, Yingying Wang, Liuhao Ge, Zhou Ren, Jonathan Rodriguez</em></p>
<h5 id="2019-accv-hand-pose-estimation-based-on-3d-residual-network-with-data-padding-and-skeleton-steadying-pdf">• [2019 ACCV] Hand Pose Estimation Based on 3D Residual Network with Data Padding and Skeleton Steadying. <a href="https://link.springer.com/chapter/10.1007/978-3-030-20873-8_19">[PDF]</a></h5>
<p><em>Pai-Wen Ting, En-Te Chou, Ya-Hui Tang, Li-Chen Fu</em></p>
<h5 id="2019-icassp-cascaded-point-network-for-3d-hand-pose-estimation-pdf">• [2019 ICASSP] Cascaded Point Network for 3D Hand Pose Estimation. <a href="https://ieeexplore.ieee.org/abstract/document/8683356">[PDF]</a></h5>
<p><em>Yikun Dou, Xuguang Wang, Yuying Zhu, Xiaoming Deng, Cuixia Ma, Liang Chang, Hongan Wang</em></p>
<h5 id="2019-icassp-a-novel-framework-of-hand-localization-and-hand-pose-estimation-pdf">• [2019 ICASSP] A Novel Framework of Hand Localization and Hand Pose Estimation. <a href="https://ieeexplore.ieee.org/abstract/document/8682382">[PDF]</a></h5>
<p><em>Yunlong Che, Yuxiang Song, Yue Qi</em></p>
<h5 id="2019-icra-vision-based-teleoperation-of-shadow-dexterous-hand-using-end-to-end-deep-neural-network-pdf-code">• [2019 ICRA] Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network. <a href="https://arxiv.org/pdf/1809.06268.pdf">[PDF]</a> <a href="https://github.com/TAMS-Group/TeachNet_Teleoperation">[Code]</a></h5>
<p><em>Shuang Li*, Xiaojian Ma*, Hongzhuo Liang, Michael Görner, Philipp Ruppel, Bing Fang, Fuchun Sun, Jianwei Zhang</em></p>
<h5 id="2019-wacv-murauer-mapping-unlabeled-real-data-for-label-austerity-pdf-project-code">• [2019 WACV] MURAUER: Mapping Unlabeled Real Data for Label AUstERity. <a href="https://poier.github.io/murauer/documents/poier2019wacv_selfpublishing.pdf">[PDF]</a> <a href="https://poier.github.io/murauer/">[Project]</a> <a href="https://github.com/poier/murauer">[Code]</a></h5>
<p><em>Georg Poier, Michael Opitz, David Schinagl and Horst Bischof</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2018-eccv">2018 ECCV</h3>
<h5 id="handmap-robust-hand-pose-estimation-via-intermediate-dense-guidance-map-supervision-pdf-project-code">• HandMap: Robust Hand Pose Estimation via Intermediate Dense Guidance Map Supervision. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaokun_Wu_HandMap_Robust_Hand_ECCV_2018_paper.pdf">[PDF]</a>  <a href="https://xkunwu.github.io/research/18HandPose/18HandPose">[Project]</a>  <a href="https://github.com/xkunwu/depth-hand">[Code]</a></h5>
<p><em>Xiaokun Wu, Daniel Finnegan, Eamonn O'Neill, Yongliang Yang</em></p>
<h5 id="hbe-hand-branch-ensemble-network-for-real-time-3d-hand-pose-estimation-pdf">• HBE: Hand Branch Ensemble network for real time 3D hand pose estimation.  <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yidan_Zhou_HBE_Hand_Branch_ECCV_2018_paper.pdf">[PDF]</a></h5>
<p><em>Yidan Zhou, Jian Lu, Kuo Du, Xiangbo Lin, Yi Sun, Xiaohong Ma</em></p>
<h5 id="point-to-point-regression-pointnet-for-3d-hand-pose-estimation-pdf">• Point-to-Point Regression PointNet for 3D Hand Pose Estimation. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Liuhao_Ge_Point-to-Point_Regression_PointNet_ECCV_2018_paper.pdf">[PDF]</a></h5>
<p><em>Liuhao Ge, Zhou Ren, Junsong Yuan</em></p>
<h5 id="weakly-supervised-3d-hand-pose-estimation-from-monocular-rgb-images-pdf-oral">• Weakly-supervised 3D Hand Pose Estimation from Monocular RGB Images. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yujun_Cai_Weakly-supervised_3D_Hand_ECCV_2018_paper.pdf">[PDF]</a> <em>(Oral)</em></h5>
<p><em>Yujun Cai, Liuhao Ge, Jianfei Cai, Junsong Yuan</em></p>
<h5 id="joint-3d-tracking-of-a-deformable-object-in-interaction-with-a-hand-pdf-project">• Joint 3D tracking of a deformable object in interaction with a hand. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Aggeliki_Tsoli_Joint_3D_tracking_ECCV_2018_paper.pdf">[PDF]</a>  <a href="https://www.ics.forth.gr/cvrl/deformable_interaction/">[Project]</a></h5>
<p><em>Aggeliki Tsoli, Antonis A. Argyros</em></p>
<h5 id="occlusion-aware-hand-pose-estimation-using-hierarchical-mixture-density-network-pdf-oral">• Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density Network. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Qi_Ye_Occlusion-aware_Hand_Pose_ECCV_2018_paper.pdf">[PDF]</a>  <em>(Oral)</em></h5>
<p><em>Qi Ye, Tae-Kyun Kim</em></p>
<h5 id="hand-pose-estimation-via-latent-25d-heatmap-regression-pdf">• Hand Pose Estimation via Latent 2.5D Heatmap Regression. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Umar_Iqbal_Hand_Pose_Estimation_ECCV_2018_paper.pdf">[PDF]</a></h5>
<p><em>Umar Iqbal, Pavlo Molchanov, Thomas Breuel, Juergen Gall, Jan Kautz</em></p>
<h5 id="hands18-workshop-adapting-egocentric-visual-hand-pose-estimation-towards-a-robot-controlled-exoskeleton-pdf">• [Hands18 Workshop] Adapting Egocentric Visual Hand Pose Estimation Towards a Robot-Controlled Exoskeleton. <a href="https://link.springer.com/chapter/10.1007/978-3-030-11024-6_16">[PDF]</a></h5>
<p><em>Gerald Baulig, Thomas Gulde, Cristobal Curio</em></p>
<h5 id="hands18-workshop-estimating-2d-multi-hand-poses-from-single-depth-images-pdf">• [Hands18 Workshop] Estimating 2D Multi-Hand Poses From Single Depth Images. <a href="https://link.springer.com/chapter/10.1007/978-3-030-11024-6_17">[PDF]</a></h5>
<p><em>Le Duan, Minmin Shen, Song Cui, Zhexiao Guo, Oliver Deussen</em></p>
<h5 id="hands18-workshop-task-oriented-hand-motion-retargeting-for-dexterous-manipulation-imitation-pdf-project">• [Hands18 Workshop] Task-Oriented Hand Motion Retargeting for Dexterous Manipulation Imitation. <a href="https://arxiv.org/pdf/1810.01845.pdf">[PDF]</a> <a href="https://daphneantotsiou.github.io/task-oriented-retargeting.html">[Project]</a></h5>
<p><em>Dafni Antotsiou, Guillermo Garcia-Hernando, Tae-Kyun Kim</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2018-cvpr">2018 CVPR</h3>
<h5 id="first-person-hand-action-benchmark-with-rgb-d-videos-and-3d-hand-pose-annotations-pdf-project-code">• First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations. <a href="https://arxiv.org/pdf/1704.02463.pdf">[PDF]</a> <a href="https://guiggh.github.io/publications/first-person-hands/">[Project]</a>  <a href="https://github.com/guiggh/hand_pose_action">[Code]</a></h5>
<p><em>Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, Tae-Kyun Kim</em></p>
<h5 id="learning-pose-specific-representations-by-predicting-different-views-pdf-project-code">• Learning Pose Specific Representations by Predicting Different Views. <a href="https://arxiv.org/pdf/1804.03390.pdf">[PDF]</a>  <a href="https://poier.github.io/PreView/">[Project]</a>  <a href="https://github.com/poier/PreView">[Code]</a></h5>
<p><em>Georg Poier, David Schinagl, Horst Bischof</em></p>
<h5 id="hand-pointnet-3d-hand-pose-estimation-using-point-sets-pdf-project-code-spotlight">• Hand PointNet: 3D Hand Pose Estimation using Point Sets. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Hand_PointNet_3D_CVPR_2018_paper.pdf">[PDF]</a>  <a href="https://sites.google.com/site/geliuhaontu/home/cvpr2018">[Project]</a> <a href="https://sites.google.com/site/geliuhaontu/HandPointNet.zip?attredirects=0&amp;d=1">[Code]</a> <em>(Spotlight)</em></h5>
<p><em>Liuhao Ge, Yujun Cai, Junwu Weng, Junsong Yuan</em></p>
<h5 id="dense-3d-regression-for-hand-pose-estimation-pdf-code">• Dense 3D Regression for Hand Pose Estimation. <a href="https://arxiv.org/pdf/1711.08996.pdf">[PDF]</a>  <a href="https://github.com/melonwan/denseReg">[Code]</a></h5>
<p><em>Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao</em></p>
<h5 id="cross-modal-deep-variational-hand-pose-estimation-pdf-project-code-spotlight">• Cross-modal Deep Variational Hand Pose Estimation. <a href="https://arxiv.org/pdf/1803.11404">[PDF]</a> <a href="https://ait.ethz.ch/projects/2018/vae_hands/">[Project]</a> <a href="https://github.com/spurra/vae-hands-3d">[Code]</a> <em>(Spotlight)</em></h5>
<p><em>Adrian Spurr, Jie Song, Seonwook Park, Otmar Hilliges</em></p>
<h5 id="feature-mapping-for-learning-fast-and-accurate-3d-pose-inference-from-synthetic-images-pdf-project">• Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images. <a href="https://arxiv.org/pdf/1712.03904.pdf">[PDF]</a>  <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/research-projects/feature-mapping/">[Project]</a></h5>
<p><em>Mahdi Rad, Markus Oberweger, Vincent Lepetit</em></p>
<h5 id="ganerated-hands-for-real-time-3d-hand-tracking-from-monocular-rgb-pdf-supp-project-spotlight">• GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB. <a href="http://handtracker.mpi-inf.mpg.de/projects/GANeratedHands/content/GANeratedHands_CVPR2018.pdf">[PDF]</a> <a href="http://handtracker.mpi-inf.mpg.de/projects/GANeratedHands/content/GANeratedHands_CVPR2018_Supp.pdf">[Supp]</a> <a href="http://handtracker.mpi-inf.mpg.de/projects/GANeratedHands/">[Project]</a> <em>(Spotlight)</em></h5>
<p><em>Franziska Mueller, Florian Bernard, Oleksandr Sotnychenko, Dushyant Mehta, Srinath Sridhar, Dan Casas, Christian Theobalt</em></p>
<h5 id="v2v-posenet-voxel-to-voxel-prediction-network-for-accurate-3d-hand-and-human-pose-estimation-from-a-single-depth-map-pdf-code">• V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map. <a href="https://arxiv.org/pdf/1711.07399.pdf">[PDF]</a> <a href="https://github.com/mks0601/V2V-PoseNet_RELEASE">[Code]</a></h5>
<p><em>Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee</em></p>
<h5 id="depth-based-3d-hand-pose-estimation-from-current-achievements-to-future-goals-pdf-spotlight">• Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future Goals. <a href="https://arxiv.org/pdf/1712.03917.pdf">[PDF]</a> <em>(Spotlight)</em></h5>
<p><em>Shanxin Yuan, Guillermo Garcia-Hernando, Bjorn Stenger, Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee, Pavlo Molchanov, Jan Kautz, Sina Honari, Liuhao Ge, Junsong Yuan, Xinghao Chen, Guijin Wang, Fan Yang, Kai Akiyama, Yang Wu, Qingfu Wan, Meysam Madadi, Sergio Escalera, Shile Li, Dongheui Lee, Iason Oikonomidis, Antonis Argyros, Tae-Kyun Kim</em></p>
<h5 id="augmented-skeleton-space-transfer-for-depth-based-hand-pose-estimation-pdf-oral">• Augmented skeleton space transfer for depth-based hand pose estimation. <a href="https://arxiv.org/pdf/1805.04497.pdf">[PDF]</a> <em>(Oral)</em></h5>
<p><em>Seungryul Baek, Kwang In Kim, Tae-Kyun Kim</em></p>
<h5 id="3d-humans-workshop-monocular-rgb-hand-pose-inference-from-unsupervised-refinable-nets-pdf">• <a href="https://project.inria.fr/humans2018/">[3D HUMANS Workshop]</a> Monocular RGB Hand Pose Inference From Unsupervised Refinable Nets. <a href="http://openaccess.thecvf.com/CVPR2018_workshops/content_CVPR_2018/papers/w17/Dibra_Monocular_RGB_Hand_CVPR_2018_paper.pdf">[PDF]</a></h5>
<p><em>Endri Dibra, Silvan Melchior, Ali Balkis, Thomas Wolf, Cengiz Oztireli, Markus Gross</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2018-others">2018 Others</h3>
<h5 id="2018-ismar-hybrid-3d-hand-articulations-tracking-guided-by-classification-and-search-space-adaptation-pdf">• [2018 ISMAR] Hybrid 3D Hand Articulations Tracking Guided by Classification and Search Space Adaptation. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8613751">[PDF]</a></h5>
<p><em>Gabyong Park, Woontack Woo</em></p>
<h5 id="2018-accv-hand-pose-estimation-based-on-3d-residual-network-with-data-padding-and-skeleton-steadying-pdf">• [2018 ACCV] Hand Pose Estimation based on 3D Residual Network with Data Padding and Skeleton Steadying. [PDF]</h5>
<p><em>Pai-Wen Ting, En-Te Chou, Ya-Hui Tang, Li-Chen Fu</em></p>
<h5 id="2018-accv-partially-occluded-hands-a-challenging-new-dataset-for-single-image-hand-pose-estimation-pdf-project-oral">• [2018 ACCV] Partially Occluded Hands: A challenging new dataset for single-image hand pose estimation. <a href="https://cbmm.mit.edu/sites/default/files/publications/partially-occluded-hands-6.pdf">[PDF]</a> <a href="http://occludedhands.com/">[Project]</a>  <em>(Oral)</em></h5>
<p><em>Battushig Myanganbayar, Cristina Mata, Gil Dekel, Boris Katz, Guy Ben-Yosef, Andrei Barbu</em></p>
<h5 id="2018-accv-domain-transfer-for-3d-pose-estimation-from-color-images-without-manual-annotations-pdf-oral">• [2018 ACCV] Domain Transfer for 3D Pose Estimation from Color Images without Manual Annotations. <a href="https://arxiv.org/pdf/1810.03707.pdf">[PDF]</a>  <em>(Oral)</em></h5>
<p><em>Mahdi Rad, Markus Oberweger, Vincent Lepetit</em></p>
<h5 id="2018-pcm-hand-pose-estimation-with-attention-and-sequence-network-pdf">• [2018 PCM] Hand Pose Estimation with Attention-and-Sequence Network. <a href="https://link.springer.com/chapter/10.1007/978-3-030-00776-8_51">[PDF]</a></h5>
<p><em>Tianping Hu*, Wenhai Wang*, Tong Lu</em></p>
<h5 id="2018-pcm-mutiple-transfer-net-with-region-ensemble-for-deep-hand-pose-estimation-pdf">• [2018 PCM] Mutiple Transfer Net with Region Ensemble for Deep Hand Pose Estimation. <a href="https://link.springer.com/chapter/10.1007/978-3-030-00776-8_58">[PDF]</a></h5>
<p><em>Haoqian Wang, Da Li, Xingzheng Wang</em></p>
<h5 id="2018-icpr-local-regression-based-hourglass-network-for-hand-pose-estimation-from-a-single-depth-image-pdf">• [2018 ICPR] Local Regression Based Hourglass Network for Hand Pose Estimation from a Single Depth Image. <a href="https://ieeexplore.ieee.org/abstract/document/8545460">[PDF]</a></h5>
<p><em>Jia Li, Zengfu Wang</em></p>
<h5 id="2018-icpr-dynamic-projected-segmentation-networks-for-hand-pose-estimation-pdf">• [2018 ICPR] Dynamic Projected Segmentation Networks for Hand Pose Estimation. <a href="https://ieeexplore.ieee.org/abstract/document/8546330">[PDF]</a></h5>
<p><em>Yunlong Che, Yue Qi</em></p>
<h5 id="2018-3dv-deephps-end-to-end-estimation-of-3d-hand-pose-and-shape-by-learning-from-synthetic-depth-pdf">• [2018 3DV] DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning from Synthetic Depth. <a href="https://arxiv.org/pdf/1808.09208.pdf">[PDF]</a></h5>
<p><em>Jameel Malik, Ahmed Elhayek, Fabrizio Nunnari, Kiran Varanasi, Kiarash Tamaddon, Alexis Heloir, Didier Stricker</em></p>
<h5 id="2018-icip-networks-effectively-utilizing-2d-spatial-information-for-accurate-3d-hand-pose-estimation-pdf">• [2018 ICIP] Networks Effectively Utilizing 2D Spatial Information for Accurate 3D Hand Pose Estimation. <a href="https://ieeexplore.ieee.org/abstract/document/8451509/">[PDF]</a></h5>
<p><em>Baoen Liu, Shiliang Huang, Zhongfu Ye</em></p>
<h5 id="2018-icip-on-the-fusion-of-rgb-and-depth-information-for-hand-pose-estimation-pdf-code">• [2018 ICIP] On the Fusion of RGB and Depth Information For Hand Pose Estimation. <a href="https://ieeexplore.ieee.org/document/8451022/">[PDF]</a> <a href="https://github.com/ekazakos/fusenet-hand-pose">[Code]</a></h5>
<p><em>Evangelos Kazakos, Christophoros Nikou, Ioannis Kakadiaris</em></p>
<h5 id="2018-icip-fast-lifting-for-3d-hand-pose-estimation-in-arvr-applications-pdf">• [2018 ICIP] Fast Lifting for 3D Hand Pose Estimation in AR/VR Applications. <a href="https://drive.google.com/file/d/1kbNSb0ySAkhpQ6ntxPhs0wPvFDbsGu8v/view">[PDF]</a></h5>
<p><em>Onur Guleryuz, Christine Kaeser-Chen</em></p>
<h5 id="2018-bmvc-structure-aware-3d-hourglass-network-for-hand-pose-estimation-from-single-depth-image-pdf">• [2018 BMVC] Structure-Aware 3D Hourglass Network for Hand Pose Estimation from Single Depth Image. <a href="http://bmvc2018.org/papers/1133.pdf">[PDF]</a></h5>
<p><em>Fuyang Huang, Ailing Zeng, Minhao Liu, Jing Qin, Qiang Xu</em></p>
<h5 id="2018-bmvc-3d-hand-pose-estimation-using-simulation-and-partial-supervision-with-a-shared-latent-space-pdf-code-oral">• [2018 BMVC] 3D Hand Pose Estimation using Simulation and Partial-Supervision with a Shared Latent Space. <a href="https://arxiv.org/pdf/1807.05380.pdf">[PDF]</a> <a href="https://github.com/masabdi/LSPS">[Code]</a> <em>(Oral)</em></h5>
<p><em>Masoud Abdi, Ehsan Abbasnejad, Chee Peng Lim, Saeid Nahavandi</em></p>
<h5 id="2018-fg-kinematic-constrained-cascaded-autoencoder-for-real-time-hand-pose-estimation-pdf">• [2018 FG] Kinematic Constrained Cascaded Autoencoder for Real-time Hand Pose Estimation. <a href="https://ieeexplore.ieee.org/abstract/document/8373810/">[PDF]</a></h5>
<p><em>Yushun Lin, Xiujuan Chai, Xilin Chen</em></p>
<h5 id="2018-wacv-using-a-single-rgb-frame-for-real-time-3d-hand-pose-estimation-in-the-wild-pdf-project-code">• [2018 WACV] Using a single RGB frame for real time 3D hand pose estimation in the wild. <a href="https://arxiv.org/pdf/1712.03866.pdf">[PDF]</a>  <a href="http://users.ics.forth.gr/~argyros/res_rgbmonohand.html">[Project]</a> <a href="https://github.com/FORTH-ModelBasedTracker/MonocularRGB_3D_Handpose_WACV18">[Code]</a></h5>
<p><em>Paschalis Panteleris, Iason Oikonomidis, Antonis Argyros</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2017-iccv">2017 ICCV</h3>
<h5 id="learning-to-estimate-3d-hand-pose-from-single-rgb-images-pdf-project-code">• Learning to Estimate 3D Hand Pose from Single RGB Images. <a href="https://arxiv.org/pdf/1705.01389.pdf">[PDF]</a>  <a href="https://lmb.informatik.uni-freiburg.de/projects/hand3d/">[Project]</a>   <a href="https://github.com/lmb-freiburg/hand3d">[Code]</a></h5>
<p><em>Christian Zimmermann, Thomas Brox</em></p>
<h5 id="real-time-hand-tracking-under-occlusion-from-an-egocentric-rgb-d-sensor-pdf-project">• Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor. <a href="http://handtracker.mpi-inf.mpg.de/projects/OccludedHands/content/OccludedHands_ICCV2017.pdf">[PDF]</a> <a href="http://handtracker.mpi-inf.mpg.de/projects/OccludedHands/">[Project]</a></h5>
<p><em>Franziska Mueller, Dushyant Mehta, Oleksandr Sotnychenko, Srinath Sridhar, Dan Casas, Christian Theobalt</em></p>
<h5 id="robust-hand-pose-estimation-during-the-interaction-with-an-unknown-object-pdf-supp-project">• Robust Hand Pose Estimation during the Interaction with an Unknown Object. <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Choi_Robust_Hand_Pose_ICCV_2017_paper.pdf">[PDF]</a> <a href="http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Choi_Robust_Hand_Pose_ICCV_2017_supplemental.pdf">[Supp]</a> <a href="https://engineering.purdue.edu/cdesign/wp/robust-hand-pose-estimation-during-the-interaction-with-an-unknown-object/">[Project]</a></h5>
<p><em>Chiho Choi, Sang Ho Yoon, Chin-Ning Chen, Karthik Ramani</em></p>
<h5 id="learning-hand-articulations-by-hallucinating-heat-distribution-pdf-supp-project">• Learning Hand Articulations by Hallucinating Heat Distribution. <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Choi_Learning_Hand_Articulations_ICCV_2017_paper.pdf">[PDF]</a> <a href="http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Choi_Learning_Hand_Articulations_ICCV_2017_supplemental.pdf">[Supp]</a>  <a href="https://engineering.purdue.edu/cdesign/wp/learning-hand-articulations-by-hallucinating-heat-distribution/">[Project]</a></h5>
<p><em>Chiho Choi, Sangpil Kim, Karthik Ramani</em></p>
<h5 id="low-dimensionality-calibration-through-local-anisotropic-scaling-for-robust-hand-model-personalization-pdf-project-code">• Low-Dimensionality Calibration through Local Anisotropic Scaling for Robust Hand Model Personalization. <a href="http://lgg.epfl.ch/publications/2017/LocalAnisotropicScaling/paper.pdf">[PDF]</a>  <a href="http://lgg.epfl.ch/publications/2017/LocalAnisotropicScaling/index.php">[Project]</a> <a href="https://github.com/edoRemelli/hadjust">[Code]</a></h5>
<p><em>Edoardo Remelli<em>, Anastasia Tkach</em>, Andrea Tagliasacchi, Mark Pauly</em></p>
<h5 id="hands17-workshop-back-to-rgb-3d-tracking-of-hands-and-hand-object-interactions-based-on-short-baseline-stereo-pdf">• [Hands17 Workshop] Back to RGB: 3D tracking of hands and hand-object interactions based on short-baseline stereo. <a href="https://arxiv.org/pdf/1705.05301.pdf">[PDF]</a></h5>
<p><em>Paschalis Panteleris, Antonis Argyros</em></p>
<h5 id="hands17-workshop-deepprior-improving-fast-and-accurate-3d-hand-pose-estimation-pdf-project-code">• [Hands17 Workshop] DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation. <a href="https://www.tugraz.at/fileadmin/user_upload/Institute/ICG/Images/team_lepetit/publications/oberweger_iccvw17.pdf">[PDF]</a> <a href="https://www.tugraz.at/institute/icg/teams/teamlepetit/research/hand-detection-and-3d-pose-estimation/">[Project]</a> <a href="https://github.com/moberweger/deep-prior-pp">[Code]</a></h5>
<p><em>Markus Oberweger and Vincent Lepetit</em></p>
<h5 id="hands17-workshop-hand-pose-estimation-using-deep-stereovision-and-markov-chain-monte-carlo-pdf">• [Hands17 Workshop] Hand Pose Estimation Using Deep Stereovision and Markov-chain Monte Carlo. <a href="http://openaccess.city.ac.uk/18087/1/BasaruICCVW2017_MCMC.pdf">[PDF]</a></h5>
<p><em>Rilwan Remilekun Basaru, Chris Child, Eduardo Alonso, Greg Slabaugh</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2017-cvpr">2017 CVPR</h3>
<h5 id="hand-keypoint-detection-in-single-images-using-multiview-bootstrapping-pdf-project-code">• Hand Keypoint Detection in Single Images using Multiview Bootstrapping. <a href="https://arxiv.org/pdf/1704.07809">[PDF]</a> <a href="http://www.cs.cmu.edu/~tsimon/projects/mvbs.html">[Project]</a> <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">[Code]</a></h5>
<p><em>Tomas Simon, Hanbyul Joo, Iain Matthews, Yaser Sheikh</em></p>
<h5 id="crossing-nets-combining-gans-and-vaes-with-a-shared-latent-space-for-hand-pose-estimation-pdf-code">• Crossing Nets: Combining GANs and VAEs with a Shared Latent Space for Hand Pose Estimation. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Wan_Crossing_Nets_Combining_CVPR_2017_paper.pdf">[PDF]</a> <a href="https://github.com/melonwan/crossingNet">[Code]</a></h5>
<p><em>Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao</em></p>
<h5 id="big-hand-22m-benchmark-hand-pose-data-set-and-state-of-the-art-analysis-pdf">• Big Hand 2.2M Benchmark: Hand Pose Data Set and State of the Art Analysis. <a href="https://labicvl.github.io/docs/pubs/Shanxin_CVPR_2017.pdf">[PDF]</a></h5>
<p><em>Shanxin Yuan*, Qi Ye*, Bjorn Stenger, Siddhand Jain, Tae-Kyun Kim</em></p>
<h5 id="3d-convolutional-neural-networks-for-efficient-and-robust-hand-pose-estimation-from-single-depth-images-pdf-project">• 3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation from Single Depth Images. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Ge_3D_Convolutional_Neural_CVPR_2017_paper.pdf">[PDF]</a> <a href="https://sites.google.com/site/geliuhaontu/home/cvpr2017">[Project]</a></h5>
<p><em>Liuhao Ge, Hui Liang, Junsong Yuan and Daniel Thalmann</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2017-others">2017 Others</h3>
<h5 id="2017-3dv-simultaneous-hand-pose-and-skeleton-bone-lengths-estimation-from-a-single-depth-image-pdf">• [2017 3DV] Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a Single Depth Image. <a href="https://arxiv.org/pdf/1712.03121.pdf">[PDF]</a></h5>
<p><em>Jameel Malik, Ahmed Elhayek, Didier Stricker</em></p>
<h5 id="2017-3dv-how-to-refine-3d-hand-pose-estimation-from-unlabelled-depth-data-pdf">• [2017 3DV] How to Refine 3D Hand Pose Estimation from Unlabelled Depth Data? <a href="https://graphics.ethz.ch/~edibra/Publications/How%20to%20Refine%203D%20Hand%20Pose%20Estimation%20from%20Unlabelled%20Depth%20Data.pdf">[PDF]</a></h5>
<p><em>Endri Dibra*, Thomas Wolf*, Cengiz Öztireli, Markus Gross</em></p>
<h5 id="2017-icip-region-ensemble-network-improving-convolutional-network-for-hand-pose-estimation-pdf-code">• [2017 ICIP] Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation. <a href="https://arxiv.org/pdf/1702.02447.pdf">[PDF]</a>  <a href="https://github.com/guohengkai/region-ensemble-network">[Code]</a></h5>
<p><em>Hengkai Guo, Guijin Wang, Xinghao Chen, Cairong Zhang, Fei Qiao, Huazhong Yang</em></p>
<h5 id="2017-icip-a-hand-pose-tracking-benchmark-from-stereo-matching-pdf-project">• [2017 ICIP] A Hand Pose Tracking Benchmark from Stereo Matching. <a href="http://www.cs.cityu.edu.hk/~jianbjiao2/pdfs/icip.pdf">[PDF]</a>  <a href="https://sites.google.com/site/zhjw1988/">[Project]</a></h5>
<p><em>Jiawei Zhang, Jianbo Jiao, Mingliang Chen, Liangqiong Qu, Xiaobin Xu, and Qingxiong Yang</em></p>
<h5 id="2017-siggraph-asia-articulated-distance-fields-for-ultra-fast-tracking-of-hands-interacting-pdf">• [2017 SIGGRAPH Asia] Articulated distance fields for ultra-fast tracking of hands interacting. <a href="https://dl.acm.org/citation.cfm?id=3130853">[PDF]</a></h5>
<p><em>Jonathan Taylor*, Vladimir Tankovich*, Danhang Tang*, Cem Keskin*, David Kim, Philip Davidson, Adarsh Kowdle, Shahram Izadi</em></p>
<h5 id="2017-siggraph-asia-online-generative-model-personalization-for-hand-tracking-pdf-project">• [2017 SIGGRAPH Asia] Online Generative Model Personalization for Hand Tracking. <a href="http://lgg.epfl.ch/publications/2017/HOnline/paper.pdf">[PDF]</a>  <a href="http://lgg.epfl.ch/publications/2017/HOnline/index.php">[Project]</a></h5>
<p><em>Anastasia Tkach*, Andrea Tagliasacchi*, Edoardo Remelli, Mark Pauly, Andrew Fitzgibbon</em></p>
<h5 id="2017-siggraph-asia-embodied-hands-modeling-and-capturing-hands-and-bodies-together-pdf-project">• [2017 SIGGRAPH Asia] Embodied Hands: Modeling and Capturing Hands and Bodies Together. <a href="http://ps.is.tue.mpg.de/uploads_file/attachment/attachment/392/Embodied_Hands_SiggraphAsia2017.pdf">[PDF]</a>  <a href="http://ps.is.tue.mpg.de/publications/embodiedhands">[Project]</a></h5>
<p><em>Javier Romero*, Dimitrios Tzionas* and Michael J. Black</em></p>
<h5 id="2017-bmvc-hand-pose-learning-combining-deep-learning-and-hierarchical-refinement-for-3d-hand-pose-estimation-pdf">• [2017 BMVC] Hand Pose Learning: Combining Deep Learning and Hierarchical Refinement for 3D Hand Pose Estimation. <a href="https://www.dropbox.com/s/3y96pnutxum3p4v/0569.pdf?dl=1">[PDF]</a></h5>
<p><em>Min-Yu Wu, Ya Hui Tang, Pai-Wei Ting and Li-Chen Fu</em></p>
<h5 id="2017-bmvc-generative-3d-hand-tracking-with-spatially-constrained-pose-sampling-pdf-project">• [2017 BMVC] Generative 3D Hand Tracking with Spatially Constrained Pose Sampling. <a href="http://users.ics.forth.gr/~argyros/mypapers/2017_09_BMVC_RDSRoditak.pdf">[PDF]</a> <a href="http://users.ics.forth.gr/~argyros/res_handRDS.html">[Project]</a></h5>
<p><em>Konstantinos Roditakis, Alexandros Makris and Antonis Argyros</em></p>
<h5 id="2017-icra-learning-a-deep-network-with-spherical-part-model-for-3d-hand-pose-estimation-pdf">• [2017 ICRA] Learning a deep network with spherical part model for 3D hand pose estimation. <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989303">[PDF]</a></h5>
<p><em>Tzu-Yang Chen, Pai-Wen Ting, Min-Yu Wu, Li-Chen Fu</em></p>
<h5 id="2017-fg-occlusion-aware-hand-pose-recovery-from-sequences-of-depth-images-pdf-slide">• [2017 FG] Occlusion aware hand pose recovery from sequences of depth images. <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961746">[PDF]</a> <a href="http://sergioescalera.com/wp-content/uploads/2017/06/FG2017Hand.pdf">[Slide]</a></h5>
<p><em>Meysam Madadi, Sergio Escalera, Alex Carruesco Llorens, Carlos Andujar, Xavier Baro, Jordi Gonzalez</em></p>
<h5 id="2017-fg-3d-hand-object-pose-estimation-from-depth-with-convolutional-neural-networks-pdf-project">• [2017 FG] 3D Hand-Object Pose Estimation from Depth with Convolutional Neural Networks. <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961770">[PDF]</a> <a href="http://www.cs.man.ac.uk/~goudied/research.html">[Project]</a></h5>
<p><em>Duncan Goudie, Aphrodite Galata</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2016-eccv">2016 ECCV</h3>
<h5 id="spatial-attention-deep-net-with-partial-pso-for-hierarchical-hybrid-hand-pose-estimation-pdf-project">• Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand Pose Estimation. <a href="https://labicvl.github.io/docs/pubs/Qi_Shanxin_ECCV_2016.pdf">[PDF]</a> <a href="https://sites.google.com/site/qiyeincv/home/eccv2016">[Project]</a></h5>
<p><em>Qi Ye*, Shanxin Yuan*, Tae-Kyun Kim</em></p>
<h5 id="hand-pose-estimation-from-local-surface-normals-pdf">• Hand Pose Estimation from Local Surface Normals. <a href="http://www.vision.ee.ethz.ch/~yaoa/pdfs/wan_eccv16.pdf">[PDF]</a></h5>
<p><em>Chengde Wan, Angela Yao, and Luc Van Gool</em></p>
<h5 id="real-time-joint-tracking-of-a-hand-manipulating-an-object-from-rgb-d-input-pdf-project">• Real-time Joint Tracking of a Hand Manipulating an Object from RGB-D Input. <a href="http://handtracker.mpi-inf.mpg.de/projects/RealtimeHO/content/RealtimeHO_ECCV2016.pdf">[PDF]</a> <a href="http://handtracker.mpi-inf.mpg.de/projects/RealtimeHO/">[Project]</a></h5>
<p><em>Srinath Sridhar, Franziska Mueller, Michael Zollhöfer, Dan Casas, Antti Oulasvirta, Christian Theobalt</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2016-cvpr">2016 CVPR</h3>
<h5 id="robust-3d-hand-pose-estimation-in-single-depth-images-from-single-view-cnn-to-multi-view-cnns-pdf-project-code">• Robust 3D Hand Pose Estimation in Single Depth Images: From Single-View CNN to Multi-View CNNs. <a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Ge_Robust_3D_Hand_CVPR_2016_paper.pdf">[PDF]</a> <a href="https://sites.google.com/site/geliuhaontu/home/cvpr2016">[Project]</a> <a href="https://github.com/geliuhao/CVPR2016_HandPoseEstimation">[Code]</a></h5>
<p><em>Liuhao Ge, Hui Liang, Junsong Yuan, Daniel Thalmann</em></p>
<h5 id="deephand-robust-hand-pose-estimation-by-completing-a-matrix-imputed-with-deep-features-pdfproject">• DeepHand: Robust Hand Pose Estimation by Completing a Matrix Imputed With Deep Features.  <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Sinha_DeepHand_Robust_Hand_CVPR_2016_paper.pdf">[PDF]</a><a href="https://engineering.purdue.edu/cdesign/wp/deephand-robust-hand-pose-estimation/">[Project]</a></h5>
<p><em>Ayan Sinha*, Chiho Choi*, Karthik Ramani</em></p>
<h5 id="efficiently-creating-3d-training-data-for-fine-hand-pose-estimation-pdf-project-code">• Efficiently Creating 3D Training Data for Fine Hand Pose Estimation. <a href="https://cvarlab.icg.tugraz.at/pubs/oberweger_cvpr16.pdf">[PDF]</a> <a href="https://cvarlab.icg.tugraz.at/projects/hand_detection/">[Project]</a> <a href="https://github.com/moberweger/semi-auto-anno">[Code]</a></h5>
<p><em>Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit</em></p>
<h5 id="fits-like-a-glove-rapid-and-reliable-hand-shape-personalization-pdf-project">• Fits Like a Glove: Rapid and Reliable Hand Shape Personalization.  <a href="http://www.samehkhamis.com/tan-cvpr2016.pdf">[PDF]</a> <a href="http://campar.in.tum.de/Main/DavidTan">[Project]</a></h5>
<p><em>David Joseph Tan, Thomas Cashman, Jonathan Taylor, Andrew Fitzgibbon, Daniel Tarlow, Sameh Khamis, Shahram Izadi, Jamie Shotton</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2016-others">2016 Others</h3>
<h5 id="2016-nips-disco-nets-dissimilarity-coefficient-networks-pdf-project-code">• [2016 NIPS] DISCO Nets : Dissimilarity Coefficient Networks. <a href="http://www.robots.ox.ac.uk/~diane/DISCONET_camera_ready.pdf">[PDF]</a> <a href="http://www.robots.ox.ac.uk/~diane/DiscoNets.html">[Project]</a> <a href="https://github.com/oval-group/DISCONets">[Code]</a></h5>
<p><em>Diane Bouchacourt, M. Pawan Kumar, Sebastian Nowozin</em></p>
<h5 id="2016-accv-hand-pose-regression-via-a-classification-guided-approach-pdf">• [2016 ACCV] Hand Pose Regression via A Classification-guided Approach. <a href="http://staff.ustc.edu.cn/~juyong/Papers/HandTracking-2016.pdf">[PDF]</a></h5>
<p><em>Hongwei Yang, Juyong Zhang</em></p>
<h5 id="2016-icpr-deep-learning-for-integrated-hand-detection-and-pose-estimation-pdf">• [2016 ICPR] Deep learning for integrated hand detection and pose estimation. <a href="https://ieeexplore.ieee.org/abstract/document/7899702/">[PDF]</a></h5>
<p><em>Tzu-Yang Chen, Min-Yu Wu, Yu-Hsun Hsieh, Li-Chen Fu</em></p>
<h5 id="2016-icpr-depth-based-3d-hand-pose-tracking-pdf">• [2016 ICPR] Depth-based 3D hand pose tracking. <a href="http://ieeexplore.ieee.org/abstract/document/7900051">[PDF]</a></h5>
<p><em>Kha Gia Quach, Chi Nhan Duong, Khoa Luu, and Tien D. Bui.</em></p>
<h5 id="2016-ijcai-model-based-deep-hand-pose-estimation-pdf-project-code">• [2016 IJCAI] Model-based Deep Hand Pose Estimation. <a href="http://xingyizhou.xyz/zhou2016model.pdf">[PDF]</a> <a href="http://xingyizhou.xyz/">[Project]</a> <a href="https://github.com/tenstep/DeepModel">[Code]</a></h5>
<p><em>Xingyi Zhou, Qingfu Wan, Wei Zhang, Xiangyang Xue, Yichen Wei</em></p>
<h5 id="2016-siggraph-efficient-and-precise-interactive-hand-tracking-through-joint-continuous-optimization-of-pose-and-correspondences-pdf">• [2016 SIGGRAPH] Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences. <a href="http://www.cs.toronto.edu/~jtaylor/papers/SIGGRAPH2016-SmoothHandTracking.pdf">[PDF]</a></h5>
<p><em>Jonathan Taylor et al.</em></p>
<h5 id="2016-siggraph-asia-sphere-meshes-for-real-time-hand-modeling-and-tracking-pdf-project-code">• [2016 SIGGRAPH Asia] Sphere-Meshes for Real-Time Hand Modeling and Tracking. <a href="http://lgg.epfl.ch/publications/2016/HModel/paper.pdf">[PDF]</a>  <a href="http://lgg.epfl.ch/publications/2016/HModel/index.php">[Project]</a> <a href="https://github.com/OpenGP/hmodel">[Code]</a></h5>
<p><em>Anastasia Tkach, Mark Pauly, Andrea Tagliasacchi</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2015-iccv">2015 ICCV</h3>
<h5 id="training-a-feedback-loop-for-hand-pose-estimation-pdf-project">• Training a Feedback Loop for Hand Pose Estimation. <a href="https://cvarlab.icg.tugraz.at/pubs/oberweger_iccv15.pdf">[PDF]</a> <a href="https://cvarlab.icg.tugraz.at/projects/hand_detection/">[Project]</a></h5>
<p><em>Markus Oberweger, Paul Wohlhart, Vincent Lepetit</em></p>
<h5 id="opening-the-black-box-hierarchical-sampling-optimization-for-estimating-human-hand-pose-pdf">• Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose.  <a href="https://labicvl.github.io/docs/pubs/Danny_ICCV_2015.pdf">[PDF]</a></h5>
<p><em>Danhang Tang, Jonathan Taylor, Pushmeet Kohli, Cem Keskin, Tae-Kyun Kim, Jamie Shotton</em></p>
<h5 id="depth-based-hand-pose-estimation-data-methods-and-challenges-pdf-project-code">• Depth-based hand pose estimation: data, methods, and challenges. <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Supancic_Depth-Based_Hand_Pose_ICCV_2015_paper.pdf">[PDF]</a> <a href="http://arrummzen.net/#HandData">[Project]</a> <a href="https://github.com/jsupancic/deep_hand_pose">[Code]</a></h5>
<p><em>James Supancic III, Deva Ramanan, Gregory Rogez, Yi Yang, Jamie Shotton</em></p>
<h5 id="3d-hand-pose-estimation-using-randomized-decision-forest-with-segmentation-index-points-pdf">• 3D Hand Pose Estimation Using Randomized Decision Forest with Segmentation Index Points. <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Li_3D_Hand_Pose_ICCV_2015_paper.pdf">[PDF]</a></h5>
<p><em>Peiyi Li, Haibin Ling</em></p>
<h5 id="a-collaborative-filtering-approach-to-real-time-hand-pose-estimation-pdf-project">• A collaborative filtering approach to real-time hand pose estimation. <a href="https://engineering.purdue.edu/cdesign/wp/wp-content/uploads/2015/08/iccv_2015_hand_pose_estimation.pdf">[PDF]</a> <a href="https://engineering.purdue.edu/cdesign/wp/a-collaborative-filtering-approach-to-real-time-hand-pose-estimation/">[Project]</a></h5>
<p><em>Chiho Choi, Ayan Sinha, Joon Hee Choi, Sujin Jang, Karthik Ramani</em></p>
<h5 id="lending-a-hand-detecting-hands-and-recognizing-activities-in-complex-egocentric-interactions-pdf">• Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions. <a href="http://homes.sice.indiana.edu/sbambach/papers/iccv-egohands.pdf">[PDF]</a></h5>
<p><em>Sven Bambach, Stefan Lee, David Crandall, Chen Yu</em></p>
<h5 id="understanding-everyday-hands-in-action-from-rgb-d-images-pdf">• Understanding Everyday Hands in Action from RGB-D Images. <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Rogez_Understanding_Everyday_Hands_ICCV_2015_paper.pdf">[PDF]</a></h5>
<p><em>Gregory Rogez, James Supancic III, Deva Ramanan</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2015-cvpr">2015 CVPR</h3>
<h5 id="cascaded-hand-pose-regression-pdf">• Cascaded Hand Pose Regression.  <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Sun_Cascaded_Hand_Pose_2015_CVPR_paper.pdf">[PDF]</a></h5>
<p><em>Xiao Sun, Yichen Wei, Shuang Liang, Xiaoou Tang, and Jian Sun</em></p>
<h5 id="fast-and-robust-hand-tracking-using-detection-guided-optimization-pdf-project">• Fast and Robust Hand Tracking Using Detection-Guided Optimization. <a href="http://handtracker.mpi-inf.mpg.de/projects/FastHandTracker/content/FastHandTracker_CVPR2015.pdf">[PDF]</a> <a href="http://handtracker.mpi-inf.mpg.de/projects/FastHandTracker/">[Project]</a></h5>
<p><em>Srinath Sridhar, Franziska Mueller, Antti Oulasvirta, Christian Theobalt</em></p>
<h5 id="learning-an-efficient-model-of-hand-shape-variation-from-depth-images-pdf">• Learning an Efficient Model of Hand Shape Variation from Depth Images. <a href="http://www.samehkhamis.com/khamis-cvpr2015.pdf">[PDF]</a></h5>
<p><em>Sameh Khamis, Jonathan Taylor, Jamie Shotton, Cem Keskin, Shahram Izadi, Andrew Fitzgibbon</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2015-others">2015 Others</h3>
<h5 id="2015-bmvc-hybrid-one-shot-3d-hand-pose-estimation-by-exploiting-uncertainties-pdf-project">• [2015 BMVC] Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties. <a href="https://arxiv.org/pdf/1510.08039.pdf">[PDF]</a> <a href="https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/hybridhpe/">[Project]</a></h5>
<p><em>Georg Poier, Konstantinos Roditakis, Samuel Schulter, Damien Michel, Horst Bischof and Antonis A. Argyros</em></p>
<h5 id="2015-bmvc-rule-of-thumb-deep-derotation-for-improved-fingertip-detection-pdf-project">• [2015 BMVC] Rule of Thumb: Deep Derotation for Improved Fingertip Detection. <a href="http://www.cs.technion.ac.il/~twerd/WetzlerSlossbergKimmel-BMVC15.pdf">[PDF]</a> <a href="http://www.cs.technion.ac.il/~twerd/HandNet/">[Project]</a></h5>
<p><em>Aaron Wetzler, Ron Slossberg and Ron Kimmel</em></p>
<h5 id="2015-chi-accurate-robust-and-flexible-real-time-hand-tracking-pdf-project">• [2015 CHI] Accurate, Robust, and Flexible Real-time Hand Tracking. <a href="http://www.cs.toronto.edu/~jtaylor/papers/CHI2015-HandTracking.pdf">[PDF]</a> <a href="https://www.microsoft.com/en-us/research/publication/accurate-robust-and-flexible-real-time-hand-tracking/">[Project]</a></h5>
<p><em>Toby Sharp, Cem Keskin, Duncan Robertson, Jonathan Taylor, Jamie Shotton, David Kim, Christoph Rhemann, Ido Leichter, Alon Vinnikov, Yichen Wei, Daniel Freedman, Pushmeet Kohli, Eyal Krupka, Andrew Fitzgibbon, Shahram Izadi</em></p>
<h5 id="2015-cvww-hands-deep-in-deep-learning-for-hand-pose-estimation-pdf-project-code">• [2015 CVWW] Hands Deep in Deep Learning for Hand Pose Estimation. <a href="https://cvarlab.icg.tugraz.at/pubs/oberweger_cvww15.pdf">[PDF]</a> <a href="https://cvarlab.icg.tugraz.at/projects/hand_detection/">[Project]</a> <a href="https://github.com/moberweger/deep-prior">[Code]</a></h5>
<p><em>Markus Oberweger, Paul Wohlhart, Vincent Lepetit</em></p>
<h5 id="2015-fg-combining-discriminative-and-model-based-approaches-for-hand-pose-estimation-pdf-project">• [2015 FG] Combining Discriminative and Model Based Approaches for Hand Pose Estimation. <a href="http://www.krejov.com/uploads/2/4/0/5/24053627/final_fg2015.pdf">[PDF]</a> <a href="http://www.krejov.com/hand-pose-estimation.html">[Project]</a></h5>
<p><em>Philip Krejov, Andrew Gilbert, Richard Bowden</em></p>
<h5 id="2015-sgp-robust-articulated-icp-for-real-time-hand-tracking-pdf-project-code">• [2015 SGP] Robust Articulated-ICP for Real-Time Hand Tracking. <a href="http://gfx.uvic.ca/pubs/2015/htrack//paper.pdf">[PDF]</a>  <a href="http://lgg.epfl.ch/publications/2015/Htrack_ICP/index.php">[Project]</a> <a href="https://github.com/OpenGP/htrack">[Code]</a></h5>
<p><em>Anastasia Tkach, Mark Pauly, Andrea Tagliasacchi</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2014-cvpr">2014 CVPR</h3>
<h5 id="realtime-and-robust-hand-tracking-from-depth-pdf-project">• Realtime and robust hand tracking from depth. <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/yichenw-cvpr14_handtracking.pdf">[PDF]</a> <a href="https://www.microsoft.com/en-us/research/people/yichenw/">[Project]</a></h5>
<p><em>Chen Qian, Xiao Sun, Yichen Wei, Xiaoou Tang and Jian Sun</em></p>
<h5 id="latent-regression-forest-structured-estimation-of-3d-articulated-hand-posture-pdf-project">• Latent regression forest: Structured estimation of 3d articulated hand posture. <a href="https://labicvl.github.io/docs/pubs/Danny_CVPR_2014.pdf">[PDF]</a> <a href="https://labicvl.github.io/hand.html">[Project]</a></h5>
<p><em>Danhang Tang, Hyung Jin Chang, Alykhan Tejani, T-K. Kim</em></p>
<h5 id="user-specific-hand-modeling-from-monocular-depth-sequences-pdf-project">• User-specific hand modeling from monocular depth sequences. <a href="http://www.cs.toronto.edu/~jtaylor/papers/CVPR2014-UserSpecificHandModeling.pdf">[PDF]</a> <a href="https://www.microsoft.com/en-us/research/publication/user-specific-hand-modeling-from-monocular-depth-sequences/">[Project]</a></h5>
<p><em>Jonathan Taylor, Richard Stebbing, Varun Ramakrishna, Cem Keskin, Jamie Shotton, Shahram Izadi, Aaron Hertzmann, Andrew Fitzgibbon</em></p>
<h5 id="evolutionary-quasi-random-search-for-hand-articulations-tracking-pdf-project">• Evolutionary Quasi-random Search for Hand Articulations Tracking. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oikonomidis_Evolutionary_Quasi-random_Search_2014_CVPR_paper.pdf">[PDF]</a> <a href="http://users.ics.forth.gr/~oikonom/pb/publications">[Project]</a></h5>
<p><em>Iason Oikonomidis, Manolis IA Lourakis, Antonis A Argyros</em></p>
<p><a href="#contents">[back to top]</a></p>
<h3 id="2014-others-before">2014 Others &amp; Before</h3>
<h5 id="2014-siggraph-real-time-continuous-pose-recovery-of-human-hands-using-convolutional-networks-pdf-project">• [2014 SIGGRAPH] Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks. <a href="http://cims.nyu.edu/~tompson/others/TOG_2014_paper_PREPRINT.pdf">[PDF]</a> <a href="http://cims.nyu.edu/~tompson/NYU_Hand_Pose_Dataset.htm">[Project]</a></h5>
<p><em>Jonathan Tompson, Murphy Stein, Yann Lecun and Ken Perlin</em></p>
<h5 id="2013-iccv-real-time-articulated-hand-pose-estimation-using-semi-supervised-transductive-regression-forests-pdf-project">• [2013 ICCV] Real-time Articulated Hand Pose Estimation using Semi-supervised Transductive Regression Forests. <a href="https://labicvl.github.io/docs/pubs/Danny_ICCV_2013.pdf">[PDF]</a> <a href="https://labicvl.github.io/hand.html">[Project]</a></h5>
<p><em>Danhang Tang, Tsz Ho Yu and T-K. Kim</em></p>
<h5 id="2013-iccv-interactive-markerless-articulated-hand-motion-tracking-using-rgb-and-depth-data-pdf-project">• [2013 ICCV] Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data. <a href="http://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/content/handtracker_iccv2013.pdf">[PDF]</a> <a href="http://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/">[Project]</a></h5>
<p><em>Srinath Sridhar, Antti Oulasvirta, Christian Theobalt</em></p>
<h5 id="2013-iccv-efficient-hand-pose-estimation-from-a-single-depth-image-pdf-project">• [2013 ICCV] Efficient Hand Pose Estimation from a Single Depth Image. <a href="http://web.bii.a-star.edu.sg/~xuchi/pdf/iccv2013.pdf">[PDF]</a> <a href="http://web.bii.a-star.edu.sg/~xuchi/dhand.htm">[Project]</a></h5>
<p><em>Chi Xu, Li Cheng</em></p>
<h5 id="2012-eccv-motion-capture-of-hands-in-action-using-discriminative-salient-points-pdf-project">• [2012 ECCV] Motion Capture of Hands in Action using Discriminative Salient Points. <a href="http://ps.is.tue.mpg.de/uploads_file/attachment/attachment/76/jgall_handcapture_eccv12.pdf">[PDF]</a> <a href="http://ps.is.tue.mpg.de/publications/btgg12">[Project]</a></h5>
<p><em>Ballan, L. and Taneja, A. and Gall, J. and van Gool, L. and Pollefeys, M.</em></p>
<h5 id="2012-eccv-hand-pose-estimation-and-hand-shape-classification-using-multi-layered-randomized-decision-forests">• [2012 ECCV] Hand pose estimation and hand shape classification using multi-layered randomized decision forests.</h5>
<p><em>Cem KeskinFurkan, KıraçYunus Emre, KaraLale Akarun</em></p>
<h5 id="2011-cvprw-real-time-hand-pose-estimation-using-depth-sensors-pdf">• [2011 CVPRW] Real Time Hand Pose Estimation using Depth Sensors. <a href="http://www.cp.jku.at/teaching/praktika/imageproc/bodyparts_Algorithmus1.pdf">[PDF]</a></h5>
<p><em>Cem Keskin, Furkan Kırac, Yunus Emre Kara, Lale Akarun</em></p>
<h5 id="2011-bmvc-efficient-model-based-3d-tracking-of-hand-articulations-using-kinect-pdf-project-code">• [2011 BMVC] Efficient Model-based 3D Tracking of Hand Articulations using Kinect. <a href="http://www.cp.jku.at/teaching/praktika/imageproc/bodyparts_Algorithmus1.pdf">[PDF]</a> <a href="http://users.ics.forth.gr/~argyros/research/kinecthandtracking.htm">[Project]</a> <a href="https://github.com/FORTH-ModelBasedTracker/HandTracker">[Code]</a></h5>
<p><em>Iason Oikonomidis, Nikolaos Kyriazis, Antonis A. Argyros</em></p>
<p><a href="#contents">[back to top]</a></p>
<h2 id="theses">Theses</h2>
<h5 id="2022-hand-analysis-from-depth-images-pdf">• [2022] Hand Analysis From Depth Images. <a href="https://drive.google.com/file/d/12YtLEQAMK0rvt6YC9c4T-UxTBmgpd921/view?usp=drive_link">[PDF]</a></h5>
<p><em><a href="https://scholar.google.com/citations?user=TYT7P9oAAAAJ&amp;hl=en">Mohammad Rezaei</a>, The University of Texas at Arlington</em></p>
<h5 id="2020-learning-without-labeling-for-3d-hand-pose-estimation-pdf">• [2020] Learning without Labeling for 3D Hand Pose Estimation. <a href="https://diglib.tugraz.at/learning-without-labeling-for-3d-hand-pose-estimation-2020">[PDF]</a></h5>
<p><em><a href="https://poier.github.io/">Georg Poier</a>, Graz University of Technology</em></p>
<h5 id="2018-computational-learning-for-hand-pose-estimation-pdf">• [2018] Computational Learning for Hand Pose Estimation. <a href="https://docs.lib.purdue.edu/dissertations/AAI10743663/">[PDF]</a></h5>
<p><em><a href="http://www.chihochoi.me/">Chiho Choi</a>, Purdue University</em></p>
<h5 id="2018-3d-hand-pose-estimation-methods-datasets-and-challenges-pdf">• [2018] 3D hand pose estimation: methods, datasets, and challenges. <a href="https://spiral.imperial.ac.uk/handle/10044/1/70791">[PDF]</a></h5>
<p><em><a href="https://sites.google.com/site/shanxinyuan/">Shanxin Yuan</a>, Imperial College London</em></p>
<h5 id="2018-3d-hand-pose-estimation-using-convolutional-neural-networks-pdf">• [2018] 3D hand pose estimation using convolutional neural networks. <a href="https://spiral.imperial.ac.uk/bitstream/10044/1/68489/1/Ye-Q-2019-PhD-thesis.pdf">[PDF]</a></h5>
<p><em><a href="https://sites.google.com/site/qiyeincv/">Qi Ye</a>, Imperial College London</em></p>
<h5 id="2018-3d-hand-pose-estimation-from-images-for-interactive-applications-pdf">• [2018] 3D Hand Pose Estimation from Images for Interactive Applications. <a href="http://diglib.tugraz.at/download.php?id=5c4a489d042d4&amp;location=aleph">[PDF]</a></h5>
<p><em><a href="https://moberweger.github.io/">Markus Oberweger</a>, Graz University of Technology</em></p>
<h5 id="2018-articulated-human-pose-estimation-in-unconstrained-images-and-videos-pdf">• [2018] Articulated Human Pose Estimation in Unconstrained Images and Videos. <a href="http://hss.ulb.uni-bonn.de/2018/5292/5292.pdf">[PDF]</a></h5>
<p><em><a href="http://www.umariqbal.info/">Umar Iqbal</a>, University of Bonn</em></p>
<h5 id="2018-real-time-generative-hand-modeling-and-tracking-pdf">• [2018] Real-Time Generative Hand Modeling and Tracking. <a href="https://infoscience.epfl.ch/record/256674/files/EPFL_TH8573.pdf">[PDF]</a></h5>
<p><em>Anastasia Tkach, EPFL</em></p>
<h5 id="2018-recovery-of-the-3d-virtual-human-monocular-estimation-of-3d-shape-and-pose-with-data-driven-priors-pdf-project">• [2018] Recovery of the 3D Virtual Human: Monocular Estimation of 3D Shape and Pose with Data Driven Priors. <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/266852/Endri_Dibra_Thesis_Final.pdf?sequence=3&amp;isAllowed=y">[PDF]</a>  <a href="https://www.research-collection.ethz.ch/handle/20.500.11850/266852">[Project]</a></h5>
<p><em><a href="https://graphics.ethz.ch/~edibra/">Endri Dibra</a>, ETH Zürich</em></p>
<h5 id="2017-human-segmentation-pose-estimation-and-applications-pdf-slides">• [2017] Human Segmentation, Pose Estimation and Applications. <a href="http://sergioescalera.com/wp-content/uploads/2017/10/MeysamPhDv2.pdf">[PDF]</a> <a href="http://sergioescalera.com/wp-content/uploads/2017/10/Thesis-presentation.pdf">[Slides]</a></h5>
<p><em>Meysam Madadi, Universitat Autònoma de Barcelonato</em></p>
<h5 id="2017-capturing-hand-object-interaction-and-reconstruction-of-manipulated-objects-pdf-project">• [2017] Capturing Hand-Object Interaction and Reconstruction of Manipulated Objects. <a href="http://ps.is.tue.mpg.de/uploads_file/attachment/attachment/340/Thesis_FINAL_online.pdf">[PDF]</a> <a href="http://ps.is.tue.mpg.de/publications/tzionas-thesis-phd">[Project]</a></h5>
<p><em><a href="http://ps.is.tue.mpg.de/person/dtzionas">Dimitrios Tzionas</a>, University of Bonn</em></p>
<h5 id="2016-tracking-hands-in-action-for-gesture-based-computer-input-pdf">• [2016] Tracking Hands in Action for Gesture-based Computer Input. <a href="http://cs.stanford.edu/people/ssrinath/pubs/Dissertation_SrinathSridhar.pdf">[PDF]</a></h5>
<p><em><a href="http://cs.stanford.edu/people/ssrinath/">Srinath Sridhar</a>,  Max Planck Institute for Informatics</em></p>
<h5 id="2016-3d-hand-pose-regression-with-variants-of-decision-forests-pdf-project">• [2016] 3D hand pose regression with variants of decision forests. <a href="https://spiral.imperial.ac.uk/bitstream/10044/1/31531/1/Tang-D-2016-PhD-Thesis.pdf">[PDF]</a> <a href="https://spiral.imperial.ac.uk/handle/10044/1/31531">[Project]</a></h5>
<p><em><a href="http://www.iis.ee.ic.ac.uk/dtang/">Danhang Tang</a>, Imperial College London</em></p>
<h5 id="2016-deep-learning-for-human-motion-analysis-pdf-project">• [2016] Deep Learning for Human Motion Analysis. <a href="https://tel.archives-ouvertes.fr/tel-01470466v1/document">[PDF]</a> <a href="https://tel.archives-ouvertes.fr/tel-01470466v1">[Project]</a></h5>
<p><em><a href="https://nneverova.github.io/">Natalia Neverova</a>, National Institut of Applied Science (INSA de Lyon), France</em></p>
<h5 id="2016-real-time-hand-pose-estimation-for-human-computer-interaction-pdf-project">• [2016] Real time hand pose estimation for human computer interaction. <a href="http://epubs.surrey.ac.uk/809973/1/thesis.pdf">[PDF]</a> <a href="http://epubs.surrey.ac.uk/809973/">[Project]</a></h5>
<p><em><a href="http://www.krejov.com/">Philip Krejov</a>, University of Surrey</em></p>
<h5 id="2015-efficient-tracking-of-the-3d-articulated-motion-of-human-hands-pdf">• [2015] Efficient Tracking of the 3D Articulated Motion of Human Hands. <a href="http://users.ics.forth.gr/~oikonom/pb/oikonomidisPhDthesis.pdf">[PDF]</a></h5>
<p><em><a href="http://users.ics.forth.gr/~oikonom/pb/">Iason Oikonomidis</a>, University of Crete</em></p>
<h5 id="2015-vision-based-hand-pose-estimation-and-gesture-recognition-pdf">• [2015] Vision-based hand pose estimation and gesture recognition. <a href="https://repository.ntu.edu.sg/bitstream/handle/10356/65842/ThesisMain.pdf?sequence=1&amp;isAllowed=y">[PDF]</a></h5>
<p><em><a href="https://sites.google.com/site/seraphlh/home">Hui Liang</a>, Nanyang Technological University</em></p>
<h5 id="2015-localization-of-humans-in-images-using-convolutional-networks-pdf">• [2015] Localization of Humans in Images Using Convolutional Networks. <a href="http://www.cims.nyu.edu/~tompson/others/thesis.pdf">[PDF]</a></h5>
<p><em><a href="http://cims.nyu.edu/~tompson/">Jonathan Tompson</a>, New York University</em></p>
<p><a href="#contents">[back to top]</a></p>
<h2 id="datasets">Datasets</h2>
<ul>
<li>S/R: Synthetic (S) or Real (R) or Both (B)</li>
<li>C/D: Color (RGB) or Depth (D)</li>
<li>Obj: Interaction with objects or not</li>
<li>
<h1 id="j-no-of-joints-or-mesh-m">J:  No. of joints or Mesh (M)</h1>
</li>
<li>V: view (3rd or egocentric)</li>
<li>
<h1 id="s-no-of-subjects">S: No. of subjects</h1>
</li>
<li>
<h1 id="f-no-of-frames-traintest">F: No. of frames (train/test)</h1>
</li>
<li>Mesh: Mesh annotations</li>
</ul>
<h3 id="depth">Depth</h3>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Year</th>
<th>S/R</th>
<th>Mesh</th>
<th>Obj</th>
<th>#J</th>
<th>V</th>
<th>#S</th>
<th>#F</th>
<th>Paper</th>
<th>License</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm#overview">NYU</a></td>
<td>2014</td>
<td>R</td>
<td>❌</td>
<td>❌</td>
<td>36</td>
<td>3rd</td>
<td>2</td>
<td>72k/8k</td>
<td>SIGGRAPH 2014 <a href="http://cims.nyu.edu/~tompson/others/TOG_2014_paper_PREPRINT.pdf">[PDF]</a></td>
<td><a href="https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm#license">Creative Commons BY 4.0 license</a></td>
</tr>
<tr>
<td><a href="https://labicvl.github.io/hand.html">ICVL</a></td>
<td>2014</td>
<td>R</td>
<td>❌</td>
<td>❌</td>
<td>16</td>
<td>3rd</td>
<td>10</td>
<td>331k/1.5k</td>
<td>CVPR 2014 <a href="https://labicvl.github.io/docs/pubs/Danny_CVPR_2014.pdf">[PDF]</a></td>
<td>No mentioning of license.</td>
</tr>
<tr>
<td><a href="https://github.com/geliuhao/CVPR2016_HandPoseEstimation/issues/4">MSRA15</a></td>
<td>2015</td>
<td>R</td>
<td>❌</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>9</td>
<td>76,375</td>
<td>CVPR 2015 <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Sun_Cascaded_Hand_Pose_2015_CVPR_paper.pdf">[PDF]</a></td>
<td>No mention. Issue raised : <a href="https://github.com/geliuhao/CVPR2016_HandPoseEstimation/issues/10#issue-1871626249">link</a></td>
</tr>
<tr>
<td><a href="https://kcvl-kaist.github.io/bighand2.2M/">BigHand2.2M</a></td>
<td>2017</td>
<td>R</td>
<td>❌</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>10</td>
<td>2.2M</td>
<td>CVPR 2017 <a href="https://labicvl.github.io/docs/pubs/Shanxin_CVPR_2017.pdf">[PDF]</a></td>
<td><a href="http://icvl.ee.ic.ac.uk/hands17/challenge/">Academic Research Only</a></td>
</tr>
<tr>
<td><a href="https://bit.ly/2WMWM5u">SynHandEgo</a></td>
<td>2019</td>
<td>R</td>
<td>✅</td>
<td>❌</td>
<td>-</td>
<td>ego</td>
<td>-</td>
<td>-</td>
<td>Computers &amp; Graphics 2019 <a href="https://www.dfki.de/fileadmin/user_upload/import/10684_CAG_Jameel.pdf">[PDF]</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://kcvl-kaist.github.io/FPHA/">FPHA</a></td>
<td>2018</td>
<td>R</td>
<td>❌</td>
<td>✅</td>
<td>21</td>
<td>ego</td>
<td>6</td>
<td>100k</td>
<td>CVPR 2018 <a href="https://arxiv.org/pdf/1704.02463">[PDF]</a></td>
<td><a href="https://github.com/guiggh/hand_pose_action#terms">Academic Research Only</a></td>
</tr>
<tr>
<td><a href="https://cloud.dfki.de/owncloud/index.php/s/iCMRF7a5FkXrdpn">SynHand5M</a></td>
<td>2018</td>
<td>S</td>
<td>-</td>
<td>❌</td>
<td>M</td>
<td>3rd</td>
<td>-</td>
<td>5M</td>
<td>3DV 2018 <a href="https://arxiv.org/pdf/1808.09208.pdf">[PDF]</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52288">MSRC (FingerPaint)</a></td>
<td>2015</td>
<td>S</td>
<td>❌</td>
<td>❌</td>
<td>21</td>
<td>both</td>
<td>1</td>
<td>100k</td>
<td>CHI 2015 <a href="http://www.cs.toronto.edu/~jtaylor/papers/CHI2015-HandTracking.pdf">[PDF]</a></td>
<td></td>
</tr>
<tr>
<td><a href="http://www.cs.technion.ac.il/~twerd/HandNet/">HandNet</a></td>
<td>2015</td>
<td>R</td>
<td>❌</td>
<td>❌</td>
<td>6</td>
<td>3rd</td>
<td>10</td>
<td>202k/10k</td>
<td>BMVC 2015 <a href="http://www.cs.technion.ac.il/~twerd/WetzlerSlossbergKimmel-BMVC15.pdf">[PDF]</a></td>
<td></td>
</tr>
<tr>
<td><a href="http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/">Hands in Action</a></td>
<td>2014</td>
<td>R</td>
<td>-</td>
<td>✅</td>
<td>-</td>
<td>3rd</td>
<td>-</td>
<td>-</td>
<td>IJCV 2016 <a href="http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/IJCV_Hand_Object_Capture.pdf">[PDF]</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://jimmysuen.github.io/">MSRA14</a></td>
<td>2014</td>
<td>R</td>
<td>❌</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>6</td>
<td>2,400</td>
<td>CVPR 2014 <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf">[PDF]</a></td>
<td></td>
</tr>
<tr>
<td><a href="http://hpes.bii.a-star.edu.sg/">ASTAR</a></td>
<td>2013</td>
<td>R</td>
<td>-</td>
<td>❌</td>
<td>20</td>
<td>3rd</td>
<td>30</td>
<td>870</td>
<td>ICCV 2013 <a href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Xu_Efficient_Hand_Pose_2013_ICCV_paper.pdf">[PDF]</a></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="rgbdepth">RGB+Depth</h3>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Year</th>
<th>S/R</th>
<th>Mesh</th>
<th>Obj</th>
<th>#J</th>
<th>V</th>
<th>#S</th>
<th>#F</th>
<th>Paper</th>
<th>License</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/ShichengChen/multiviewDataset">MVHand</a></td>
<td>2021</td>
<td>R</td>
<td>✅</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>4</td>
<td>83k</td>
<td>BMVC 2021 <a href="https://arxiv.org/pdf/2112.06389.pdf">[PDF]</a></td>
<td><a href="https://github.com/ShichengChen/multiviewDataset/blob/main/LICENSE">MIT</a></td>
</tr>
<tr>
<td><a href="https://contactpose.cc.gatech.edu/">ContactPose</a></td>
<td>2020</td>
<td>R</td>
<td>✅</td>
<td>✅</td>
<td>21</td>
<td>3rd</td>
<td>50</td>
<td>2.9M</td>
<td>ECCV 2020 <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580358.pdf">[PDF]</a></td>
<td><a href="https://github.com/facebookresearch/ContactPose/blob/main/LICENSE.txt">MIT</a></td>
</tr>
<tr>
<td><a href="https://github.com/AlextheEngineer/Ego3DHands">Ego3DHands</a></td>
<td>2020</td>
<td>S</td>
<td>-</td>
<td>❌</td>
<td>21</td>
<td>ego</td>
<td>1</td>
<td>50k/5k</td>
<td>arXiv 2020 <a href="https://arxiv.org/pdf/2006.01320.pdf">[PDF]</a></td>
<td><a href="https://github.com/AlextheEngineer/Ego3DHands#license">Non-Commercial / Scientific only</a></td>
</tr>
<tr>
<td><a href="https://www.di.ens.fr/willow/research/obman/data/">ObMan</a></td>
<td>2019</td>
<td>S</td>
<td>✅</td>
<td>✅</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>150k</td>
<td>CVPR 2019 <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Hasson_Learning_Joint_Reconstruction_of_Hands_and_Manipulated_Objects_CVPR_2019_paper.pdf">[PDF]</a></td>
<td><a href="https://www.di.ens.fr/willow/research/obman/data/requestaccess.php">Non-Commercial / Scientific</a></td>
</tr>
<tr>
<td><a href="http://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm">EgoDexter</a></td>
<td>2017</td>
<td>R</td>
<td>-</td>
<td>✅</td>
<td>5</td>
<td>ego</td>
<td>4</td>
<td>1485</td>
<td>ICCV 2017 <a href="http://handtracker.mpi-inf.mpg.de/projects/OccludedHands/content/OccludedHands_ICCV2017.pdf">[PDF]</a></td>
<td><a href="https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm">Non-Commerical / Scientific</a></td>
</tr>
<tr>
<td><a href="http://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm">SynthHands</a></td>
<td>2017</td>
<td>S</td>
<td>-</td>
<td>Both</td>
<td>21</td>
<td>ego</td>
<td>2</td>
<td>63,530</td>
<td>ICCV 2017 <a href="http://handtracker.mpi-inf.mpg.de/projects/OccludedHands/content/OccludedHands_ICCV2017.pdf">[PDF]</a></td>
<td><a href="https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm">Non-Commercial / Scientific</a></td>
</tr>
<tr>
<td><a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html">RHD</a></td>
<td>2017</td>
<td>S</td>
<td>-</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>20</td>
<td>41k/2.7k</td>
<td>ICCV 2017 <a href="https://arxiv.org/pdf/1705.01389.pdf">[PDF]</a></td>
<td><a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html">Research Only</a></td>
</tr>
<tr>
<td><a href="https://sites.google.com/site/zhjw1988/">STB</a></td>
<td>2017</td>
<td>R</td>
<td>-</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>1</td>
<td>18k</td>
<td>ICIP 2017 <a href="http://www.cs.cityu.edu.hk/~jianbjiao2/pdfs/icip.pdf">[PDF]</a></td>
<td><a href="https://github.com/zhjwustc/icip17_stereo_hand_pose_dataset/blob/master/LICENSE">MIT</a></td>
</tr>
<tr>
<td><a href="http://handtracker.mpi-inf.mpg.de/projects/RealtimeHO/dexter+object.htm">Dexter+Object</a></td>
<td>2016</td>
<td>R</td>
<td>-</td>
<td>✅</td>
<td>5</td>
<td>3rd</td>
<td>2</td>
<td>3,014</td>
<td>ECCV 2016 <a href="http://handtracker.mpi-inf.mpg.de/projects/RealtimeHO/content/RealtimeHO_ECCV2016.pdf">[PDF]</a></td>
<td>No restriction mentioned. check manually.<a href="https://handtracker.mpi-inf.mpg.de/projects/RealtimeHO/dexter+object.htm">link</a></td>
</tr>
<tr>
<td><a href="http://pascal.inrialpes.fr/data2/grogez/UCI-EGO/UCI-EGO.tar.gz">UCI-EGO</a></td>
<td>2014</td>
<td>R</td>
<td>-</td>
<td>❌</td>
<td>26</td>
<td>ego</td>
<td>2</td>
<td>400</td>
<td>ECCVW 2014 <a href="https://www.cs.cmu.edu/~deva/papers/egocentric_depth_workshop.pdf">[PDF]</a></td>
<td>No restriction mentioned in the file downloaded.</td>
</tr>
<tr>
<td><a href="http://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/dexter1.htm">Dexter1</a></td>
<td>2013</td>
<td>R</td>
<td>-</td>
<td>❌</td>
<td>6</td>
<td>3rd</td>
<td>1</td>
<td>2,137</td>
<td>ICCV 2013 <a href="http://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/content/handtracker_iccv2013.pdf">[PDF]</a></td>
<td>No restriction Mentioned.<a href="https://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/dexter1.htm">link</a></td>
</tr>
</tbody>
</table>
<h3 id="rgb">RGB</h3>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Year</th>
<th>S/R</th>
<th>Mesh</th>
<th>Obj</th>
<th>#J</th>
<th>V</th>
<th>#S</th>
<th>#F</th>
<th>Paper</th>
<th>License</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/MandyMo/HIU-DMTL/">HIU-DMTL-Data</a></td>
<td>2021</td>
<td>R</td>
<td>❌</td>
<td>❌</td>
<td>21</td>
<td>3rd/ego</td>
<td>200</td>
<td>40,000</td>
<td>ICCV 2021 <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Hand_Image_Understanding_via_Deep_Multi-Task_Learning_ICCV_2021_paper.pdf">[PDF]</a></td>
<td><a href="https://github.com/MandyMo/HIU-DMTL/blob/main/LICENSE">MIT</a></td>
</tr>
<tr>
<td><a href="https://mks0601.github.io/InterHand2.6M/">InterHand2.6M</a></td>
<td>2020</td>
<td>R</td>
<td>❌</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>27</td>
<td>2.6M</td>
<td>ECCV 2020 <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650545.pdf">[PDF]</a></td>
<td><a href="https://github.com/facebookresearch/InterHand2.6M#license">CC-BY-NC 4.0</a></td>
</tr>
<tr>
<td><a href="https://github.com/arielai/youtube_3d_hands">YouTube 3D Hands</a></td>
<td>2020</td>
<td>R</td>
<td>✅</td>
<td>✅</td>
<td>-</td>
<td>3rd</td>
<td>-</td>
<td>47,125/1525/1525</td>
<td>CVPR 2020 <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kulon_Weakly-Supervised_Mesh-Convolutional_Hand_Reconstruction_in_the_Wild_CVPR_2020_paper.pdf">[PDF]</a></td>
<td><a href="https://github.com/snap-research/arielai_youtube_3d_hands/blob/master/LICENSE">Non-Commercial</a></td>
</tr>
<tr>
<td><a href="https://yangangwang.com/papers/WANG-MCC-2018-10.html">OneHand10K</a></td>
<td>2019</td>
<td>R</td>
<td>-</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>1</td>
<td>10k/1.3k</td>
<td>TCSVT 2019 <a href="https://yangangwang.com/papers/WANG-MCC-2018-10.pdf">[PDF]</a></td>
<td><a href="https://www.yangangwang.com/papers/WANG-MCC-2018-10.html">Non-Commercial</a></td>
</tr>
<tr>
<td><a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html">FreiHAND</a></td>
<td>2019</td>
<td>R</td>
<td>-</td>
<td>✅</td>
<td>21</td>
<td>3rd</td>
<td>-</td>
<td>130k/3960</td>
<td>ICCV 2019 <a href="https://arxiv.org/pdf/1909.04349.pdf">[PDF]</a></td>
<td><a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html">Research Only</a></td>
</tr>
<tr>
<td><a href="https://handtracker.mpi-inf.mpg.de/projects/GANeratedHands/GANeratedDataset.htm">GANerated Hands</a></td>
<td>2018</td>
<td>S</td>
<td>-</td>
<td>Both</td>
<td>21</td>
<td>ego</td>
<td>-</td>
<td>330k</td>
<td>CVPR 2018 <a href="https://handtracker.mpi-inf.mpg.de/projects/GANeratedHands/content/GANeratedHands_CVPR2018.pdf">[PDF]</a></td>
<td><a href="https://handtracker.mpi-inf.mpg.de/projects/GANeratedHands/GANeratedDataset.htm">Scientific / Non-commercial</a></td>
</tr>
<tr>
<td><a href="http://domedb.perception.cs.cmu.edu/handdb.html">CMU Panoptic HandDB</a></td>
<td>2017</td>
<td>B</td>
<td>-</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>-</td>
<td>14,817</td>
<td>CVPR 2017 <a href="https://arxiv.org/pdf/1704.07809">[PDF]</a></td>
<td>TODO  - waiting for confirmation from authors.</td>
</tr>
<tr>
<td><a href="http://www.rovit.ua.es/dataset/mhpdataset/">MHP</a></td>
<td>2017</td>
<td>R</td>
<td>-</td>
<td>❌</td>
<td>21</td>
<td>3rd</td>
<td>9</td>
<td>80k</td>
<td>IVC 2017 <a href="https://arxiv.org/pdf/1707.03742.pdf">[PDF]</a></td>
<td><a href="http://www.rovit.ua.es/dataset/mhpdataset/license.txt">BSD</a></td>
</tr>
</tbody>
</table>
<p><strong>Credits:</strong></p>
<ul>
<li>[1] Big Hand 2.2M Benchmark: Hand Pose Data Set and State of the Art Analysis, CVPR 2017 <a href="https://labicvl.github.io/docs/pubs/Shanxin_CVPR_2017.pdf">[PDF]</a></li>
<li>[2] Depth-based hand pose estimation: methods, data, and challenges, ICCV 2015  <a href="http://arrummzen.net/#HandData">Link</a></li>
<li>[3] Capturing Hand-Object Interaction and Reconstruction of Manipulated Objects, IJCV 2016 <a href="http://ps.is.tue.mpg.de/uploads_file/attachment/attachment/340/Thesis_FINAL_online.pdf">[PDF]</a></li>
<li>[4] <a href="http://handtracker.mpi-inf.mpg.de/">MPI Hand Tracking Central</a></li>
</ul>
<p><a href="#contents">[back to top]</a></p>
<h2 id="workshops">Workshops</h2>
<h4 id="1-workshops-on-observing-and-understanding-hands-in-action">[1] <em>Workshops on Observing and Understanding Hands in Action:</em></h4>
<h5 id="hands-2019-in-conjunction-with-iccv-2019">• <a href="https://sites.google.com/view/hands2019/">HANDS 2019</a>, In conjunction with ICCV 2019</h5>
<h5 id="hands-2018-in-conjunction-with-eccv-2018">• <a href="https://sites.google.com/view/hands2018/">HANDS 2018</a>, In conjunction with ECCV 2018</h5>
<ul>
<li>HANDS18: Methods, Techniques and Applications for Hand Observation. <a href="https://arxiv.org/abs/1810.10818">[PDF]</a></li>
</ul>
<h5 id="hands-2017-in-conjunction-with-iccv-2017">• <a href="http://icvl.ee.ic.ac.uk/hands17/">HANDS 2017</a>, In conjunction with ICCV 2017</h5>
<ul>
<li>Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future Goals. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yuan_Depth-Based_3D_Hand_CVPR_2018_paper.pdf">[PDF]</a></li>
</ul>
<h5 id="hands-2016-in-conjunction-with-cvpr-2016">• <a href="https://labicvl.github.io/hand/Hands2016/">HANDS 2016</a>, In conjunction with CVPR 2016</h5>
<h5 id="hands-2015-in-conjunction-with-cvpr-2015">• <a href="http://www.ics.uci.edu/~jsupanci/HANDS-2015/">HANDS 2015</a>, In conjunction with CVPR 2015</h5>
<h4 id="2-workshops-on-capturing-and-modeling-human-bodies-faces-and-hands">[2] <em>Workshops on Capturing and modeling human bodies, faces and hands:</em></h4>
<h5 id="peoplecap-2018-in-conjunction-with-eccv-2018">• <a href="https://peoplecap2018.weebly.com/">PeopleCap 2018</a>, In conjunction with ECCV 2018</h5>
<h5 id="peoplecap-2017-in-conjunction-with-iccv-2017">• <a href="http://peoplecap.weebly.com/">PeopleCap 2017</a>, In conjunction with ICCV 2017</h5>
<p><a href="#contents">[back to top]</a></p>
<h2 id="challenges">Challenges</h2>
<h4 id="1-hands19-challenge">[1] <a href="https://sites.google.com/view/hands2019/challenge">HANDS19 Challenge</a></h4>
<p><em><a href="https://sites.google.com/view/hands2019/">HANDS 2019</a>, <a href="http://iccv2019.thecvf.com/">ICCV 2019</a></em></p>
<ul>
<li>Submission Website: <a href="https://competitions.codalab.org/competitions/20913">Depth-Based Task</a>, <a href="https://competitions.codalab.org/competitions/20449">Depth-Based Hand-Object Task</a>, <a href="https://competitions.codalab.org/competitions/21116">RGB-Based Hand-Object Task</a></li>
<li>Documents</li>
<li><a href="https://arxiv.org/pdf/2003.13764.pdf">Measuring Generalisation to Unseen Viewpoints, Articulations, Shapes and Objects for 3D Hand Pose Estimation under Hand-Object Interaction</a>, ECCV 2020</li>
</ul>
<h4 id="2-the-2017-hands-in-the-million-challenge-on-3d-hand-pose-estimation">[2] <a href="http://icvl.ee.ic.ac.uk/hands17/challenge/">The 2017 Hands in the Million Challenge on 3D Hand Pose Estimation</a></h4>
<p><em><a href="http://icvl.ee.ic.ac.uk/hands17/">HANDS 2017</a>, <a href="http://iccv2017.thecvf.com/">ICCV 2017</a></em></p>
<ul>
<li>Submission Website: <a href="https://competitions.codalab.org/competitions/17356#results">Frame and Tracking Task</a>, <a href="https://competitions.codalab.org/competitions/17452#results">Hand-Object Task</a></li>
<li>Documents</li>
<li><a href="https://arxiv.org/pdf/1707.02237.pdf">The 2017 Hands in the Million Challenge on 3D Hand Pose Estimation</a>, arXiv 2017</li>
<li><a href="https://arxiv.org/pdf/1712.03917.pdf">Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future Goals.</a>, CVPR 2018</li>
</ul>
<p><a href="#contents">[back to top]</a></p>
<h2 id="other-related-papers">Other Related Papers</h2>
<h5 id="arxiv-201107252-ego2hands-a-dataset-for-egocentric-two-hand-segmentation-and-detection-pdf-code">• <a href="https://arxiv.org/abs/2011.07252">[arXiv 2011.07252]</a> Ego2Hands: A Dataset for Egocentric Two-hand Segmentation and Detection. <a href="https://arxiv.org/pdf/2011.07252.pdf">[PDF]</a> <a href="https://github.com/AlextheEngineer/Ego2Hands">[Code]</a></h5>
<p><em>Fanqing Lin, Tony Martinez</em></p>
<h5 id="2020-ijcv-learning-multi-human-optical-flow-pdf-project-code">• [2020 IJCV] Learning Multi-Human Optical Flow. <a href="https://link.springer.com/content/pdf/10.1007/s11263-019-01279-w.pdf">[PDF]</a> <a href="https://humanflow.is.tue.mpg.de/">[Project]</a> <a href="https://github.com/anuragranj/humanflow2">[Code]</a></h5>
<p><em>Anurag Ranjan, David T. Hoffmann, Dimitrios Tzionas, Siyu Tang, Javier Romero, and Michael J. Black</em></p>
<h5 id="2019-iccv-contextual-attention-for-hand-detection-in-the-wild-pdf-project-code">• [2019 ICCV] Contextual Attention for Hand Detection in the Wild. <a href="https://arxiv.org/pdf/1904.04882.pdf">[PDF]</a> <a href="http://vision.cs.stonybrook.edu/~supreeth/">[Project]</a> <a href="https://github.com/SupreethN/Hand-CNN">[Code]</a></h5>
<p><em>Supreeth Narasimhaswamy*, Zhengwei Wei*, Yang Wang, Justin Zhang, Minh Hoai</em></p>
<h5 id="2018-siggraph-online-optical-marker-based-hand-tracking-with-deep-labels-pdf-code">• [2018 SIGGRAPH] Online Optical Marker-based Hand Tracking with Deep Labels. <a href="https://research.fb.com/wp-content/uploads/2018/06/Online-Optical-Marker-based-Hand-Tracking-with-Deep-Labels.pdf">[PDF]</a> <a href="https://github.com/Beibei88/Mocap_SIG18_Data">[Code]</a></h5>
<p><em>Shangchen Han, Beibei Liu, Robert Wang, Yuting Ye, Christopher D. Twigg, Kenrick Kin</em></p>
<h5 id="2018-cvprw-handynet-a-one-stop-solution-to-detect-segment-localize-analyze-driver-hands-pdf-code">• [2018 CVPRW] HandyNet: A One-stop Solution to Detect, Segment, Localize &amp; Analyze Driver Hands. <a href="https://arxiv.org/pdf/1804.07834.pdf">[PDF]</a> <a href="https://github.com/arangesh/HandyNet">[Code]</a></h5>
<p><em>Akshay Rangesh, Mohan M. Trivedi</em></p>
<h5 id="2018-cvpr-densepose-dense-human-pose-estimation-in-the-wild-pdf-project-code">• [2018 CVPR] DensePose: Dense Human Pose Estimation In The Wild. <a href="https://arxiv.org/pdf/1802.00434.pdf">[PDF]</a>  <a href="http://densepose.org/">[Project]</a> <a href="https://github.com/facebookresearch/DensePose">[Code]</a></h5>
<p><em>Rıza Alp Güler, Natalia Neverova, Iasonas Kokkinos</em></p>
<h5 id="2018-cvpr-analysis-of-hand-segmentation-in-the-wild-pdf-code">• [2018 CVPR] Analysis of Hand Segmentation in the Wild. <a href="https://arxiv.org/pdf/1803.03317.pdf">[PDF]</a> <a href="https://github.com/aurooj/Hand-Segmentation-in-the-Wild">[Code]</a></h5>
<p><em>Aisha Urooj Khan, Ali Borji</em></p>
<h5 id="2018-cvpr-total-capture-a-3d-deformation-model-for-tracking-faces-hands-and-bodies-pdf-project-cvpr-best-student-paper-award">• [2018 CVPR] Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies. <a href="https://arxiv.org/pdf/1801.01615.pdf">[PDF]</a> <a href="http://www.cs.cmu.edu/~hanbyulj/totalcapture/">[Project]</a> <em>(CVPR Best Student Paper Award)</em></h5>
<p><em>Hanbyul Joo, Tomas Simon, Yaser Sheikh</em></p>
<h5 id="2017-cvpr-surfnet-generating-3d-shape-surfaces-using-deep-residual-networks-pdf">• [2017 CVPR] SurfNet: Generating 3D shape surfaces using deep residual networks. <a href="https://engineering.purdue.edu/cdesign/wp/wp-content/uploads/2017/03/Sinha_CVPR17.pdf">[PDF]</a></h5>
<p><em>Ayan Sinha, Asim Unmesh, Qixing Huang, Karthik Ramani</em></p>
<h5 id="2017-cvpr-learning-from-simulated-and-unsupervised-images-through-adversarial-training-pdf-project-code-tensorflow-code-keras-code-tensorflow-nyu-hand-cvpr-best-paper-award">• [2017 CVPR] Learning from Simulated and Unsupervised Images through Adversarial Training. <a href="https://arxiv.org/pdf/1511.06728">[PDF]</a> <a href="https://machinelearning.apple.com/2017/07/07/GAN.html">[Project]</a> <a href="https://github.com/carpedm20/simulated-unsupervised-tensorflow">[Code-Tensorflow]</a> <a href="https://github.com/wayaai/SimGAN">[Code-Keras]</a> <a href="https://github.com/shinseung428/simGAN_NYU_Hand">[Code-Tensorflow-NYU-Hand]</a> <em>(CVPR Best Paper Award)</em></h5>
<p><em>Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, Russ Webb</em></p>
<h5 id="2016-3dv-learning-to-navigate-the-energy-landscape-pdf-project">• [2016 3DV] Learning to Navigate the Energy Landscape. <a href="http://www.robots.ox.ac.uk/~tvg/publications/2016/LNEL.pdf">[PDF]</a> <a href="http://graphics.stanford.edu/projects/reloc/">[Project]</a></h5>
<p><em>Julien Valentin, Angela Dai, Matthias Niessner, Pushmeet Kohli, Philip H.S. Torr, Shahram Izadi</em></p>
<h5 id="2015-iccv-3d-object-reconstruction-from-hand-object-interactions-pdf-project">• [2015 ICCV] 3D Object Reconstruction from Hand-Object Interactions. <a href="http://files.is.tue.mpg.de/dtzionas/In-Hand-Scanning/ICCV15_Reconstruction_from_HandObject_Interactions.pdf">[PDF]</a> <a href="http://ps.is.tue.mpg.de/publications/-886ddd69-ebde-4f83-8b77-9c41f8af1065">[Project]</a></h5>
<p><em>Dimitrios Tzionas and Juergen Gall</em></p>
<p><a href="#contents">[back to top]</a></p>
<h2 id="contribute">Contribute</h2>
<p>Contributions welcome! Read the <a href="../contributing/">contribution guidelines</a> first.</p>
<p>Please feel free to <a href="https://github.com/xinghaochen/awesome-hand-pose-estimation/pulls">pull requests</a>, <a href="https://github.com/xinghaochen/awesome-hand-pose-estimation/issues">open an issue</a> or send me email (chenxinghaothu@gmail.com) to add awesome papers.</p>
<h2 id="license">License</h2>
<p><a href="https://creativecommons.org/publicdomain/zero/1.0"><img alt="CC0" src="https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg" /></a></p>
<p>To the extent possible under law, <a href="https://github.com/xinghaochen">xinghaochen</a> has waived all copyright and
related or neighboring rights to this work.</p>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.indexes"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5090c770.min.js"></script>
      
    
  </body>
</html>