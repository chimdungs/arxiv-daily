
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://chimdungs.github.io/arxiv-daily/Awesome_Pages/Awesome-Multimodal-Large-Language-Models/Awesome-Multimodal-Large-Language-Models_README/">
      
      
        <link rel="prev" href="../">
      
      
        <link rel="next" href="../images/readme/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.1">
    
    
      
        <title>Awesome-Multimodal-Large-Language-Models - arxiv-daily</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.a40c8224.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config",""),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#awesome-multimodal-large-language-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="arxiv-daily" class="md-header__button md-logo" aria-label="arxiv-daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            arxiv-daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Awesome-Multimodal-Large-Language-Models
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/chimdungs/arxiv-daily" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="arxiv-daily" class="md-nav__button md-logo" aria-label="arxiv-daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    arxiv-daily
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/chimdungs/arxiv-daily" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../3D%20Vision/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    3D Vision
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            3D Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/3D%20Object%20Detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3D Object Detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/3D%20Object%20Tracking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3D Object Tracking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/3D%20Reconstruction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3D Reconstruction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud%20Completion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud Completion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud%20Matching/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud Matching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud%20Registration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud Registration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud%20Segmentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud Segmentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../3D%20Vision/Point%20Cloud/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Point Cloud
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Awesome Pages
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Awesome Pages
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Awesome-Diffusion-Models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome Diffusion Models
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Awesome Diffusion Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-Diffusion-Models/Awesome-Diffusion-Models_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome Diffusion Models README
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Awesome-Foundation-Models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome Foundation Models
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Awesome Foundation Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-Foundation-Models/Awesome-Foundation-Models_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome-Foundation-Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Awesome-GenAI-Watermarking/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome GenAI Watermarking
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Awesome GenAI Watermarking
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-GenAI-Watermarking/Awesome-GenAI-Watermarking_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome-GenAI-Watermarking
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Awesome-LLM/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome LLM
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Awesome LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/Awesome-LLM_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome-LLM Awesome
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/LICENSE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LICENSE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/contributing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contribution Guidelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_5" >
        
          
          <label class="md-nav__link" for="__nav_3_4_5" id="__nav_3_4_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Paper list
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4_5">
            <span class="md-nav__icon md-icon"></span>
            Paper list
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/RLHF/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/Retrieval_Augmented_Generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Retrieval-Augmented Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/acceleration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Acceleration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/alignment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Alignment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/application/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Application
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/augmentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Augmentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/chain_of_thougt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Chain-of-Thought
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/code_pretraining/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code pretraining
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM-Evaluation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/in_context_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    In-context Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/instruction-tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Instruction-Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Moe
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Awesome-LLM/paper_list/prompt_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome Multimodal Large Language Models
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            Awesome Multimodal Large Language Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Awesome-Multimodal-Large-Language-Models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Awesome-Multimodal-Large-Language-Models
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#our-mllm-works" class="md-nav__link">
    <span class="md-ellipsis">
      Our MLLM works
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5_3" >
        
          
          <label class="md-nav__link" for="__nav_3_5_3" id="__nav_3_5_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Images
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5_3">
            <span class="md-nav__icon md-icon"></span>
            Images
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../images/readme/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Readme
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../awesome-3D-generation/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    awesome 3D generation
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            awesome 3D generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-3D-generation/awesome-3D-generation_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome 3D Generation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../awesome-3d-reconstruction-papers/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome 3d reconstruction papers
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            Awesome 3d reconstruction papers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-3d-reconstruction-papers/awesome-3d-reconstruction-papers_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome 3D Reconstruction Papers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../awesome-hand-pose-estimation/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome hand pose estimation
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8">
            <span class="md-nav__icon md-icon"></span>
            Awesome hand pose estimation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-hand-pose-estimation/awesome-hand-pose-estimation_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome Hand Pose Estimation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-hand-pose-estimation/code-of-conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributor Covenant Code of Conduct
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-hand-pose-estimation/contributing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contribution Guidelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../awesome-hand-pose-estimation/evaluation/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Evaluation
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_8_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8_5">
            <span class="md-nav__icon md-icon"></span>
            Evaluation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../awesome-scene-understanding/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Awesome scene understanding
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_9">
            <span class="md-nav__icon md-icon"></span>
            Awesome scene understanding
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-scene-understanding/awesome-scene-understanding_README/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome Scene Understanding Awesome
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Computer%20Vision/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Computer Vision
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Computer Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/3D%20Reconstruction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3D Reconstruction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Image%20Classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Classification
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Image%20Matching/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Matching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Instance%20Segmentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Instance Segmentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Keypoint%20Detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Keypoint Detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Multi-Object%20Tracking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi Object Tracking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Object%20Detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Object Detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Object%20Tracking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Object Tracking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Computer%20Vision/Semantic%20Segmentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Semantic Segmentation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Contrastive%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Contrastive Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Contrastive Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Contrastive%20Learning/Contrastive%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contrastive Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Federated%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Federated Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Federated Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Asynchronous/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Asynchronous
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Benchmark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Benchmark
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataset
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Federated%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Federated Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Framework/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Heterogeneous/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Heterogeneous
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Federated%20Learning/Personalized/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Personalized
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Few-shot%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Few shot Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_7" id="__nav_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Few shot Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Few-shot%20Learning/Few-shot%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Few shot Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Few-shot%20Learning/Meta%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Meta Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Few-shot%20Learning/One-shot%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    One shot Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Graph%20Neural%20Network/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Graph Neural Network
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_8" id="__nav_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Graph Neural Network
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Graph%20Neural%20Network/Graph%20Neural%20Network/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Graph Neural Network
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    HuggingFace
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            HuggingFace
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../HuggingFace/huggface_news/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face News
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Large-Language%20Model/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Large Language Model
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10" id="__nav_10_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Large Language Model
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Large-Language%20Model/Large-Language%20Model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Large Language Model
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Medical%20Application/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Medical Application
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11" id="__nav_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            Medical Application
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Medical%20Application/Medical%20Application/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Medical Application
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Medical%20Application/Medical%20Image%20Analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Medical Image Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Medical%20Application/Medical%20Multi-modal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Medical Multi modal
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Multi-modal/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Multi modal
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_12" id="__nav_12_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12">
            <span class="md-nav__icon md-icon"></span>
            Multi modal
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Alignment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Alignment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Image%20Caption/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Caption
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Multi-modal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi modal
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Text%20and%20Image%20Generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text and Image Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/VQA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VQA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Multi-modal/Vision-Language/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision Language
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Reinforcement%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13" id="__nav_13_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Reinforcement%20Learning/Reinforcement%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_14" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Robotics/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Robotics
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_14" id="__nav_14_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_14">
            <span class="md-nav__icon md-icon"></span>
            Robotics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Robotics/Robotics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Robotics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Robotics/SFM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SFM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Robotics/SLAM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Robotics/Visual%20Localization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Visual Localization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_15" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Transfer%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transfer Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_15" id="__nav_15_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_15">
            <span class="md-nav__icon md-icon"></span>
            Transfer Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Transfer%20Learning/Transfer%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transfer Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_16" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Transformer/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformer
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_16" id="__nav_16_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_16">
            <span class="md-nav__icon md-icon"></span>
            Transformer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Transformer/Transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Transformer/Vision%20Transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_17" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Unsupervised%20Learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_17" id="__nav_17_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_17">
            <span class="md-nav__icon md-icon"></span>
            Unsupervised Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Unsupervised%20Learning/GAN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GAN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Unsupervised%20Learning/Unsupervised%20Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_18" >
        
          
          <label class="md-nav__link" for="__nav_18" id="__nav_18_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    History
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_18">
            <span class="md-nav__icon md-icon"></span>
            History
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-11-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2021-12-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-01-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-02-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-03-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-04-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-05-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-06-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-24/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-25/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-26/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-27/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-28/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-29/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-30/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-07-31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2022-08-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-01-08/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-01-09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-01-10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-01-13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../history/2025-02-01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    arxiv-daily
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#our-mllm-works" class="md-nav__link">
    <span class="md-ellipsis">
      Our MLLM works
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="awesome-multimodal-large-language-models">Awesome-Multimodal-Large-Language-Models</h1>
<h2 id="our-mllm-works">Our MLLM works</h2>
<p> <strong>A Survey on Multimodal Large Language Models</strong><br />
<strong><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Project Page [This Page]</a></strong> | <strong><a href="https://arxiv.org/pdf/2306.13549.pdf">Paper</a></strong></p>
<p>The first comprehensive survey for Multimodal Large Language Models (MLLMs). <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/2728.svg" title=":sparkles:" />  </p>
<p>Welcome to add WeChat ID (wmd_ustc) to join our MLLM communication group! <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f31f.svg" title=":star2:" />  </p>
<hr />
<p> <strong>VITA: Towards Open-Source Interactive Omni Multimodal LLM</strong>  </p>
<p align="center">
    <img src="./images/vita-1.5.jpg" width="80%" height="80%">
</p>

<p><font size=7><div align='center' > [<a href="https://youtu.be/tyi6SVFT5mM?si=fkMQCrwa5fVnmEe7"> VITA-1.5 Demo Show! Here We Go! </a>] </div></font>  </p>
<p><font size=7><div align='center' > [<a href="https://arxiv.org/pdf/2501.01957"> VITA-1.5 Paper</a>] [<a href="https://github.com/VITA-MLLM/VITA"> GitHub</a>] [<a href="https://modelscope.cn/studios/modelscope/VITA1.5_demo"> Basic Demo</a>] [<a href="https://vita-home.github.io/"> VITA-1.0</a>] [<a href="https://github.com/VITA-MLLM/VITA/blob/main/asset/wechat-group.jpg"> WeChat ()</a>]</div></font>  </p>
<p><font size=7><div align='center' > We are excited to introduce the <strong>VITA-1.5</strong>, a more powerful and more real-time version.  </div></font></p>
<p><font size=7><div align='center' ><strong>All codes of VITA-1.5 have been released</strong>! <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f31f.svg" title=":star2:" /> </div></font>  </p>
<p>You can experience our <a href="https://modelscope.cn/studios/modelscope/VITA1.5_demo">Basic Demo</a> on ModelScope directly. The Real-Time Interactive Demo needs to be configured according to the <a href="https://github.com/VITA-MLLM/VITA?tab=readme-ov-file#-real-time-interactive-demo">instructions</a>.</p>
<hr />
<p> <strong>MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs</strong>  </p>
<p align="center">
    <img src="./images/mme-survey.jpg" width="90%" height="90%">
</p>

<p><font size=7><div align='center' > [<a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks"> Project Page</a>] [<a href="https://arxiv.org/pdf/2411.15296"> arXiv Paper</a>] </div></font></p>
<p><font size=7><div align='center' > Jointly introduced by <strong>MME</strong>, <strong>MMBench</strong>, and <strong>LLaVA</strong> teams.  </div></font></p>
<hr />
<p> <strong>Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</strong><br />
<strong><a href="https://video-mme.github.io/">Project Page</a></strong> | <strong><a href="https://arxiv.org/pdf/2405.21075">Paper</a></strong> | <strong><a href="https://github.com/BradyFU/Video-MME">GitHub</a></strong> | <strong><a href="https://github.com/BradyFU/Video-MME?tab=readme-ov-file#-dataset">Dataset</a></strong> | <strong><a href="https://video-mme.github.io/home_page.html#leaderboard">Leaderboard</a></strong></p>
<p>We are very proud to launch Video-MME, the first-ever comprehensive evaluation benchmark of MLLMs in Video Analysis!   </p>
<p>It includes short- (&lt; 2min), medium- (4min\~15min), and long-term (30min\~60min) videos, ranging from <b>11 seconds to 1 hour</b>. All data are newly collected and annotated by humans, not from any existing video dataset.  </p>
<hr />
<p> <strong>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</strong><br />
<strong><a href="https://arxiv.org/pdf/2306.13394.pdf">Paper</a></strong> | <strong><a href="https://huggingface.co/datasets/darkyarding/MME/blob/main/MME_Benchmark_release_version.zip">Download</a></strong> | <strong><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/blob/Evaluation/tools/eval_tool.zip">Eval Tool</a></strong> | <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/2712.svg" title=":black_nib:" /> <strong><a href="../images/bib_mme.txt">Citation</a></strong></p>
<p>A representative evaluation benchmark for MLLMs. <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/2728.svg" title=":sparkles:" />  </p>
<hr />
<p> <strong>Woodpecker: Hallucination Correction for Multimodal Large Language Models</strong><br />
<strong><a href="https://arxiv.org/pdf/2310.16045">Paper</a></strong> | <strong><a href="https://github.com/BradyFU/Woodpecker">GitHub</a></strong></p>
<p>This is the first work to correct hallucination in multimodal large language models. <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/2728.svg" title=":sparkles:" />  </p>
<hr />
<p> <strong>Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM</strong><br />
<strong><a href="https://freeze-omni.github.io/">Project Page</a></strong> | <strong><a href="https://arxiv.org/abs/2411.00774">Paper</a></strong> | <strong><a href="https://github.com/VITA-MLLM/Freeze-Omni">GitHub</a></strong>  </p>
<p>A speech-to-speech dialogue model with both low-latency and high intelligence while the training process is based on a frozen LLM. <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/2728.svg" title=":sparkles:" />  </p>
<hr />
<p><font size=5><center><b> Table of Contents </b> </center></font>
- <a href="#awesome-papers">Awesome Papers</a>
  - <a href="#multimodal-instruction-tuning">Multimodal Instruction Tuning</a>
  - <a href="#multimodal-hallucination">Multimodal Hallucination</a>
  - <a href="#multimodal-in-context-learning">Multimodal In-Context Learning</a>
  - <a href="#multimodal-chain-of-thought">Multimodal Chain-of-Thought</a>
  - <a href="#llm-aided-visual-reasoning">LLM-Aided Visual Reasoning</a>
  - <a href="#foundation-models">Foundation Models</a>
  - <a href="#evaluation">Evaluation</a>
  - <a href="#multimodal-rlhf">Multimodal RLHF</a>
  - <a href="#others">Others</a>
- <a href="#awesome-datasets">Awesome Datasets</a>
  - <a href="#datasets-of-pre-training-for-alignment">Datasets of Pre-Training for Alignment</a>
  - <a href="#datasets-of-multimodal-instruction-tuning">Datasets of Multimodal Instruction Tuning</a>
  - <a href="#datasets-of-in-context-learning">Datasets of In-Context Learning</a>
  - <a href="#datasets-of-multimodal-chain-of-thought">Datasets of Multimodal Chain-of-Thought</a>
  - <a href="#datasets-of-multimodal-rlhf">Datasets of Multimodal RLHF</a>
  - <a href="#benchmarks-for-evaluation">Benchmarks for Evaluation</a>
  - <a href="#others-1">Others</a></p>
<hr />
<h1 id="awesome-papers">Awesome Papers</h1>
<h2 id="multimodal-instruction-tuning">Multimodal Instruction Tuning</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: center;">Venue</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Demo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL.svg?style=social&amp;label=Star" /> <br> <a href="https://qwenlm.github.io/blog/qwen2.5-vl/"><strong>Qwen2.5-VL</strong></a> <br></td>
<td style="text-align: center;">Qwen</td>
<td style="text-align: center;">2025-01-26</td>
<td style="text-align: center;"><a href="https://github.com/QwenLM/Qwen2.5-VL">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/Qwen/Qwen2.5-VL">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/baichuan-inc/Baichuan-Omni-1.5.svg?style=social&amp;label=Star" /> <br> <a href="https://github.com/baichuan-inc/Baichuan-Omni-1.5/blob/main/baichuan_omni_1_5.pdf"><strong>Baichuan-Omni-1.5 Technical Report</strong></a> <br></td>
<td style="text-align: center;">Tech Report</td>
<td style="text-align: center;">2025-01-26</td>
<td style="text-align: center;"><a href="https://github.com/baichuan-inc/Baichuan-Omni-1.5">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mbzuai-oryx/LlamaV-o1.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2501.06186"><strong>LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2025-01-10</td>
<td style="text-align: center;"><a href="https://github.com/mbzuai-oryx/LlamaV-o1">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2501.01957"><strong>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2025-01-03</td>
<td style="text-align: center;"><a href="https://github.com/VITA-MLLM/VITA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/QwenLM/Qwen2-VL.svg?style=social&amp;label=Star" /> <br> <a href="https://qwenlm.github.io/blog/qvq-72b-preview/"><strong>QVQ: To See the World with Wisdom</strong></a> <br></td>
<td style="text-align: center;">Qwen</td>
<td style="text-align: center;">2024-12-25</td>
<td style="text-align: center;"><a href="https://github.com/QwenLM/Qwen2-VL">Github</a></td>
<td style="text-align: center;"><a href="https://qwenlm.github.io/blog/qvq-72b-preview/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-VL2.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2412.10302"><strong>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-13</td>
<td style="text-align: center;"><a href="https://github.com/deepseek-ai/DeepSeek-VL2">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2412.10360"><strong>Apollo: An Exploration of Video Understanding in Large Multimodal Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2412.09596"><strong>InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-12</td>
<td style="text-align: center;"><a href="https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2412.08646"><strong>StreamChat: Chatting with Streaming Video</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-11</td>
<td style="text-align: center;">Coming soon</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2412.05243"><strong>CompCap: Improving Multimodal Large Language Models with Composite Captions</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-06</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/gls0425/LinVT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2412.05185"><strong>LinVT: Empower Your Image-level Large Language Model to Understand Videos</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-06</td>
<td style="text-align: center;"><a href="https://github.com/gls0425/LinVT">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2412.05271"><strong>Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-06</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/InternVL">Github</a></td>
<td style="text-align: center;"><a href="https://internvl.opengvlab.com">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/NVlabs/VILA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2412.04468"><strong>NVILA: Efficient Frontier Visual Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-05</td>
<td style="text-align: center;"><a href="https://github.com/NVlabs/VILA">Github</a></td>
<td style="text-align: center;"><a href="https://vila.mit.edu">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/inst-it/inst-it.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2412.03565"><strong>Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-04</td>
<td style="text-align: center;"><a href="https://github.com/inst-it/inst-it">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/xjtupanda/T2Vid.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2411.19951"><strong>T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-11-29</td>
<td style="text-align: center;"><a href="https://github.com/xjtupanda/T2Vid">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/TimeMarker-LLM/TimeMarker.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2411.18211"><strong>TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-11-27</td>
<td style="text-align: center;"><a href="https://github.com/TimeMarker-LLM/TimeMarker/">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/IDEA-Research/ChatRex.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2411.18363"><strong>ChatRex: Taming Multimodal LLM for Joint Perception and Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-11-27</td>
<td style="text-align: center;"><a href="https://github.com/IDEA-Research/ChatRex">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Vision-CAIR/LongVU.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2410.17434"><strong>LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-10-22</td>
<td style="text-align: center;"><a href="https://github.com/Vision-CAIR/LongVU">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/Vision-CAIR/LongVU">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/shikiw/Modality-Integration-Rate.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2410.07167"><strong>Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-10-09</td>
<td style="text-align: center;"><a href="https://github.com/shikiw/Modality-Integration-Rate">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/rese1f/aurora.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2410.03051"><strong>AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-10-04</td>
<td style="text-align: center;"><a href="https://github.com/rese1f/aurora">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2409.17146"><strong>Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-09-25</td>
<td style="text-align: center;"><a href="https://huggingface.co/allenai/MolmoE-1B-0924">Huggingface</a></td>
<td style="text-align: center;"><a href="https://molmo.allenai.org">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/QwenLM/Qwen2-VL.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2409.12191"><strong>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-09-18</td>
<td style="text-align: center;"><a href="https://github.com/QwenLM/Qwen2-VL">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/Qwen/Qwen2-VL">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/IDEA-FinAI/ChartMoE.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2409.03277"><strong>ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2024-09-05</td>
<td style="text-align: center;"><a href="https://github.com/IDEA-FinAI/ChartMoE">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/FreedomIntelligence/LongLLaVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2409.02889"><strong>LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-09-04</td>
<td style="text-align: center;"><a href="https://github.com/FreedomIntelligence/LongLLaVA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/NVlabs/Eagle.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2408.15998"><strong>EAGLE: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-28</td>
<td style="text-align: center;"><a href="https://github.com/NVlabs/Eagle">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/NVEagle/Eagle-X5-13B-Chat">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/shufangxun/LLaVA-MoD.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2408.15881"><strong>LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-28</td>
<td style="text-align: center;"><a href="https://github.com/shufangxun/LLaVA-MoD">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&amp;label=Star" /> <br> <a href="https://www.arxiv.org/pdf/2408.04840"><strong>mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-09</td>
<td style="text-align: center;"><a href="https://github.com/X-PLUG/mPLUG-Owl">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2408.05211"><strong>VITA: Towards Open-Source Interactive Omni Multimodal LLM</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-09</td>
<td style="text-align: center;"><a href="https://github.com/VITA-MLLM/VITA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2408.03326"><strong>LLaVA-OneVision: Easy Visual Task Transfer</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-06</td>
<td style="text-align: center;"><a href="https://github.com/LLaVA-VL/LLaVA-NeXT">Github</a></td>
<td style="text-align: center;"><a href="https://llava-onevision.lmms-lab.com">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenBMB/MiniCPM-V.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2408.01800"><strong>MiniCPM-V: A GPT-4V Level MLLM on Your Phone</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-03</td>
<td style="text-align: center;"><a href="https://github.com/OpenBMB/MiniCPM-V">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2407.17453"><strong>VILA^2: VILA Augmented VILA</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-07-24</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2407.15841"><strong>SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-07-22</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2407.14177"><strong>EVLM: An Efficient Vision-Language Model for Visual Understanding</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-07-19</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/jiyt17/IDA-VLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2407.07577"><strong>IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-07-10</td>
<td style="text-align: center;"><a href="https://github.com/jiyt17/IDA-VLM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2407.03320"><strong>InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-07-03</td>
<td style="text-align: center;"><a href="https://github.com/InternLM/InternLM-XComposer">Github</a></td>
<td style="text-align: center;"><a href="https://openxlab.org.cn/apps/detail/WillowBreeze/InternLM-XComposer">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/lxtGH/OMG-Seg.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.19389"><strong>OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-27</td>
<td style="text-align: center;"><a href="https://github.com/lxtGH/OMG-Seg">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ZZZHANG-jx/DocKylin.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.19101"><strong>DocKylin: A Large Multimodal Model for Visual Document Understanding with Efficient Visual Slimming</strong></a> <br></td>
<td style="text-align: center;">AAAI</td>
<td style="text-align: center;">2024-06-27</td>
<td style="text-align: center;"><a href="https://github.com/ZZZHANG-jx/DocKylin">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/cambrian-mllm/cambrian.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.16860"><strong>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-24</td>
<td style="text-align: center;"><a href="https://github.com/cambrian-mllm/cambrian">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/EvolvingLMMs-Lab/LongVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.16852"><strong>Long Context Transfer from Language to Vision</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-24</td>
<td style="text-align: center;"><a href="https://github.com/EvolvingLMMs-Lab/LongVA">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/bytedance/SALMONN.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.15704"><strong>video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models</strong></a> <br></td>
<td style="text-align: center;">ICML</td>
<td style="text-align: center;">2024-06-22</td>
<td style="text-align: center;"><a href="https://github.com/bytedance/SALMONN">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ByungKwanLee/TroL.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.12246"><strong>TroL: Traversal of Layers for Large Language and Vision Models</strong></a> <br></td>
<td style="text-align: center;">EMNLP</td>
<td style="text-align: center;">2024-06-18</td>
<td style="text-align: center;"><a href="https://github.com/ByungKwanLee/TroL">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/baaivision/EVE.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.11832"><strong>Unveiling Encoder-Free Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-17</td>
<td style="text-align: center;"><a href="https://github.com/baaivision/EVE">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/showlab/VideoLLM-online.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.11816"><strong>VideoLLM-online: Online Video Large Language Model for Streaming Video</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2024-06-17</td>
<td style="text-align: center;"><a href="https://github.com/showlab/VideoLLM-online">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/wentaoyuan/RoboPoint.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.10721"><strong>RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics</strong></a> <br></td>
<td style="text-align: center;">CoRL</td>
<td style="text-align: center;">2024-06-15</td>
<td style="text-align: center;"><a href="https://github.com/wentaoyuan/RoboPoint">Github</a></td>
<td style="text-align: center;"><a href="https://007e03d34429a2517b.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/wlin-at/CaD-VI" /> <br> <a href="https://arxiv.org/abs/2406.09240"><strong>Comparison Visual Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-13</td>
<td style="text-align: center;"><a href="https://wlin-at.github.io/cad_vi">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yfzhang114/SliME.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.08487"><strong>Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-12</td>
<td style="text-align: center;"><a href="https://github.com/yfzhang114/SliME">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.07476"><strong>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-11</td>
<td style="text-align: center;"><a href="https://github.com/DAMO-NLP-SG/VideoLLaMA2">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/AIDC-AI/Parrot.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.02539"><strong>Parrot: Multilingual Visual Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-04</td>
<td style="text-align: center;"><a href="https://github.com/AIDC-AI/Parrot">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/AIDC-AI/Ovis.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2405.20797"><strong>Ovis: Structural Embedding Alignment for Multimodal Large Language Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-05-31</td>
<td style="text-align: center;"><a href="https://github.com/AIDC-AI/Ovis/">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/gordonhu608/MQT-LLaVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2405.19315"><strong>Matryoshka Query Transformer for Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-05-29</td>
<td style="text-align: center;"><a href="https://github.com/gordonhu608/MQT-LLaVA">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/gordonhu/MQT-LLaVA">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/alibaba/conv-llava.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2405.15738"><strong>ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-05-24</td>
<td style="text-align: center;"><a href="https://github.com/alibaba/conv-llava">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ByungKwanLee/Meteor.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2405.15574"><strong>Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-05-24</td>
<td style="text-align: center;"><a href="https://github.com/ByungKwanLee/Meteor">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/BK-Lee/Meteor">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/YifanXu74/Libra.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2405.10140"><strong>Libra: Building Decoupled Vision System on Large Language Models</strong></a> <br></td>
<td style="text-align: center;">ICML</td>
<td style="text-align: center;">2024-05-16</td>
<td style="text-align: center;"><a href="https://github.com/YifanXu74/Libra">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/SHI-Labs/CuMo.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2405.05949"><strong>CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-05-09</td>
<td style="text-align: center;"><a href="https://github.com/SHI-Labs/CuMo">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2404.16821"><strong>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-04-25</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/InternVL">Github</a></td>
<td style="text-align: center;"><a href="https://internvl.opengvlab.com">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/graphic-design-ai/graphist.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2404.14368"><strong>Graphic Design with Large Multimodal Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-04-22</td>
<td style="text-align: center;"><a href="https://github.com/graphic-design-ai/graphist">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2404.07204"><strong>BRAVE: Broadening the visual encoding of vision-language models</strong></a></td>
<td style="text-align: center;">ECCV</td>
<td style="text-align: center;">2024-04-10</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2404.06512.pdf"><strong>InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-04-09</td>
<td style="text-align: center;"><a href="https://github.com/InternLM/InternLM-XComposer">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/Willow123/InternLM-XComposer">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2404.05719.pdf"><strong>Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-04-08</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/boheumd/MA-LMM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2404.05726.pdf"><strong>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2024-04-08</td>
<td style="text-align: center;"><a href="https://github.com/boheumd/MA-LMM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/SkyworkAI/Vitron.svg?style=social&amp;label=Star" /> <br> <a href="https://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf"><strong>VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2024-04-04</td>
<td style="text-align: center;"><a href="https://github.com/SkyworkAI/Vitron">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://dl.acm.org/doi/pdf/10.1145/3654674"><strong>TOMGPT: Reliable Text-Only Training Approach for Cost-Effective Multi-modal Large Language Model</strong></a></td>
<td style="text-align: center;">ACM TKDD</td>
<td style="text-align: center;">2024-03-28</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/NVlabs/LITA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.19046"><strong>LITA: Language Instructed Temporal-Localization Assistant</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-27</td>
<td style="text-align: center;"><a href="https://github.com/NVlabs/LITA">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/dvlab-research/MiniGemini.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.18814.pdf"><strong>Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-27</td>
<td style="text-align: center;"><a href="https://github.com/dvlab-research/MiniGemini">Github</a></td>
<td style="text-align: center;"><a href="http://103.170.5.190:7860">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2403.09611.pdf"><strong>MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-14</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ByungKwanLee/MoAI.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.07508.pdf"><strong>MoAI: Mixture of All Intelligence for Large Language and Vision Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-12</td>
<td style="text-align: center;"><a href="https://github.com/ByungKwanLee/MoAI">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-VL.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.05525"><strong>DeepSeek-VL: Towards Real-World Vision-Language Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-08</td>
<td style="text-align: center;"><a href="https://github.com/deepseek-ai/DeepSeek-VL">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Yuliang-Liu/Monkey.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.04473.pdf"><strong>TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-07</td>
<td style="text-align: center;"><a href="https://github.com/Yuliang-Liu/Monkey">Github</a></td>
<td style="text-align: center;"><a href="http://vlrlab-monkey.xyz:7684">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/all-seeing.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.19474.pdf"><strong>The All-Seeing Project V2: Towards General Relation Comprehension of the Open World</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-29</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/all-seeing">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2402.16846.pdf"><strong>GROUNDHOG: Grounding Large Language Models to Holistic Segmentation</strong></a></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2024-02-26</td>
<td style="text-align: center;">Coming soon</td>
<td style="text-align: center;">Coming soon</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenMOSS/AnyGPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.12226.pdf"><strong>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-19</td>
<td style="text-align: center;"><a href="https://github.com/OpenMOSS/AnyGPT">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/DCDmllm/Momentor.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.11435.pdf"><strong>Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-18</td>
<td style="text-align: center;"><a href="https://github.com/DCDmllm/Momentor">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/FreedomIntelligence/ALLaVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.11684.pdf"><strong>ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-18</td>
<td style="text-align: center;"><a href="https://github.com/FreedomIntelligence/ALLaVA">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/FreedomIntelligence/ALLaVA-3B">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ByungKwanLee/CoLLaVO-Crayon-Large-Language-and-Vision-mOdel.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.11248.pdf"><strong>CoLLaVO: Crayon Large Language and Vision mOdel</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-17</td>
<td style="text-align: center;"><a href="https://github.com/ByungKwanLee/CoLLaVO-Crayon-Large-Language-and-Vision-mOdel">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/TRI-ML/prismatic-vlms.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.07865"><strong>Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models</strong></a> <br></td>
<td style="text-align: center;">ICML</td>
<td style="text-align: center;">2024-02-12</td>
<td style="text-align: center;"><a href="https://github.com/TRI-ML/prismatic-vlms">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/THUDM/CogCoM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.04236.pdf"><strong>CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-06</td>
<td style="text-align: center;"><a href="https://github.com/THUDM/CogCoM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Meituan-AutoML/MobileVLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.03766.pdf"><strong>MobileVLM V2: Faster and Stronger Baseline for Vision Language Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-06</td>
<td style="text-align: center;"><a href="https://github.com/Meituan-AutoML/MobileVLM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/WEIYanbin1999/GITA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.02130"><strong>GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2024-02-03</td>
<td style="text-align: center;"><a href="https://github.com/WEIYanbin1999/GITA/">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2401.17981.pdf"><strong>Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-01-31</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&amp;label=Star" /> <br> <a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/"><strong>LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</strong></a></td>
<td style="text-align: center;">Blog</td>
<td style="text-align: center;">2024-01-30</td>
<td style="text-align: center;"><a href="https://github.com/haotian-liu/LLaVA">Github</a></td>
<td style="text-align: center;"><a href="https://llava.hliu.cc">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/PKU-YuanGroup/MoE-LLaVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2401.15947.pdf"><strong>MoE-LLaVA: Mixture of Experts for Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-01-29</td>
<td style="text-align: center;"><a href="https://github.com/PKU-YuanGroup/MoE-LLaVA">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/LanguageBind/MoE-LLaVA">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2401.16420.pdf"><strong>InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-01-29</td>
<td style="text-align: center;"><a href="https://github.com/InternLM/InternLM-XComposer">Github</a></td>
<td style="text-align: center;"><a href="https://openxlab.org.cn/apps/detail/WillowBreeze/InternLM-XComposer">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/01-ai/Yi.svg?style=social&amp;label=Star" /> <br> <a href="https://github.com/01-ai/Yi/tree/main/VL"><strong>Yi-VL</strong></a> <br></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2024-01-23</td>
<td style="text-align: center;"><a href="https://github.com/01-ai/Yi/tree/main/VL">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2401.12168.pdf"><strong>SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-01-22</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/ChartAst.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2401.02384"><strong>ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">ACL</td>
<td style="text-align: center;">2024-01-04</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/ChartAst">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Meituan-AutoML/MobileVLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.16886.pdf"><strong>MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-28</td>
<td style="text-align: center;"><a href="https://github.com/Meituan-AutoML/MobileVLM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.14238.pdf"><strong>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/InternVL">Github</a></td>
<td style="text-align: center;"><a href="https://internvl.opengvlab.com">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/CircleRadon/Osprey.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.10032.pdf"><strong>Osprey: Pixel Understanding with Visual Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-12-15</td>
<td style="text-align: center;"><a href="https://github.com/CircleRadon/Osprey">Github</a></td>
<td style="text-align: center;"><a href="http://111.0.123.204:8000/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/THUDM/CogVLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.08914.pdf"><strong>CogAgent: A Visual Language Model for GUI Agents</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="https://github.com/THUDM/CogVLM">Github</a></td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2312.09237.pdf"><strong>Pixel Aligned Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-14</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/NVlabs/VILA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.07533"><strong>VILA: On Pre-training for Visual Language Models</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-12-13</td>
<td style="text-align: center;"><a href="https://github.com/NVlabs/VILA">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2312.08366.pdf"><strong>See, Say, and Segment: Teaching LMMs to Overcome False Premises</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-13</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Ucas-HaoranWei/Vary.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.06109.pdf"><strong>Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">ECCV</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://github.com/Ucas-HaoranWei/Vary">Github</a></td>
<td style="text-align: center;"><a href="http://region-31.seetacloud.com:22701/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/kakaobrain/honeybee.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.06742.pdf"><strong>Honeybee: Locality-enhanced Projector for Multimodal LLM</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-12-11</td>
<td style="text-align: center;"><a href="https://github.com/kakaobrain/honeybee">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"><strong>Gemini: A Family of Highly Capable Multimodal Models</strong></a></td>
<td style="text-align: center;">Google</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/csuhan/OneLLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.03700.pdf"><strong>OneLLM: One Framework to Align All Modalities with Language</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://github.com/csuhan/OneLLM">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/csuhan/OneLLM">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Meituan-AutoML/Lenna.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.02433.pdf"><strong>Lenna: Language Enhanced Reasoning Detection Assistant</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://github.com/Meituan-AutoML/Lenna">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2312.02310.pdf"><strong>VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/RenShuhuai-Andy/TimeChat.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.02051.pdf"><strong>TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://github.com/RenShuhuai-Andy/TimeChat">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mu-cai/vip-llava.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.00784.pdf"><strong>Making Large Multimodal Models Understand Arbitrary Visual Prompts</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://github.com/mu-cai/vip-llava">Github</a></td>
<td style="text-align: center;"><a href="https://pages.cs.wisc.edu/~mucai/vip-llava.html">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/vlm-driver/Dolphins.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.00438.pdf"><strong>Dolphins: Multimodal Language Model for Driving</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://github.com/vlm-driver/Dolphins">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Open3DA/LL3DA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.18651.pdf"><strong>LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://github.com/Open3DA/LL3DA">Github</a></td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/huangb23/VTimeLLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.18445.pdf"><strong>VTimeLLM: Empower LLM to Grasp Video Moments</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://github.com/huangb23/VTimeLLM/">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.18248.pdf"><strong>mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-30</td>
<td style="text-align: center;"><a href="https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.17043.pdf"><strong>LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://github.com/dvlab-research/LLaMA-VID">Github</a></td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/dvlab-research/LLMGA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.16500.pdf"><strong>LLMGA: Multimodal Large Language Model based Generation Assistant</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://github.com/dvlab-research/LLMGA">Github</a></td>
<td style="text-align: center;"><a href="https://baa55ef8590b623f18.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/tingxueronghua/ChartLlama-code.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.16483.pdf"><strong>ChartLlama: A Multimodal LLM for Chart Understanding and Generation</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://github.com/tingxueronghua/ChartLlama-code">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.12793.pdf"><strong>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-21</td>
<td style="text-align: center;"><a href="https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/Lin-Chen/ShareGPT4V-7B">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/rshaojimmy/JiuTian.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.11860.pdf"><strong>LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-20</td>
<td style="text-align: center;"><a href="https://github.com/rshaojimmy/JiuTian">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/embodied-generalist/embodied-generalist.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.12871.pdf"><strong>An Embodied Generalist Agent in 3D World</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-18</td>
<td style="text-align: center;"><a href="https://github.com/embodied-generalist/embodied-generalist">Github</a></td>
<td style="text-align: center;"><a href="https://www.youtube.com/watch?v=mlnjz4eSjB4">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.10122.pdf"><strong>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-16</td>
<td style="text-align: center;"><a href="https://github.com/PKU-YuanGroup/Video-LLaVA">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/LanguageBind/Video-LLaVA">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.08046"><strong>Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-11-14</td>
<td style="text-align: center;"><a href="https://github.com/PKU-YuanGroup/Chat-UniVi">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/X2FD/LVIS-INSTRUCT4V.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.07574.pdf"><strong>To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-13</td>
<td style="text-align: center;"><a href="https://github.com/X2FD/LVIS-INSTRUCT4V">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Alpha-VLLM/LLaMA2-Accessory.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.07575.pdf"><strong>SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-13</td>
<td style="text-align: center;"><a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory">Github</a></td>
<td style="text-align: center;"><a href="http://imagebind-llm.opengvlab.com/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Yuliang-Liu/Monkey.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.06607.pdf"><strong>Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-11-11</td>
<td style="text-align: center;"><a href="https://github.com/Yuliang-Liu/Monkey">Github</a></td>
<td style="text-align: center;"><a href="http://27.17.184.224:7681/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-Plus-Codebase.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.05437.pdf"><strong>LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-09</td>
<td style="text-align: center;"><a href="https://github.com/LLaVA-VL/LLaVA-Plus-Codebase">Github</a></td>
<td style="text-align: center;"><a href="https://llavaplus.ngrok.io/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/NExT-ChatV/NExT-Chat.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.04498.pdf"><strong>NExT-Chat: An LMM for Chat, Detection and Segmentation</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-08</td>
<td style="text-align: center;"><a href="https://github.com/NExT-ChatV/NExT-Chat">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.04257.pdf"><strong>mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-07</td>
<td style="text-align: center;"><a href="https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2">Github</a></td>
<td style="text-align: center;"><a href="https://modelscope.cn/studios/damo/mPLUG-Owl2/summary">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Luodian/Otter.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.04219.pdf"><strong>OtterHD: A High-Resolution Multi-modality Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-07</td>
<td style="text-align: center;"><a href="https://github.com/Luodian/Otter">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2311.03354.pdf"><strong>CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-06</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mbzuai-oryx/groundingLMM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.03356.pdf"><strong>GLaMM: Pixel Grounding Large Multimodal Model</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-11-06</td>
<td style="text-align: center;"><a href="https://github.com/mbzuai-oryx/groundingLMM">Github</a></td>
<td style="text-align: center;"><a href="https://glamm.mbzuai-oryx.ngrok.app/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/RUCAIBox/ComVint.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.01487.pdf"><strong>What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-02</td>
<td style="text-align: center;"><a href="https://github.com/RUCAIBox/ComVint">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.09478.pdf"><strong>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-14</td>
<td style="text-align: center;"><a href="https://github.com/Vision-CAIR/MiniGPT-4">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/bytedance/SALMONN.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.13289"><strong>SALMONN: Towards Generic Hearing Abilities for Large Language Models</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-10-20</td>
<td style="text-align: center;"><a href="https://github.com/bytedance/SALMONN">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/apple/ml-ferret.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.07704.pdf"><strong>Ferret: Refer and Ground Anything Anywhere at Any Granularity</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-11</td>
<td style="text-align: center;"><a href="https://github.com/apple/ml-ferret">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/THUDM/CogVLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.03079.pdf"><strong>CogVLM: Visual Expert For Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-09</td>
<td style="text-align: center;"><a href="https://github.com/THUDM/CogVLM">Github</a></td>
<td style="text-align: center;"><a href="http://36.103.203.44:7861/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.03744.pdf"><strong>Improved Baselines with Visual Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-05</td>
<td style="text-align: center;"><a href="https://github.com/haotian-liu/LLaVA">Github</a></td>
<td style="text-align: center;"><a href="https://llava.hliu.cc/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/PKU-YuanGroup/LanguageBind.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.01852.pdf"><strong>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-10-03</td>
<td style="text-align: center;"><a href="https://github.com/PKU-YuanGroup/LanguageBind">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/LanguageBind/LanguageBind">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/SY-Xuan/Pink.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.00582.pdf"><strong>Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://github.com/SY-Xuan/Pink">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/thunlp/Muffin.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.00653.pdf"><strong>Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://github.com/thunlp/Muffin">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2309.16058.pdf"><strong>AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-27</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.15112.pdf"><strong>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-26</td>
<td style="text-align: center;"><a href="https://github.com/InternLM/InternLM-XComposer">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/RunpeiDong/DreamLLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.11499.pdf"><strong>DreamLLM: Synergistic Multimodal Comprehension and Creation</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-09-20</td>
<td style="text-align: center;"><a href="https://github.com/RunpeiDong/DreamLLM">Github</a></td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2309.09958.pdf"><strong>An Empirical Study of Scaling Instruction-Tuned Large Multimodal Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-18</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/SihengLi99/TextBind.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.08637.pdf"><strong>TextBind: Multi-turn Interleaved Multimodal Instruction-following</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-14</td>
<td style="text-align: center;"><a href="https://github.com/SihengLi99/TextBind">Github</a></td>
<td style="text-align: center;"><a href="https://ailabnlp.tencent.com/research_demos/textbind/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.05519.pdf"><strong>NExT-GPT: Any-to-Any Multimodal LLM</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-11</td>
<td style="text-align: center;"><a href="https://github.com/NExT-GPT/NExT-GPT">Github</a></td>
<td style="text-align: center;"><a href="https://fc7a82a1c76b336b6f.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/UCSC-VLAA/Sight-Beyond-Text.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.07120.pdf"><strong>Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-13</td>
<td style="text-align: center;"><a href="https://github.com/UCSC-VLAA/Sight-Beyond-Text">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.03905.pdf"><strong>ImageBind-LLM: Multi-modality Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-07</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/LLaMA-Adapter">Github</a></td>
<td style="text-align: center;"><a href="http://imagebind-llm.opengvlab.com/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2309.02591.pdf"><strong>Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-05</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenRobotLab/PointLLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.16911.pdf"><strong>PointLLM: Empowering Large Language Models to Understand Point Clouds</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://github.com/OpenRobotLab/PointLLM">Github</a></td>
<td style="text-align: center;"><a href="http://101.230.144.196/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/HYPJUDY/Sparkles.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.16463.pdf"><strong>Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://github.com/HYPJUDY/Sparkles">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/opendatalab/MLLM-DataEngine.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.13566.pdf"><strong>MLLM-DataEngine: An Iterative Refinement Approach for MLLM</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-25</td>
<td style="text-align: center;"><a href="https://github.com/opendatalab/MLLM-DataEngine">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/PVIT-official/PVIT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.13437.pdf"><strong>Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-25</td>
<td style="text-align: center;"><a href="https://github.com/PVIT-official/PVIT">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/PVIT/pvit">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/QwenLM/Qwen-VL.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.12966.pdf"><strong>Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-24</td>
<td style="text-align: center;"><a href="https://github.com/QwenLM/Qwen-VL">Github</a></td>
<td style="text-align: center;"><a href="https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenBMB/VisCPM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.12038.pdf"><strong>Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-08-23</td>
<td style="text-align: center;"><a href="https://github.com/OpenBMB/VisCPM">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/openbmb/viscpm-chat">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/icoz69/StableLLAVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.10253.pdf"><strong>StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-20</td>
<td style="text-align: center;"><a href="https://github.com/icoz69/StableLLAVA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mlpc-ucsd/BLIVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.09936.pdf"><strong>BLIVA: A Simple Multimodal LLM for Better Handling of Text-rich Visual Questions</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-19</td>
<td style="text-align: center;"><a href="https://github.com/mlpc-ucsd/BLIVA">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/mlpc-lab/BLIVA">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/DCDmllm/Cheetah.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.04152.pdf"><strong>Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-08</td>
<td style="text-align: center;"><a href="https://github.com/DCDmllm/Cheetah">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/All-Seeing.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.01907.pdf"><strong>The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-08-03</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/All-Seeing">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/OpenGVLab/all-seeing">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/dvlab-research/LISA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.00692.pdf"><strong>LISA: Reasoning Segmentation via Large Language Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-01</td>
<td style="text-align: center;"><a href="https://github.com/dvlab-research/LISA">Github</a></td>
<td style="text-align: center;"><a href="http://103.170.5.190:7860">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/rese1f/MovieChat.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.16449.pdf"><strong>MovieChat: From Dense Token to Sparse Memory for Long Video Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-31</td>
<td style="text-align: center;"><a href="https://github.com/rese1f/MovieChat">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/UMass-Foundation-Model/3D-LLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.12981.pdf"><strong>3D-LLM: Injecting the 3D World into Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-24</td>
<td style="text-align: center;"><a href="https://github.com/UMass-Foundation-Model/3D-LLM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2307.09474.pdf"><strong>ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-18</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><a href="https://chatspot.streamlit.app/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/magic-research/bubogpt.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.08581.pdf"><strong>BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-17</td>
<td style="text-align: center;"><a href="https://github.com/magic-research/bubogpt">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/magicr/BuboGPT">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/BAAI-DCAI/Visual-Instruction-Tuning.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.04087.pdf"><strong>SVIT: Scaling up Visual Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-09</td>
<td style="text-align: center;"><a href="https://github.com/BAAI-DCAI/Visual-Instruction-Tuning">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/jshilong/GPT4RoI.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.03601.pdf"><strong>GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-07</td>
<td style="text-align: center;"><a href="https://github.com/jshilong/GPT4RoI">Github</a></td>
<td style="text-align: center;"><a href="http://139.196.83.164:7000/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/bytedance/lynx-llm.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.02469.pdf"><strong>What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-05</td>
<td style="text-align: center;"><a href="https://github.com/bytedance/lynx-llm">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.02499.pdf"><strong>mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-04</td>
<td style="text-align: center;"><a href="https://github.com/X-PLUG/mPLUG-DocOwl">Github</a></td>
<td style="text-align: center;"><a href="https://modelscope.cn/studios/damo/mPLUG-DocOwl/summary">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ChenDelong1999/polite_flamingo.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.01003.pdf"><strong>Visual Instruction Tuning with Polite Flamingo</strong></a> <br ></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-03</td>
<td style="text-align: center;"><a href="https://github.com/ChenDelong1999/polite_flamingo">Github</a></td>
<td style="text-align: center;"><a href="http://clever_flamingo.xiaoice.com/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/SALT-NLP/LLaVAR.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.17107.pdf"><strong>LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-29</td>
<td style="text-align: center;"><a href="https://github.com/SALT-NLP/LLaVAR">Github</a></td>
<td style="text-align: center;"><a href="https://eba470c07c805702b8.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/shikras/shikra.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.15195.pdf"><strong>Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-27</td>
<td style="text-align: center;"><a href="https://github.com/shikras/shikra">Github</a></td>
<td style="text-align: center;"><a href="http://demo.zhaozhang.net:7860/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenMotionLab/MotionGPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.14795.pdf"><strong>MotionGPT: Human Motion as a Foreign Language</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-26</td>
<td style="text-align: center;"><a href="https://github.com/OpenMotionLab/MotionGPT">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.09093.pdf"><strong>Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-15</td>
<td style="text-align: center;"><a href="https://github.com/lyuchenyang/Macaw-LLM">Github</a></td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenLAMM/LAMM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.06687.pdf"><strong>LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-11</td>
<td style="text-align: center;"><a href="https://github.com/OpenLAMM/LAMM">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/openlamm/LAMM">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.05424.pdf"><strong>Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://github.com/mbzuai-oryx/Video-ChatGPT">Github</a></td>
<td style="text-align: center;"><a href="https://www.ival-mbzuai.com/video-chatgpt">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Luodian/Otter.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.05425.pdf"><strong>MIMIC-IT: Multi-Modal In-Context Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://github.com/Luodian/Otter">Github</a></td>
<td style="text-align: center;"><a href="https://otter.cliangyu.com/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2306.04387.pdf"><strong>M<sup>3</sup>IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-07</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.02858.pdf"><strong>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-05</td>
<td style="text-align: center;"><a href="https://github.com/DAMO-NLP-SG/Video-LLaMA">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/LLaVA-Med.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.00890.pdf"><strong>LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/LLaVA-Med">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/StevenGrove/GPT4Tools.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.18752.pdf"><strong>GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-30</td>
<td style="text-align: center;"><a href="https://github.com/StevenGrove/GPT4Tools">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/stevengrove/GPT4Tools">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yxuansu/PandaGPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.16355.pdf"><strong>PandaGPT: One Model To Instruction-Follow Them All</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-25</td>
<td style="text-align: center;"><a href="https://github.com/yxuansu/PandaGPT">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/GMFTBY/PandaGPT">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/joez17/ChatBridge.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.16103.pdf"><strong>ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-25</td>
<td style="text-align: center;"><a href="https://github.com/joez17/ChatBridge">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/luogen1996/LaVIN.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.15023.pdf"><strong>Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-24</td>
<td style="text-align: center;"><a href="https://github.com/luogen1996/LaVIN">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OptimalScale/DetGPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.14167.pdf"><strong>DetGPT: Detect What You Need via Reasoning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-23</td>
<td style="text-align: center;"><a href="https://github.com/OptimalScale/DetGPT">Github</a></td>
<td style="text-align: center;"><a href="https://d3c431c0c77b1d9010.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/Pengi.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.11834.pdf"><strong>Pengi: An Audio Language Model for Audio Tasks</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2023-05-19</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/Pengi">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/VisionLLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.11175.pdf"><strong>VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-18</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/VisionLLM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/YuanGongND/ltu.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.10790.pdf"><strong>Listen, Think, and Understand</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-18</td>
<td style="text-align: center;"><a href="https://github.com/YuanGongND/ltu">Github</a></td>
<td style="text-align: center;"><a href="https://github.com/YuanGongND/ltu">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/THUDM/VisualGLM-6B.svg?style=social&amp;label=Star" /> <br> <strong>VisualGLM-6B</strong> <br></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2023-05-17</td>
<td style="text-align: center;"><a href="https://github.com/THUDM/VisualGLM-6B">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/xiaoman-zhang/PMC-VQA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.10415.pdf"><strong>PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-17</td>
<td style="text-align: center;"><a href="https://github.com/xiaoman-zhang/PMC-VQA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.06500.pdf"><strong>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-11</td>
<td style="text-align: center;"><a href="https://github.com/salesforce/LAVIS/tree/main/projects/instructblip">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.06355.pdf"><strong>VideoChat: Chat-Centric Video Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-10</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/Ask-Anything">Github</a></td>
<td style="text-align: center;"><a href="https://ask.opengvlab.com/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/open-mmlab/Multimodal-GPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.04790.pdf"><strong>MultiModal-GPT: A Vision and Language Model for Dialogue with Humans</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-08</td>
<td style="text-align: center;"><a href="https://github.com/open-mmlab/Multimodal-GPT">Github</a></td>
<td style="text-align: center;"><a href="https://mmgpt.openmmlab.org.cn/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/phellonchen/X-LLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.04160.pdf"><strong>X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-07</td>
<td style="text-align: center;"><a href="https://github.com/phellonchen/X-LLM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/YunxinLi/LingCloud.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.03701.pdf"><strong>LMEye: An Interactive Perception Network for Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-05</td>
<td style="text-align: center;"><a href="https://github.com/YunxinLi/LingCloud">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2304.15010.pdf"><strong>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-04-28</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/LLaMA-Adapter">Github</a></td>
<td style="text-align: center;"><a href="http://llama-adapter.opengvlab.com/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2304.14178.pdf"><strong>mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-04-27</td>
<td style="text-align: center;"><a href="https://github.com/X-PLUG/mPLUG-Owl">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/MAGAer13/mPLUG-Owl">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2304.10592.pdf"><strong>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-04-20</td>
<td style="text-align: center;"><a href="https://github.com/Vision-CAIR/MiniGPT-4">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2304.08485.pdf"><strong>Visual Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://github.com/haotian-liu/LLaVA">GitHub</a></td>
<td style="text-align: center;"><a href="https://llava.hliu.cc/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.16199.pdf"><strong>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/LLaMA-Adapter">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/csuhan/LLaMA-Adapter">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/VT-NLP/MultiInstruct.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2212.10773.pdf"><strong>MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">ACL</td>
<td style="text-align: center;">2022-12-21</td>
<td style="text-align: center;"><a href="https://github.com/VT-NLP/MultiInstruct">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<h2 id="multimodal-hallucination">Multimodal Hallucination</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: center;">Venue</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Demo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/1zhou-Wang/MemVR.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2410.03577"><strong>Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-10-04</td>
<td style="text-align: center;"><a href="https://github.com/1zhou-Wang/MemVR">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/nickjiang2378/vl-interp.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2410.02762"><strong>Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-10-03</td>
<td style="text-align: center;"><a href="https://github.com/nickjiang2378/vl-interp/">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2409.13612"><strong>FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-09-20</td>
<td style="text-align: center;"><a href="https://anonymous.4open.science/r/FIHA-45BB">Link</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2408.00555"><strong>Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-01</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/LALBJ/PAI.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2407.21771"><strong>Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs</strong></a> <br></td>
<td style="text-align: center;">ECCV</td>
<td style="text-align: center;">2024-07-31</td>
<td style="text-align: center;"><a href="https://github.com/LALBJ/PAI">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mrwu-mac/R-Bench.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.16449"><strong>Evaluating and Analyzing Relationship Hallucinations in LVLMs</strong></a> <br></td>
<td style="text-align: center;">ICML</td>
<td style="text-align: center;">2024-06-24</td>
<td style="text-align: center;"><a href="https://github.com/mrwu-mac/R-Bench">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Lackel/AGLA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.12718"><strong>AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-18</td>
<td style="text-align: center;"><a href="https://github.com/Lackel/AGLA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2406.01920"><strong>CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-04</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2405.18654"><strong>Mitigating Object Hallucination via Data Augmented Contrastive Tuning</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-05-28</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2405.15683"><strong>VDGD: Mitigating LVLM Hallucinations in Cognitive Prompts by Bridging the Visual Perception Gap</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-05-24</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2404.14233.pdf"><strong>Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-04-22</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2403.18715.pdf"><strong>Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-27</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/IVY-LVLM/Counterfactual-Inception.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.13513.pdf"><strong>What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-20</td>
<td style="text-align: center;"><a href="https://github.com/IVY-LVLM/Counterfactual-Inception">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2403.08730.pdf"><strong>Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yfzhang114/LLaVA-Align.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.05262"><strong>Debiasing Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-08</td>
<td style="text-align: center;"><a href="https://github.com/yfzhang114/LLaVA-Align">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/BillChan226/HALC.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.00425.pdf"><strong>HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-01</td>
<td style="text-align: center;"><a href="https://github.com/BillChan226/HALC">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2402.18476.pdf"><strong>IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-28</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yuezih/less-is-more.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.14545.pdf"><strong>Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-22</td>
<td style="text-align: center;"><a href="https://github.com/yuezih/less-is-more">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Hyperwjf/LogicCheckGPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.11622.pdf"><strong>Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-18</td>
<td style="text-align: center;"><a href="https://github.com/Hyperwjf/LogicCheckGPT">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/MasaiahHan/CorrelationQA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.03757.pdf"><strong>The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-06</td>
<td style="text-align: center;"><a href="https://github.com/MasaiahHan/CorrelationQA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenKG-ORG/EasyDetect.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.03190.pdf"><strong>Unified Hallucination Detection for Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-05</td>
<td style="text-align: center;"><a href="https://github.com/OpenKG-ORG/EasyDetect">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2402.00253.pdf"><strong>A Survey on Hallucination in Large Vision-Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-01</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2401.09861.pdf"><strong>Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-01-18</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/X-PLUG/mPLUG-HalOwl.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.06968.pdf"><strong>Hallucination Augmented Contrastive Learning for Multimodal Large Language Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-12</td>
<td style="text-align: center;"><a href="https://github.com/X-PLUG/mPLUG-HalOwl/tree/main/hacl">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/assafbk/mocha_code.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.03631.pdf"><strong>MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;"><a href="https://github.com/assafbk/mocha_code">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Anonymousanoy/FOHE.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.01701.pdf"><strong>Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-04</td>
<td style="text-align: center;"><a href="https://github.com/Anonymousanoy/FOHE">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/RLHF-V/RLHF-V.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.00849.pdf"><strong>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://github.com/RLHF-V/RLHF-V">Github</a></td>
<td style="text-align: center;"><a href="http://120.92.209.146:8081/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/shikiw/OPERA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.17911.pdf"><strong>OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-11-29</td>
<td style="text-align: center;"><a href="https://github.com/shikiw/OPERA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/DAMO-NLP-SG/VCD.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.16922.pdf"><strong>Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://github.com/DAMO-NLP-SG/VCD">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2311.16839.pdf"><strong>Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-28</td>
<td style="text-align: center;"><a href="https://github.com/opendatalab/HA-DPO">Github</a></td>
<td style="text-align: center;"><a href="">Comins Soon</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2311.16479.pdf"><strong>Mitigating Hallucination in Visual Language Models with Visual Supervision</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Yuqifan1117/HalluciDoctor.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.13614.pdf"><strong>HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-22</td>
<td style="text-align: center;"><a href="https://github.com/Yuqifan1117/HalluciDoctor">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/junyangwang0410/AMBER.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.07397.pdf"><strong>An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-13</td>
<td style="text-align: center;"><a href="https://github.com/junyangwang0410/AMBER">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/bcdnlp/FAITHSCORE.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.01477.pdf"><strong>FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-02</td>
<td style="text-align: center;"><a href="https://github.com/bcdnlp/FAITHSCORE">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/BradyFU/Woodpecker.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.16045.pdf"><strong>Woodpecker: Hallucination Correction for Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-24</td>
<td style="text-align: center;"><a href="https://github.com/BradyFU/Woodpecker">Github</a></td>
<td style="text-align: center;"><a href="https://deb6a97bae6fab67ae.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2310.05338.pdf"><strong>Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-09</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/bronyayang/HallE_Switch.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.01779.pdf"><strong>HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-03</td>
<td style="text-align: center;"><a href="https://github.com/bronyayang/HallE_Switch">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/YiyangZhou/LURE.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.00754.pdf"><strong>Analyzing and Mitigating Object Hallucination in Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://github.com/YiyangZhou/LURE">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/llava-rlhf/LLaVA-RLHF.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.14525.pdf"><strong>Aligning Large Multimodal Models with Factually Augmented RLHF</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-25</td>
<td style="text-align: center;"><a href="https://github.com/llava-rlhf/LLaVA-RLHF">Github</a></td>
<td style="text-align: center;"><a href="http://pitt.lti.cs.cmu.edu:7890/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2309.04041.pdf"><strong>Evaluation and Mitigation of Agnosia in Multimodal Large Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-07</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2309.02301.pdf"><strong>CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-05</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/junyangwang0410/HaELM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.15126.pdf"><strong>Evaluation and Analysis of Hallucination in Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-29</td>
<td style="text-align: center;"><a href="https://github.com/junyangwang0410/HaELM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/opendatalab/VIGC.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.12714.pdf"><strong>VIGC: Visual Instruction Generation and Correction</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-24</td>
<td style="text-align: center;"><a href="https://github.com/opendatalab/VIGC">Github</a></td>
<td style="text-align: center;"><a href="https://opendatalab.github.io/VIGC">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2308.06394.pdf"><strong>Detecting and Preventing Hallucinations in Large Vision Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-11</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/FuxiaoLiu/LRV-Instruction.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.14565.pdf"><strong>Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-06-26</td>
<td style="text-align: center;"><a href="https://github.com/FuxiaoLiu/LRV-Instruction">Github</a></td>
<td style="text-align: center;"><a href="https://7b6590ed039a06475d.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/RUCAIBox/POPE.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.10355.pdf"><strong>Evaluating Object Hallucination in Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">EMNLP</td>
<td style="text-align: center;">2023-05-17</td>
<td style="text-align: center;"><a href="https://github.com/RUCAIBox/POPE">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<h2 id="multimodal-in-context-learning">Multimodal In-Context Learning</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: center;">Venue</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Demo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2402.11574.pdf"><strong>Visual In-Context Learning for Large Vision-Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-18</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/YuanJianhao508/RAG-Driver.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/abs/2402.10828"><strong>RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model</strong></a> <br></td>
<td style="text-align: center;">RSS</td>
<td style="text-align: center;">2024-02-16</td>
<td style="text-align: center;"><a href="https://github.com/YuanJianhao508/RAG-Driver">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/UW-Madison-Lee-Lab/CoBSAT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.01293.pdf"><strong>Can MLLMs Perform Text-to-Image In-Context Learning?</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-02</td>
<td style="text-align: center;"><a href="https://github.com/UW-Madison-Lee-Lab/CoBSAT">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.13286"><strong>Generative Multimodal Models are In-Context Learners</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-12-20</td>
<td style="text-align: center;"><a href="https://github.com/baaivision/Emu/tree/main/Emu2">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/BAAI/Emu2">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2312.07553.pdf"><strong>Hijacking Context in Large Multi-modal Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2312.02520.pdf"><strong>Towards More Unified In-context Visual Understanding</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/HaozheZhao/MIC.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.07915.pdf"><strong>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-14</td>
<td style="text-align: center;"><a href="https://github.com/HaozheZhao/MIC">Github</a></td>
<td style="text-align: center;"><a href="https://8904cdd23621858859.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/isekai-portal/Link-Context-Learning.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.07891.pdf"><strong>Link-Context Learning for Multimodal LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-15</td>
<td style="text-align: center;"><a href="https://github.com/isekai-portal/Link-Context-Learning">Github</a></td>
<td style="text-align: center;"><a href="http://117.144.81.99:20488/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mlfoundations/open_flamingo.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.01390.pdf"><strong>OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-02</td>
<td style="text-align: center;"><a href="https://github.com/mlfoundations/open_flamingo">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/openflamingo/OpenFlamingo">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/snap-stanford/med-flamingo.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.15189.pdf"><strong>Med-Flamingo: a Multimodal Medical Few-shot Learner</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-27</td>
<td style="text-align: center;"><a href="https://github.com/snap-stanford/med-flamingo">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.05222.pdf"><strong>Generative Pretraining in Multimodality</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-07-11</td>
<td style="text-align: center;"><a href="https://github.com/baaivision/Emu/tree/main/Emu1">Github</a></td>
<td style="text-align: center;"><a href="http://218.91.113.230:9002/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2306.08129.pdf"><strong>AVIS: Autonomous Visual Information Seeking with Large Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Luodian/Otter.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.05425.pdf"><strong>MIMIC-IT: Multi-Modal In-Context Instruction Tuning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://github.com/Luodian/Otter">Github</a></td>
<td style="text-align: center;"><a href="https://otter.cliangyu.com/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yongliang-wu/ExploreCfg.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.14800.pdf"><strong>Exploring Diverse In-Context Configurations for Image Captioning</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2023-05-24</td>
<td style="text-align: center;"><a href="https://github.com/yongliang-wu/ExploreCfg">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/lupantech/chameleon-llm.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2304.09842.pdf"><strong>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://github.com/lupantech/chameleon-llm">Github</a></td>
<td style="text-align: center;"><a href="https://chameleon-llm.github.io/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.17580.pdf"><strong>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/JARVIS">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/microsoft/HuggingGPT">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/MM-REACT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.11381.pdf"><strong>MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/MM-REACT">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/microsoft-cognitive-service/mm-react">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/MAEHCM/ICL-D3IE.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.05063.pdf"><strong>ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction</strong></a> <br></td>
<td style="text-align: center;">ICCV</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://github.com/MAEHCM/ICL-D3IE">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/MILVLG/prophet.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.01903.pdf"><strong>Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://github.com/MILVLG/prophet">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/allenai/visprog.svg?style=social&amp;label=Star" /> <br> <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf"><strong>Visual Programming: Compositional visual reasoning without training</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://github.com/allenai/visprog">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/PICa.svg?style=social&amp;label=Star" /> <br> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/20215/19974"><strong>An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</strong></a> <br></td>
<td style="text-align: center;">AAAI</td>
<td style="text-align: center;">2022-06-28</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/PICa">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mlfoundations/open_flamingo.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2204.14198.pdf"><strong>Flamingo: a Visual Language Model for Few-Shot Learning</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2022-04-29</td>
<td style="text-align: center;"><a href="https://github.com/mlfoundations/open_flamingo">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/dhansmair/flamingo-mini-cap">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2106.13884.pdf"><strong>Multimodal Few-Shot Learning with Frozen Language Models</strong></a></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2021-06-25</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<h2 id="multimodal-chain-of-thought">Multimodal Chain-of-Thought</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: center;">Venue</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Demo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/dongyh20/Insight-V.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2411.14432"><strong>Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-11-21</td>
<td style="text-align: center;"><a href="https://github.com/dongyh20/Insight-V">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ggg0919/cantor.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2404.16033.pdf"><strong>Cantor: Inspiring Multimodal Chain-of-Thought of MLLM</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-04-24</td>
<td style="text-align: center;"><a href="https://github.com/ggg0919/cantor">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/deepcs233/Visual-CoT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.16999.pdf"><strong>Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-25</td>
<td style="text-align: center;"><a href="https://github.com/deepcs233/Visual-CoT">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/chancharikmitra/CCoT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.17076"><strong>Compositional Chain-of-Thought Prompting for Large Multimodal Models</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://github.com/chancharikmitra/CCoT">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/SooLab/DDCOT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.16436.pdf"><strong>DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2023-10-25</td>
<td style="text-align: center;"><a href="https://github.com/SooLab/DDCOT">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/shikras/shikra.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.15195.pdf"><strong>Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-27</td>
<td style="text-align: center;"><a href="https://github.com/shikras/shikra">Github</a></td>
<td style="text-align: center;"><a href="http://demo.zhaozhang.net:7860/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/zeroQiaoba/Explainable-Multimodal-Emotion-Reasoning.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.15401.pdf"><strong>Explainable Multimodal Emotion Reasoning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-27</td>
<td style="text-align: center;"><a href="https://github.com/zeroQiaoba/Explainable-Multimodal-Emotion-Reasoning">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/EmbodiedGPT/EmbodiedGPT_Pytorch.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.15021.pdf"><strong>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-24</td>
<td style="text-align: center;"><a href="https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2305.13903.pdf"><strong>Lets Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-23</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2305.03453.pdf"><strong>T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-05</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ttengwang/Caption-Anything.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.02677.pdf"><strong>Caption Anything: Interactive Image Description with Diverse Multimodal Controls</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-04</td>
<td style="text-align: center;"><a href="https://github.com/ttengwang/Caption-Anything">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/TencentARC/Caption-Anything">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2305.02317.pdf"><strong>Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-03</td>
<td style="text-align: center;"><a href="https://github.com/dannyrose30/VCOT">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/lupantech/chameleon-llm.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2304.09842.pdf"><strong>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://github.com/lupantech/chameleon-llm">Github</a></td>
<td style="text-align: center;"><a href="https://chameleon-llm.github.io/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2304.07919.pdf"><strong>Chain of Thought Prompt Tuning in Vision Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-04-16</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/MM-REACT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.11381.pdf"><strong>MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/MM-REACT">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/microsoft-cognitive-service/mm-react">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/TaskMatrix.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.04671.pdf"><strong>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-08</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/TaskMatrix">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/microsoft/visual_chatgpt">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/amazon-science/mm-cot.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2302.00923.pdf"><strong>Multimodal Chain-of-Thought Reasoning in Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-02-02</td>
<td style="text-align: center;"><a href="https://github.com/amazon-science/mm-cot">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/allenai/visprog.svg?style=social&amp;label=Star" /> <br> <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf"><strong>Visual Programming: Compositional visual reasoning without training</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://github.com/allenai/visprog">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/lupantech/ScienceQA.svg?style=social&amp;label=Star" /> <br> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf"><strong>Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2022-09-20</td>
<td style="text-align: center;"><a href="https://github.com/lupantech/ScienceQA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<h2 id="llm-aided-visual-reasoning">LLM-Aided Visual Reasoning</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: center;">Venue</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Demo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/LaVi-Lab/Visual-Table.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2403.18252.pdf"><strong>Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-03-27</td>
<td style="text-align: center;"><a href="https://github.com/LaVi-Lab/Visual-Table">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/penghao-wu/vstar.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.14135.pdf"><strong>V: Guided Visual Search as a Core Mechanism in Multimodal LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://github.com/penghao-wu/vstar">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-Interactive-Demo.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.00571.pdf"><strong>LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-01</td>
<td style="text-align: center;"><a href="https://github.com/LLaVA-VL/LLaVA-Interactive-Demo">Github</a></td>
<td style="text-align: center;"><a href="https://6dd3-20-163-117-69.ngrok-free.app/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2310.19773.pdf"><strong>MM-VID: Advancing Video Understanding with GPT-4V(vision)</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-30</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/ControlLLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.17796.pdf"><strong>ControlLLM: Augment Language Models with Tools by Searching on Graphs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-26</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/ControlLLM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/BradyFU/Woodpecker.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.16045.pdf"><strong>Woodpecker: Hallucination Correction for Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-24</td>
<td style="text-align: center;"><a href="https://github.com/BradyFU/Woodpecker">Github</a></td>
<td style="text-align: center;"><a href="https://deb6a97bae6fab67ae.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mindagent/mindagent.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.09971.pdf"><strong>MindAgent: Emergent Gaming Interaction</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-18</td>
<td style="text-align: center;"><a href="https://github.com/mindagent/mindagent">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ContextualAI/lens.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.16410.pdf"><strong>Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-28</td>
<td style="text-align: center;"><a href="https://github.com/ContextualAI/lens">Github</a></td>
<td style="text-align: center;"><a href="https://lens.contextual.ai/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2306.11732.pdf"><strong>Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-15</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/showlab/assistgpt.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.08640.pdf"><strong>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-14</td>
<td style="text-align: center;"><a href="https://github.com/showlab/assistgpt">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2306.08129.pdf"><strong>AVIS: Autonomous Visual Information Seeking with Large Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/StevenGrove/GPT4Tools.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.18752.pdf"><strong>GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-30</td>
<td style="text-align: center;"><a href="https://github.com/StevenGrove/GPT4Tools">Github</a></td>
<td style="text-align: center;"><a href="https://c60eb7e9400930f31b.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2305.17066.pdf"><strong>Mindstorms in Natural Language-Based Societies of Mind</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-26</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/weixi-feng/LayoutGPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.15393.pdf"><strong>LayoutGPT: Compositional Visual Planning and Generation with Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-24</td>
<td style="text-align: center;"><a href="https://github.com/weixi-feng/LayoutGPT">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Hxyou/IdealGPT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.14985.pdf"><strong>IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-24</td>
<td style="text-align: center;"><a href="https://github.com/Hxyou/IdealGPT">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/matrix-alpha/Accountable-Textual-Visual-Chat.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.05983.pdf"><strong>Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-10</td>
<td style="text-align: center;"><a href="https://github.com/matrix-alpha/Accountable-Textual-Visual-Chat">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ttengwang/Caption-Anything.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.02677.pdf"><strong>Caption Anything: Interactive Image Description with Diverse Multimodal Controls</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-04</td>
<td style="text-align: center;"><a href="https://github.com/ttengwang/Caption-Anything">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/TencentARC/Caption-Anything">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/lupantech/chameleon-llm.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2304.09842.pdf"><strong>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://github.com/lupantech/chameleon-llm">Github</a></td>
<td style="text-align: center;"><a href="https://chameleon-llm.github.io/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.17580.pdf"><strong>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/JARVIS">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/microsoft/HuggingGPT">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/MM-REACT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.11381.pdf"><strong>MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/MM-REACT">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/microsoft-cognitive-service/mm-react">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/cvlab-columbia/viper.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.08128.pdf"><strong>ViperGPT: Visual Inference via Python Execution for Reasoning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://github.com/cvlab-columbia/viper">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Vision-CAIR/ChatCaptioner.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.06594.pdf"><strong>ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://github.com/Vision-CAIR/ChatCaptioner">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2303.05063.pdf"><strong>ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction</strong></a></td>
<td style="text-align: center;">ICCV</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/TaskMatrix.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.04671.pdf"><strong>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-08</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/TaskMatrix">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/microsoft/visual_chatgpt">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ZrrSkywalker/CaFo.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.02151.pdf"><strong>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://github.com/ZrrSkywalker/CaFo">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2212.10846.pdf"><strong>From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2022-12-21</td>
<td style="text-align: center;"><a href="https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa">Github</a></td>
<td style="text-align: center;"><a href="https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/img2llm-vqa/img2llm_vqa.ipynb">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/vishaal27/SuS-X.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2211.16198.pdf"><strong>SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2022-11-28</td>
<td style="text-align: center;"><a href="https://github.com/vishaal27/SuS-X">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yangyangyang127/PointCLIP_V2.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2211.11682.pdf"><strong>PointCLIP V2: Adapting CLIP for Powerful 3D Open-world Learning</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://github.com/yangyangyang127/PointCLIP_V2">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/allenai/visprog.svg?style=social&amp;label=Star" /> <br> <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf"><strong>Visual Programming: Compositional visual reasoning without training</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://github.com/allenai/visprog">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/google-research/google-research.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2204.00598.pdf"><strong>Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2022-04-01</td>
<td style="text-align: center;"><a href="https://github.com/google-research/google-research/tree/master/socraticmodels">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<h2 id="foundation-models">Foundation Models</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: center;">Venue</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Demo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA3.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2501.13106"><strong>VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2025-01-22</td>
<td style="text-align: center;"><a href="https://github.com/DAMO-NLP-SG/VideoLLaMA3">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/lixin4ever/VideoLLaMA3">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/baaivision/Emu3.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2409.18869"><strong>Emu3: Next-Token Prediction is All You Need</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-09-27</td>
<td style="text-align: center;"><a href="https://github.com/baaivision/Emu3">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/"><strong>Llama 3.2: Revolutionizing edge AI and vision with open, customizable models</strong></a></td>
<td style="text-align: center;">Meta</td>
<td style="text-align: center;">2024-09-25</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><a href="https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://mistral.ai/news/pixtral-12b/"><strong>Pixtral-12B</strong></a></td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">2024-09-17</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2408.08872"><strong>xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-16</td>
<td style="text-align: center;"><a href="https://github.com/salesforce/LAVIS/tree/xgen-mm">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2407.21783"><strong>The Llama 3 Herd of Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-07-31</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2405.09818"><strong>Chameleon: Mixed-Modal Early-Fusion Foundation Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-05-16</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://openai.com/index/hello-gpt-4o/"><strong>Hello GPT-4o</strong></a></td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: center;">2024-05-13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf"><strong>The Claude 3 Model Family: Opus, Sonnet, Haiku</strong></a></td>
<td style="text-align: center;">Anthropic</td>
<td style="text-align: center;">2024-03-04</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf"><strong>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</strong></a></td>
<td style="text-align: center;">Google</td>
<td style="text-align: center;">2024-02-15</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf"><strong>Gemini: A Family of Highly Capable Multimodal Models</strong></a></td>
<td style="text-align: center;">Google</td>
<td style="text-align: center;">2023-12-06</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://www.adept.ai/blog/fuyu-8b"><strong>Fuyu-8B: A Multimodal Architecture for AI Agents</strong></a></td>
<td style="text-align: center;">blog</td>
<td style="text-align: center;">2023-10-17</td>
<td style="text-align: center;"><a href="https://huggingface.co/adept/fuyu-8b">Huggingface</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/adept/fuyu-8b">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mshukor/UnIVAL.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.16184.pdf"><strong>Unified Model for Image, Video, Audio and Language Tasks</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-30</td>
<td style="text-align: center;"><a href="https://github.com/mshukor/UnIVAL">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/mshukor/UnIVAL">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2310.09199.pdf"><strong>PaLI-3 Vision Language Models: Smaller, Faster, Stronger</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf"><strong>GPT-4V(ision) System Card</strong></a></td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: center;">2023-09-25</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/jy0205/LaVIT.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.04669.pdf"><strong>Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-09</td>
<td style="text-align: center;"><a href="https://github.com/jy0205/LaVIT">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://browse.arxiv.org/pdf/2309.10020.pdf"><strong>Multimodal Foundation Models: From Specialists to General-Purpose Assistants</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-18</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yiren-jian/BLIText.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.07063.pdf"><strong>Bootstrapping Vision-Language Learning with Decoupled Language Pre-training</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2023-07-13</td>
<td style="text-align: center;"><a href="https://github.com/yiren-jian/BLIText">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.05222.pdf"><strong>Generative Pretraining in Multimodality</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-11</td>
<td style="text-align: center;"><a href="https://github.com/baaivision/Emu">Github</a></td>
<td style="text-align: center;"><a href="http://218.91.113.230:9002/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.14824.pdf"><strong>Kosmos-2: Grounding Multimodal Large Language Models to the World</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-26</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/unilm/tree/master/kosmos-2">Github</a></td>
<td style="text-align: center;"><a href="https://aka.ms/kosmos-2-demo">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/VPGTrans/VPGTrans.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.01278.pdf"><strong>Transfer Visual Prompt Generator across LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-02</td>
<td style="text-align: center;"><a href="https://github.com/VPGTrans/VPGTrans">Github</a></td>
<td style="text-align: center;"><a href="https://3fc7715dbc44234a7f.gradio.live/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2303.08774.pdf"><strong>GPT-4 Technical Report</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2303.03378.pdf"><strong>PaLM-E: An Embodied Multimodal Language Model</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-06</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><a href="https://palm-e.github.io/#demo">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/NVlabs/prismer.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2303.02506.pdf"><strong>Prismer: A Vision-Language Model with An Ensemble of Experts</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-03-04</td>
<td style="text-align: center;"><a href="https://github.com/NVlabs/prismer">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/lorenmt/prismer">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2302.14045.pdf"><strong>Language Is Not All You Need: Aligning Perception with Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-02-27</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/unilm">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2301.12597.pdf"><strong>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-01-30</td>
<td style="text-align: center;"><a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">Github</a></td>
<td style="text-align: center;"><a href="https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/vimalabs/VIMA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2210.03094.pdf"><strong>VIMA: General Robot Manipulation with Multimodal Prompts</strong></a> <br></td>
<td style="text-align: center;">ICML</td>
<td style="text-align: center;">2022-10-06</td>
<td style="text-align: center;"><a href="https://github.com/vimalabs/VIMA">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/MineDojo/MineDojo.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2206.08853.pdf"><strong>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</strong></a> <br></td>
<td style="text-align: center;">NeurIPS</td>
<td style="text-align: center;">2022-06-17</td>
<td style="text-align: center;"><a href="https://github.com/MineDojo/MineDojo">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/shizhediao/DaVinci.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2206.07699.pdf"><strong>Write and Paint: Generative Vision-Language Models are Unified Modal Learners</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2022-06-15</td>
<td style="text-align: center;"><a href="https://github.com/shizhediao/DaVinci">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2206.06336.pdf"><strong>Language Models are General-Purpose Interfaces</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2022-06-13</td>
<td style="text-align: center;"><a href="https://github.com/microsoft/unilm">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<h2 id="evaluation">Evaluation</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: center;">Venue</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Page</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/lerogo/MMGenBench?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2412.14171"><strong>Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-12-18</td>
<td style="text-align: center;"><a href="https://github.com/vision-x-nyu/thinking-in-space">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/lerogo/MMGenBench?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2411.14062"><strong>MMGenBench: Evaluating the Limits of LMMs from the Text-to-Image Generation Perspective</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-11-21</td>
<td style="text-align: center;"><a href="https://github.com/lerogo/MMGenBench">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/multimodal-art-projection/OmniBench?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2409.15272"><strong>OmniBench: Towards The Future of Universal Omni-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-09-23</td>
<td style="text-align: center;"><a href="https://github.com/multimodal-art-projection/OmniBench">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/yfzhang114/MME-RealWorld?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2408.13257"><strong>MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-23</td>
<td style="text-align: center;"><a href="https://github.com/yfzhang114/MME-RealWorld">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/guoyang9/UNK-VQA?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.10942"><strong>UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models</strong></a> <br></td>
<td style="text-align: center;">TPAMI</td>
<td style="text-align: center;">2023-10-17</td>
<td style="text-align: center;"><a href="https://github.com/guoyang9/UNK-VQA">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/chenllliang/MMEvalPro?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2407.00468"><strong>MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-29</td>
<td style="text-align: center;"><a href="https://github.com/chenllliang/MMEvalPro">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/MBZUAI-LLM/web2code?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.20098"><strong>Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-28</td>
<td style="text-align: center;"><a href="https://github.com/MBZUAI-LLM/web2code">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/princeton-nlp/CharXiv?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.18521"><strong>CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-06-26</td>
<td style="text-align: center;"><a href="https://github.com/princeton-nlp/CharXiv">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/ChartMimic/ChartMimic?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2406.09961"><strong>ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-04-15</td>
<td style="text-align: center;"><a href="https://github.com/ChartMimic/ChartMimic">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/BradyFU/Video-MME?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2405.21075"><strong>Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-05-31</td>
<td style="text-align: center;"><a href="https://github.com/BradyFU/Video-MME">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/sail-sg/MMCBench?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2401.11943.pdf"><strong>Benchmarking Large Multimodal Models against Common Corruptions</strong></a> <br></td>
<td style="text-align: center;">NAACL</td>
<td style="text-align: center;">2024-01-22</td>
<td style="text-align: center;"><a href="https://github.com/sail-sg/MMCBench">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/tsb0601/MMVP?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2401.06209.pdf"><strong>Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-01-11</td>
<td style="text-align: center;"><a href="https://github.com/tsb0601/MMVP">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.12436.pdf"><strong>A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-19</td>
<td style="text-align: center;"><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Stars" src="https://img.shields.io/github/stars/AIFEG/BenchLMM?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.02896.pdf"><strong>BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-05</td>
<td style="text-align: center;"><a href="https://github.com/AIFEG/BenchLMM">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/UCSC-VLAA/vllm-safety-benchmark.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.16101.pdf"><strong>How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-27</td>
<td style="text-align: center;"><a href="https://github.com/UCSC-VLAA/vllm-safety-benchmark">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/jonathan-roberts1/charting-new-territories.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.14656.pdf"><strong>Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-24</td>
<td style="text-align: center;"><a href="https://github.com/jonathan-roberts1/charting-new-territories">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/FreedomIntelligence/MLLM-Bench?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.13951"><strong>MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-23</td>
<td style="text-align: center;"><a href="https://github.com/FreedomIntelligence/MLLM-Bench">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2311.11865.pdf"><strong>VLM-Eval: A General Evaluation on Video Large Language Models</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-20</td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/gzcch/Bingo.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.03287.pdf"><strong>Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-06</td>
<td style="text-align: center;"><a href="https://github.com/gzcch/Bingo">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/PJLab-ADG/GPT4V-AD-Exploration.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2311.05332.pdf"><strong>On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-09</td>
<td style="text-align: center;"><a href="https://github.com/PJLab-ADG/GPT4V-AD-Exploration">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2311.02782.pdf"><strong>Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-11-05</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2310.20381.pdf"><strong>A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-31</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/albertwy/GPT-4V-Evaluation.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.16534.pdf"><strong>An Early Evaluation of GPT-4V(ision)</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-25</td>
<td style="text-align: center;"><a href="https://github.com/albertwy/GPT-4V-Evaluation">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/SCUT-DLVCLab/GPT-4V_OCR.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.16809.pdf"><strong>Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-25</td>
<td style="text-align: center;"><a href="https://github.com/SCUT-DLVCLab/GPT-4V_OCR">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/tianyi-lab/HallusionBench.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.14566.pdf"><strong>HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-10-23</td>
<td style="text-align: center;"><a href="https://github.com/tianyi-lab/HallusionBench">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/lupantech/MathVista.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.02255.pdf"><strong>MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models</strong></a> <br></td>
<td style="text-align: center;">ICLR</td>
<td style="text-align: center;">2023-10-03</td>
<td style="text-align: center;"><a href="https://github.com/lupantech/MathVista">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ys-zong/FoolyourVLLMs.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.01651.pdf"><strong>Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-02</td>
<td style="text-align: center;"><a href="https://github.com/ys-zong/FoolyourVLLMs">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/mshukor/EvALign-ICL.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.00647.pdf"><strong>Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-01</td>
<td style="text-align: center;"><a href="https://github.com/mshukor/EvALign-ICL">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/zjunlp/EasyEdit.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.08475.pdf"><strong>Can We Edit Multimodal Large Language Models?</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-12</td>
<td style="text-align: center;"><a href="https://github.com/zjunlp/EasyEdit">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/liaoning97/REVO-LION.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2310.06594.pdf"><strong>REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning Datasets</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-10-10</td>
<td style="text-align: center;"><a href="https://github.com/liaoning97/REVO-LION">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2309.17421.pdf"><strong>The Dawn of LMMs: Preliminary Explorations with GPT-4V(vision)</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-29</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OFA-Sys/TouchStone.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.16890.pdf"><strong>TouchStone: Evaluating Vision-Language Models by Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://github.com/OFA-Sys/TouchStone">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/HYPJUDY/Sparkles.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.16463.pdf"><strong>Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-31</td>
<td style="text-align: center;"><a href="https://github.com/HYPJUDY/Sparkles#sparkleseval">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/findalexli/SciGraphQA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.03349.pdf"><strong>SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-07</td>
<td style="text-align: center;"><a href="https://github.com/findalexli/SciGraphQA">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/Multi-Modality-Arena.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.03729.pdf"><strong>Tiny LVLM-eHub: Early Multimodal Experiments with Bard</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-07</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/Multi-Modality-Arena">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yuweihao/MM-Vet.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2308.02490.pdf"><strong>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-08-04</td>
<td style="text-align: center;"><a href="https://github.com/yuweihao/MM-Vet">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/AILab-CVC/SEED-Bench.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.16125.pdf"><strong>SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</strong></a> <br></td>
<td style="text-align: center;">CVPR</td>
<td style="text-align: center;">2023-07-30</td>
<td style="text-align: center;"><a href="https://github.com/AILab-CVC/SEED-Bench">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/open-compass/MMBench.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.06281.pdf"><strong>MMBench: Is Your Multi-modal Model an All-around Player?</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-12</td>
<td style="text-align: center;"><a href="https://github.com/open-compass/MMBench">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.13394.pdf"><strong>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-23</td>
<td style="text-align: center;"><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenGVLab/Multi-Modality-Arena.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.09265.pdf"><strong>LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-15</td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/Multi-Modality-Arena">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/OpenLAMM/LAMM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.06687.pdf"><strong>LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-11</td>
<td style="text-align: center;"><a href="https://github.com/OpenLAMM/LAMM#lamm-benchmark">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/DAMO-NLP-SG/M3Exam.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.05179.pdf"><strong>M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://github.com/DAMO-NLP-SG/M3Exam">Github</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.07895.pdf"><strong>On The Hidden Mystery of OCR in Large Multimodal Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-13</td>
<td style="text-align: center;"><a href="https://github.com/Yuliang-Liu/MultimodalOCR">Github</a></td>
</tr>
</tbody>
</table>
<h2 id="multimodal-rlhf">Multimodal RLHF</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: center;">Venue</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Demo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2410.06682"><strong>Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization</strong></a></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-10-09</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/vlf-silkie/VLFeedback.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.10665.pdf"><strong>Silkie: Preference Distillation for Large Visual Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-17</td>
<td style="text-align: center;"><a href="https://github.com/vlf-silkie/VLFeedback">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/RLHF-V/RLHF-V.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.00849.pdf"><strong>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-01</td>
<td style="text-align: center;"><a href="https://github.com/RLHF-V/RLHF-V">Github</a></td>
<td style="text-align: center;"><a href="http://120.92.209.146:8081/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/llava-rlhf/LLaVA-RLHF.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2309.14525.pdf"><strong>Aligning Large Multimodal Models with Factually Augmented RLHF</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-09-25</td>
<td style="text-align: center;"><a href="https://github.com/llava-rlhf/LLaVA-RLHF">Github</a></td>
<td style="text-align: center;"><a href="http://pitt.lti.cs.cmu.edu:7890/">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/wangclnlp/Vision-LLM-Alignment.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2408.12109"><strong>RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-08-22</td>
<td style="text-align: center;"><a href="https://github.com/wangclnlp/Vision-LLM-Alignment">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<h2 id="others">Others</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: center;">Venue</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Demo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/tingyu215/TS-LLaVA.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2411.11066"><strong>TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-11-17</td>
<td style="text-align: center;"><a href="https://github.com/tingyu215/TS-LLaVA">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/ys-zong/VLGuard.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2402.02207.pdf"><strong>Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2024-02-03</td>
<td style="text-align: center;"><a href="https://github.com/ys-zong/VLGuard">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/SHI-Labs/VCoder.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.14233.pdf"><strong>VCoder: Versatile Vision Encoders for Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-21</td>
<td style="text-align: center;"><a href="https://github.com/SHI-Labs/VCoder">Github</a></td>
<td style="text-align: center;">Local Demo</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/dvlab-research/Prompt-Highlighter.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2312.04302.pdf"><strong>Prompt Highlighter: Interactive Control for Multi-Modal LLMs</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-12-07</td>
<td style="text-align: center;"><a href="https://github.com/dvlab-research/Prompt-Highlighter">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/AIlab-CVC/SEED.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2307.08041.pdf"><strong>Planting a SEED of Vision in Large Language Model</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-07-16</td>
<td style="text-align: center;"><a href="https://github.com/AILab-CVC/SEED">Github</a></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/huawei-noah/Efficient-Computing.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2306.00693.pdf"><strong>Can Large Pre-trained Models Help Vision Models on Perception Tasks?</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://github.com/huawei-noah/Efficient-Computing/tree/master/GPT4Image/">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yuhangzang/ContextDET.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.18279.pdf"><strong>Contextual Object Detection with Multimodal Large Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-29</td>
<td style="text-align: center;"><a href="https://github.com/yuhangzang/ContextDET">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/yuhangzang/ContextDet-Demo">Demo</a></td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/kohjingyu/gill.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.17216.pdf"><strong>Generating Images with Multimodal Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-26</td>
<td style="text-align: center;"><a href="https://github.com/kohjingyu/gill">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/yunqing-me/AttackVLM.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2305.16934.pdf"><strong>On Evaluating Adversarial Robustness of Large Vision-Language Models</strong></a> <br></td>
<td style="text-align: center;">arXiv</td>
<td style="text-align: center;">2023-05-26</td>
<td style="text-align: center;"><a href="https://github.com/yunqing-me/AttackVLM">Github</a></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"><img alt="Star" src="https://img.shields.io/github/stars/kohjingyu/fromage.svg?style=social&amp;label=Star" /> <br> <a href="https://arxiv.org/pdf/2301.13823.pdf"><strong>Grounding Language Models to Images for Multimodal Inputs and Outputs</strong></a> <br></td>
<td style="text-align: center;">ICML</td>
<td style="text-align: center;">2023-01-31</td>
<td style="text-align: center;"><a href="https://github.com/kohjingyu/fromage">Github</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/spaces/jykoh/fromage">Demo</a></td>
</tr>
</tbody>
</table>
<hr />
<h1 id="awesome-datasets">Awesome Datasets</h1>
<h2 id="datasets-of-pre-training-for-alignment">Datasets of Pre-Training for Alignment</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Modalities</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>ShareGPT4Video</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2406.04325v1">ShareGPT4Video: Improving Video Understanding and Generation with Better Captions</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Video-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>COYO-700M</strong></td>
<td style="text-align: center;"><a href="https://github.com/kakaobrain/coyo-dataset/">COYO-700M: Image-Text Pair Dataset</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ShareGPT4V</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.12793.pdf">ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>AS-1B</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.01907.pdf">The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World</a></td>
<td style="text-align: center;">Hybrid</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>InternVid</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2307.06942.pdf">InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Video-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MS-COCO</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/1405.0312.pdf">Microsoft COCO: Common Objects in Context</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>SBU Captions</strong></td>
<td style="text-align: center;"><a href="https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf">Im2Text: Describing Images Using 1 Million Captioned Photographs</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Conceptual Captions</strong></td>
<td style="text-align: center;"><a href="https://aclanthology.org/P18-1238.pdf">Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LAION-400M</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2111.02114.pdf">LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>VG Captions</strong></td>
<td style="text-align: center;"><a href="https://link.springer.com/content/pdf/10.1007/s11263-016-0981-7.pdf">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Flickr30k</strong></td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.pdf">Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>AI-Caps</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/1711.06475.pdf">AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Wukong Captions</strong></td>
<td style="text-align: center;"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/a90b9a09a6ee43d6631cf42e225d73b4-Paper-Datasets_and_Benchmarks.pdf">Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>GRIT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.14824.pdf">Kosmos-2: Grounding Multimodal Large Language Models to the World</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Image-Text-Bounding-Box</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Youku-mPLUG</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.04362.pdf">Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Video-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MSR-VTT</strong></td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf">MSR-VTT: A Large Video Description Dataset for Bridging Video and Language</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Video-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Webvid10M</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2104.00650.pdf">Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Video-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>WavCaps</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2303.17395.pdf">WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research</a></td>
<td style="text-align: center;">Caption</td>
<td style="text-align: center;">Audio-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>AISHELL-1</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/1709.05522.pdf">AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline</a></td>
<td style="text-align: center;">ASR</td>
<td style="text-align: center;">Audio-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>AISHELL-2</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/1808.10583.pdf">AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale</a></td>
<td style="text-align: center;">ASR</td>
<td style="text-align: center;">Audio-Text</td>
</tr>
<tr>
<td style="text-align: left;"><strong>VSDial-CN</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.04160.pdf">X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages</a></td>
<td style="text-align: center;">ASR</td>
<td style="text-align: center;">Image-Audio-Text</td>
</tr>
</tbody>
</table>
<h2 id="datasets-of-multimodal-instruction-tuning">Datasets of Multimodal Instruction Tuning</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Link</th>
<th style="text-align: center;">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Inst-IT Dataset</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2412.03565">Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://github.com/inst-it/inst-it">Link</a></td>
<td style="text-align: center;">An instruction-tuning dataset which contains fine-grained multi-level annotations for 21k videos and 51k images</td>
</tr>
<tr>
<td style="text-align: left;"><strong>E.T. Instruct 164K</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2409.18111">E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding</a></td>
<td style="text-align: center;"><a href="https://github.com/PolyU-ChenLab/ETBench">Link</a></td>
<td style="text-align: center;">An instruction-tuning dataset for time-sensitive video understanding</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MSQA</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2409.02389">Multi-modal Situated Reasoning in 3D Scenes</a></td>
<td style="text-align: center;"><a href="https://msr3d.github.io/">Link</a></td>
<td style="text-align: center;">A large scale dataset for multi-modal situated reasoning in 3D scenes</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MM-Evol</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2409.05840">MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct</a></td>
<td style="text-align: center;"><a href="https://mmevol.github.io/home_page.html">Link</a></td>
<td style="text-align: center;">An instruction dataset with rich diversity</td>
</tr>
<tr>
<td style="text-align: left;"><strong>UNK-VQA</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2310.10942">UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models</a></td>
<td style="text-align: center;"><a href="https://github.com/guoyang9/UNK-VQA">Link</a></td>
<td style="text-align: center;">A dataset designed to teach models to refrain from answering unanswerable questions</td>
</tr>
<tr>
<td style="text-align: left;"><strong>VEGA</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2406.10228">VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models</a></td>
<td style="text-align: center;"><a href="https://github.com/zhourax/VEGA">Link</a></td>
<td style="text-align: center;">A dataset for enhancing model capabilities in comprehension of interleaved information</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ALLaVA-4V</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2402.11684.pdf">ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V">Link</a></td>
<td style="text-align: center;">Vision and language caption and instruction dataset generated by GPT4V</td>
</tr>
<tr>
<td style="text-align: left;"><strong>IDK</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2402.09717.pdf">Visually Dehallucinative Instruction Generation: Know What You Don't Know</a></td>
<td style="text-align: center;"><a href="https://github.com/ncsoft/idk">Link</a></td>
<td style="text-align: center;">Dehallucinative visual instruction for "I Know" hallucination</td>
</tr>
<tr>
<td style="text-align: left;"><strong>CAP2QA</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2402.08348.pdf">Visually Dehallucinative Instruction Generation</a></td>
<td style="text-align: center;"><a href="https://github.com/ncsoft/cap2qa">Link</a></td>
<td style="text-align: center;">Image-aligned visual instruction dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>M3DBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2312.10763.pdf">M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts</a></td>
<td style="text-align: center;"><a href="https://github.com/OpenM3D/M3DBench">Link</a></td>
<td style="text-align: center;">A large-scale 3D instruction tuning dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ViP-LLaVA-Instruct</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2312.00784.pdf">Making Large Multimodal Models Understand Arbitrary Visual Prompts</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/mucai/ViP-LLaVA-Instruct">Link</a></td>
<td style="text-align: center;">A mixture of LLaVA-1.5 instruction data and the region-level visual prompting data</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LVIS-Instruct4V</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.07574.pdf">To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/X2FD/LVIS-Instruct4V">Link</a></td>
<td style="text-align: center;">A visual instruction dataset via self-instruction from GPT-4V</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ComVint</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.01487.pdf">What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://github.com/RUCAIBox/ComVint#comvint-data">Link</a></td>
<td style="text-align: center;">A synthetic instruction dataset for complex visual reasoning</td>
</tr>
<tr>
<td style="text-align: left;"><strong>SparklesDialogue</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.16463.pdf">Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models</a></td>
<td style="text-align: center;"><a href="https://github.com/HYPJUDY/Sparkles#sparklesdialogue">Link</a></td>
<td style="text-align: center;">A machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions to augment the conversational competence of instruction-following LLMs across multiple images and dialogue turns.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>StableLLaVA</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.10253v1.pdf">StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data</a></td>
<td style="text-align: center;"><a href="https://github.com/icoz69/StableLLAVA">Link</a></td>
<td style="text-align: center;">A cheap and effective approach to collect visual instruction tuning data</td>
</tr>
<tr>
<td style="text-align: left;"><strong>M-HalDetect</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.06394.pdf">Detecting and Preventing Hallucinations in Large Vision Language Models</a></td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">A dataset used to train and benchmark models for hallucination detection and prevention</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MGVLID</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2307.09474.pdf">ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning</a></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">A high-quality instruction-tuning dataset including image-text and region-text pairs</td>
</tr>
<tr>
<td style="text-align: left;"><strong>BuboGPT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2307.08581.pdf">BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/magicr/BuboGPT">Link</a></td>
<td style="text-align: center;">A high-quality instruction-tuning dataset including audio-text audio caption data and audio-image-text localization data</td>
</tr>
<tr>
<td style="text-align: left;"><strong>SVIT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2307.04087.pdf">SVIT: Scaling up Visual Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/BAAI/SVIT">Link</a></td>
<td style="text-align: center;">A large-scale dataset with 4.2M informative visual instruction tuning data, including conversations, detailed descriptions, complex reasoning and referring QAs</td>
</tr>
<tr>
<td style="text-align: left;"><strong>mPLUG-DocOwl</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2307.02499.pdf">mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding</a></td>
<td style="text-align: center;"><a href="https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocLLM">Link</a></td>
<td style="text-align: center;">An instruction tuning dataset featuring a wide range of visual-text understanding tasks including OCR-free document understanding</td>
</tr>
<tr>
<td style="text-align: left;"><strong>PF-1M</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2307.01003.pdf">Visual Instruction Tuning with Polite Flamingo</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/chendelong/PF-1M/tree/main">Link</a></td>
<td style="text-align: center;">A collection of 37 vision-language datasets with responses rewritten by Polite Flamingo.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ChartLlama</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.16483.pdf">ChartLlama: A Multimodal LLM for Chart Understanding and Generation</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/listen2you002/ChartLlama-Dataset">Link</a></td>
<td style="text-align: center;">A multi-modal instruction-tuning dataset for chart understanding and generation</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LLaVAR</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.17107.pdf">LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding</a></td>
<td style="text-align: center;"><a href="https://llavar.github.io/#data">Link</a></td>
<td style="text-align: center;">A visual instruction-tuning dataset for Text-rich Image Understanding</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MotionGPT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.14795.pdf">MotionGPT: Human Motion as a Foreign Language</a></td>
<td style="text-align: center;"><a href="https://github.com/OpenMotionLab/MotionGPT">Link</a></td>
<td style="text-align: center;">A instruction-tuning dataset including multiple human motion-related tasks</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LRV-Instruction</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.14565.pdf">Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://github.com/FuxiaoLiu/LRV-Instruction#visual-instruction-data-lrv-instruction">Link</a></td>
<td style="text-align: center;">Visual instruction tuning dataset for addressing hallucination issue</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Macaw-LLM</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.09093.pdf">Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration</a></td>
<td style="text-align: center;"><a href="https://github.com/lyuchenyang/Macaw-LLM/tree/main/data">Link</a></td>
<td style="text-align: center;">A large-scale multi-modal instruction dataset in terms of multi-turn dialogue</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LAMM-Dataset</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.06687.pdf">LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</a></td>
<td style="text-align: center;"><a href="https://github.com/OpenLAMM/LAMM#lamm-dataset">Link</a></td>
<td style="text-align: center;">A comprehensive multi-modal instruction tuning dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Video-ChatGPT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.05424.pdf">Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</a></td>
<td style="text-align: center;"><a href="https://github.com/mbzuai-oryx/Video-ChatGPT#video-instruction-dataset-open_file_folder">Link</a></td>
<td style="text-align: center;">100K high-quality video instruction dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MIMIC-IT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.05425.pdf">MIMIC-IT: Multi-Modal In-Context Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://github.com/Luodian/Otter/blob/main/mimic-it/README.md">Link</a></td>
<td style="text-align: center;">Multimodal in-context instruction tuning</td>
</tr>
<tr>
<td style="text-align: left;"><strong>M<sup>3</sup>IT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.04387.pdf">M<sup>3</sup>IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/MMInstruction/M3IT">Link</a></td>
<td style="text-align: center;">Large-scale, broad-coverage multimodal instruction tuning dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LLaVA-Med</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.00890.pdf">LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day</a></td>
<td style="text-align: center;"><a href="https://github.com/microsoft/LLaVA-Med#llava-med-dataset">Coming soon</a></td>
<td style="text-align: center;">A large-scale, broad-coverage biomedical instruction-following dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>GPT4Tools</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.18752.pdf">GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</a></td>
<td style="text-align: center;"><a href="https://github.com/StevenGrove/GPT4Tools#dataset">Link</a></td>
<td style="text-align: center;">Tool-related instruction datasets</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MULTIS</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.16103.pdf">ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst</a></td>
<td style="text-align: center;"><a href="https://iva-chatbridge.github.io/">Coming soon</a></td>
<td style="text-align: center;">Multimodal instruction tuning dataset covering 16 multimodal tasks</td>
</tr>
<tr>
<td style="text-align: left;"><strong>DetGPT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.14167.pdf">DetGPT: Detect What You Need via Reasoning</a></td>
<td style="text-align: center;"><a href="https://github.com/OptimalScale/DetGPT/tree/main/dataset">Link</a></td>
<td style="text-align: center;">Instruction-tuning dataset with 5000 images and around 30000 query-answer pairs</td>
</tr>
<tr>
<td style="text-align: left;"><strong>PMC-VQA</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.10415.pdf">PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</a></td>
<td style="text-align: center;"><a href="https://xiaoman-zhang.github.io/PMC-VQA/">Coming soon</a></td>
<td style="text-align: center;">Large-scale medical visual question-answering dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>VideoChat</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.06355.pdf">VideoChat: Chat-Centric Video Understanding</a></td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data">Link</a></td>
<td style="text-align: center;">Video-centric multimodal instruction dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>X-LLM</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.04160.pdf">X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages</a></td>
<td style="text-align: center;"><a href="https://github.com/phellonchen/X-LLM">Link</a></td>
<td style="text-align: center;">Chinese multimodal instruction dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LMEye</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.03701.pdf">LMEye: An Interactive Perception Network for Large Language Models</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/YunxinLi/Multimodal_Insturction_Data_V2">Link</a></td>
<td style="text-align: center;">A multi-modal instruction-tuning dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>cc-sbu-align</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2304.10592.pdf">MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align">Link</a></td>
<td style="text-align: center;">Multimodal aligned dataset for improving model's usability and generation's fluency</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LLaVA-Instruct-150K</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2304.08485.pdf">Visual Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K">Link</a></td>
<td style="text-align: center;">Multimodal instruction-following data generated by GPT</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MultiInstruct</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2212.10773.pdf">MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://github.com/VT-NLP/MultiInstruct">Link</a></td>
<td style="text-align: center;">The first multimodal instruction tuning benchmark dataset</td>
</tr>
</tbody>
</table>
<h2 id="datasets-of-in-context-learning">Datasets of In-Context Learning</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Link</th>
<th style="text-align: center;">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>MIC</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2309.07915.pdf">MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/BleachNick/MIC_full">Link</a></td>
<td style="text-align: center;">A manually constructed instruction tuning dataset including interleaved text-image inputs, inter-related multiple image inputs, and multimodal in-context learning inputs.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MIMIC-IT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.05425.pdf">MIMIC-IT: Multi-Modal In-Context Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://github.com/Luodian/Otter/blob/main/mimic-it/README.md">Link</a></td>
<td style="text-align: center;">Multimodal in-context instruction dataset</td>
</tr>
</tbody>
</table>
<h2 id="datasets-of-multimodal-chain-of-thought">Datasets of Multimodal Chain-of-Thought</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Link</th>
<th style="text-align: center;">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>EMER</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.15401.pdf">Explainable Multimodal Emotion Reasoning</a></td>
<td style="text-align: center;"><a href="https://github.com/zeroQiaoba/Explainable-Multimodal-Emotion-Reasoning">Coming soon</a></td>
<td style="text-align: center;">A benchmark dataset for explainable emotion reasoning task</td>
</tr>
<tr>
<td style="text-align: left;"><strong>EgoCOT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.15021.pdf">EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought</a></td>
<td style="text-align: center;"><a href="https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch">Coming soon</a></td>
<td style="text-align: center;">Large-scale embodied planning dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>VIP</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.13903.pdf">Lets Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction</a></td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">An inference-time dataset that can be used to evaluate VideoCOT</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ScienceQA</strong></td>
<td style="text-align: center;"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf">Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering</a></td>
<td style="text-align: center;"><a href="https://github.com/lupantech/ScienceQA#ghost-download-the-dataset">Link</a></td>
<td style="text-align: center;">Large-scale multi-choice dataset, featuring multimodal science questions and diverse domains</td>
</tr>
</tbody>
</table>
<h2 id="datasets-of-multimodal-rlhf">Datasets of Multimodal RLHF</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Link</th>
<th style="text-align: center;">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>VLFeedback</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2312.10665.pdf">Silkie: Preference Distillation for Large Visual Language Models</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/MMInstruction/VLFeedback">Link</a></td>
<td style="text-align: center;">A vision-language feedback dataset annotated by AI</td>
</tr>
</tbody>
</table>
<h2 id="benchmarks-for-evaluation">Benchmarks for Evaluation</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Link</th>
<th style="text-align: center;">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Inst-IT Bench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2412.03565">Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://github.com/inst-it/inst-it">Link</a></td>
<td style="text-align: center;">A benchmark to evaluate fine-grained instance-level understanding in images and videos</td>
</tr>
<tr>
<td style="text-align: left;"><strong>M<sup>3</sup>CoT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2405.16473">M<sup>3</sup>CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</a></td>
<td style="text-align: center;"><a href="https://github.com/LightChen233/M3CoT">Link</a></td>
<td style="text-align: center;">A multi-domain, multi-step benchmark for multimodal CoT</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MMGenBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2411.14062">MMGenBench: Evaluating the Limits of LMMs from the Text-to-Image Generation Perspective</a></td>
<td style="text-align: center;"><a href="https://github.com/lerogo/MMGenBench">Link</a></td>
<td style="text-align: center;">A benchmark that gauges the performance of constructing image-generation prompt given an image</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MiCEval</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2410.14668">MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps</a></td>
<td style="text-align: center;"><a href="https://github.com/alenai97/MiCEval">Link</a></td>
<td style="text-align: center;">A multimodal CoT benchmark to evaluate MLLMs' reasoning capabilities</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LiveXiv</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2410.10783">LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/LiveXiv/LiveXiv">Link</a></td>
<td style="text-align: center;">A live benchmark based on arXiv papers</td>
</tr>
<tr>
<td style="text-align: left;"><strong>TemporalBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2410.10818">TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/microsoft/TemporalBench">Link</a></td>
<td style="text-align: center;">A benchmark for evaluation of fine-grained temporal understanding</td>
</tr>
<tr>
<td style="text-align: left;"><strong>OmniBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2409.15272">OmniBench: Towards The Future of Universal Omni-Language Models</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/m-a-p/OmniBench">Link</a></td>
<td style="text-align: center;">A benchmark that evaluates models' capabilities of processing visual, acoustic, and textual inputs simultaneously</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MME-RealWorld</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2408.13257">MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/yifanzhang114/MME-RealWorld">Link</a></td>
<td style="text-align: center;">A challenging benchmark that involves real-life scenarios</td>
</tr>
<tr>
<td style="text-align: left;"><strong>VELOCITI</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2406.10889">VELOCITI: Can Video-Language Models Bind Semantic Concepts through Time?</a></td>
<td style="text-align: center;"><a href="https://github.com/katha-ai/VELOCITI">Link</a></td>
<td style="text-align: center;">A video benhcmark that evaluates on perception and binding capabilities</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MMR</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2406.10638.pdf">Seeing Clearly, Answering Incorrectly: A Multimodal Robustness Benchmark for Evaluating MLLMs on Leading Questions</a></td>
<td style="text-align: center;"><a href="https://github.com/BAAI-DCAI/Multimodal-Robustness-Benchmark">Link</a></td>
<td style="text-align: center;">A benchmark for measuring MLLMs' understanding capability and robustness to leading questions</td>
</tr>
<tr>
<td style="text-align: left;"><strong>CharXiv</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2406.18521">CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/princeton-nlp/CharXiv">Link</a></td>
<td style="text-align: center;">Chart understanding benchmark curated by human experts</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Video-MME</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2405.21075">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</a></td>
<td style="text-align: center;"><a href="https://github.com/BradyFU/Video-MME">Link</a></td>
<td style="text-align: center;">A comprehensive evaluation benchmark of Multi-modal LLMs in video analysis</td>
</tr>
<tr>
<td style="text-align: left;"><strong>VL-ICL Bench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2403.13164.pdf">VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning</a></td>
<td style="text-align: center;"><a href="https://github.com/ys-zong/VL-ICL">Link</a></td>
<td style="text-align: center;">A benchmark for M-ICL evaluation, covering a wide spectrum of tasks</td>
</tr>
<tr>
<td style="text-align: left;"><strong>TempCompass</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2403.00476.pdf">TempCompass: Do Video LLMs Really Understand Videos?</a></td>
<td style="text-align: center;"><a href="https://github.com/llyx97/TempCompass">Link</a></td>
<td style="text-align: center;">A benchmark to evaluate the temporal perception ability of Video LLMs</td>
</tr>
<tr>
<td style="text-align: left;"><strong>GVLQA</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2402.02130">GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/collections/Yanbin99/gvlqa-datasets-65c705c9488606617e246bd3">Link</a></td>
<td style="text-align: center;">A benchmark for evaluation of graph reasoning capabilities</td>
</tr>
<tr>
<td style="text-align: left;"><strong>CoBSAT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2402.01293.pdf">Can MLLMs Perform Text-to-Image In-Context Learning?</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/yzeng58/CoBSAT">Link</a></td>
<td style="text-align: center;">A benchmark for text-to-image ICL</td>
</tr>
<tr>
<td style="text-align: left;"><strong>VQAv2-IDK</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2402.09717.pdf">Visually Dehallucinative Instruction Generation: Know What You Don't Know</a></td>
<td style="text-align: center;"><a href="https://github.com/ncsoft/idk">Link</a></td>
<td style="text-align: center;">A benchmark for assessing "I Know" visual hallucination</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Math-Vision</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2402.14804.pdf">Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset</a></td>
<td style="text-align: center;"><a href="https://github.com/mathvision-cuhk/MathVision">Link</a></td>
<td style="text-align: center;">A diverse mathematical reasoning benchmark</td>
</tr>
<tr>
<td style="text-align: left;"><strong>SciMMIR</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2401.13478">SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval</a></td>
<td style="text-align: center;"><a href="https://github.com/Wusiwei0410/SciMMIR">Link</a></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"><strong>CMMMU</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2401.11944.pdf">CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark</a></td>
<td style="text-align: center;"><a href="https://github.com/CMMMU-Benchmark/CMMMU">Link</a></td>
<td style="text-align: center;">A Chinese benchmark involving reasoning and knowledge across multiple disciplines</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MMCBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2401.11943.pdf">Benchmarking Large Multimodal Models against Common Corruptions</a></td>
<td style="text-align: center;"><a href="https://github.com/sail-sg/MMCBench">Link</a></td>
<td style="text-align: center;">A benchmark for examining self-consistency under common corruptions</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MMVP</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2401.06209.pdf">Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</a></td>
<td style="text-align: center;"><a href="https://github.com/tsb0601/MMVP">Link</a></td>
<td style="text-align: center;">A benchmark for assessing visual capabilities</td>
</tr>
<tr>
<td style="text-align: left;"><strong>TimeIT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2312.02051.pdf">TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/ShuhuaiRen/TimeIT">Link</a></td>
<td style="text-align: center;">A video instruction-tuning dataset with timestamp annotations, covering diverse time-sensitive video-understanding tasks.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ViP-Bench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2312.00784.pdf">Making Large Multimodal Models Understand Arbitrary Visual Prompts</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/mucai/ViP-Bench">Link</a></td>
<td style="text-align: center;">A benchmark for visual prompts</td>
</tr>
<tr>
<td style="text-align: left;"><strong>M3DBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2312.10763.pdf">M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts</a></td>
<td style="text-align: center;"><a href="https://github.com/OpenM3D/M3DBench">Link</a></td>
<td style="text-align: center;">A 3D-centric benchmark</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Video-Bench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.16103.pdf">Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models</a></td>
<td style="text-align: center;"><a href="https://github.com/PKU-YuanGroup/Video-Bench">Link</a></td>
<td style="text-align: center;">A benchmark for video-MLLM evaluation</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Charting-New-Territories</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.14656.pdf">Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs</a></td>
<td style="text-align: center;"><a href="https://github.com/jonathan-roberts1/charting-new-territories">Link</a></td>
<td style="text-align: center;">A benchmark for evaluating geographic and geospatial capabilities</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MLLM-Bench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.13951.pdf">MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V</a></td>
<td style="text-align: center;"><a href="https://github.com/FreedomIntelligence/MLLM-Bench">Link</a></td>
<td style="text-align: center;">GPT-4V evaluation with per-sample criteria</td>
</tr>
<tr>
<td style="text-align: left;"><strong>BenchLMM</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2312.02896.pdf">BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/AIFEG/BenchLMM">Link</a></td>
<td style="text-align: center;">A benchmark for assessment of the robustness against different image styles</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MMC-Benchmark</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.10774.pdf">MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://github.com/FuxiaoLiu/MMC">Link</a></td>
<td style="text-align: center;">A comprehensive human-annotated benchmark with distinct tasks evaluating reasoning capabilities over charts</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MVBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.17005.pdf">MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</a></td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/Ask-Anything/blob/main/video_chat2/MVBENCH.md">Link</a></td>
<td style="text-align: center;">A comprehensive multimodal benchmark for video understanding</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Bingo</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.03287.pdf">Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges</a></td>
<td style="text-align: center;"><a href="https://github.com/gzcch/Bingo">Link</a></td>
<td style="text-align: center;">A benchmark for hallucination evaluation that focuses on two common types</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MagnifierBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.04219.pdf">OtterHD: A High-Resolution Multi-modality Model</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/Otter-AI/MagnifierBench">Link</a></td>
<td style="text-align: center;">A benchmark designed to probe models' ability of fine-grained perception</td>
</tr>
<tr>
<td style="text-align: left;"><strong>HallusionBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2310.14566.pdf">HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models</a></td>
<td style="text-align: center;"><a href="https://github.com/tianyi-lab/HallusionBench">Link</a></td>
<td style="text-align: center;">An image-context reasoning benchmark for evaluation of hallucination</td>
</tr>
<tr>
<td style="text-align: left;"><strong>PCA-EVAL</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2310.02071.pdf">Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond</a></td>
<td style="text-align: center;"><a href="https://github.com/pkunlp-icler/PCA-EVAL">Link</a></td>
<td style="text-align: center;">A benchmark for evaluating multi-domain embodied decision-making.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MMHal-Bench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2309.14525.pdf">Aligning Large Multimodal Models with Factually Augmented RLHF</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/Shengcao1006/MMHal-Bench">Link</a></td>
<td style="text-align: center;">A benchmark for hallucination evaluation</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MathVista</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2310.02255.pdf">MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/datasets/AI4Math/MathVista">Link</a></td>
<td style="text-align: center;">A benchmark that challenges both visual and math reasoning capabilities</td>
</tr>
<tr>
<td style="text-align: left;"><strong>SparklesEval</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.16463.pdf">Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models</a></td>
<td style="text-align: center;"><a href="https://github.com/HYPJUDY/Sparkles#sparkleseval">Link</a></td>
<td style="text-align: center;">A GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns based on three distinct criteria.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ISEKAI</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.07891.pdf">Link-Context Learning for Multimodal LLMs</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/ISEKAI-Portal">Link</a></td>
<td style="text-align: center;">A benchmark comprising exclusively of unseen generated image-label pairs designed for link-context learning</td>
</tr>
<tr>
<td style="text-align: left;"><strong>M-HalDetect</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.06394.pdf">Detecting and Preventing Hallucinations in Large Vision Language Models</a></td>
<td style="text-align: center;"><a href="">Coming soon</a></td>
<td style="text-align: center;">A dataset used to train and benchmark models for hallucination detection and prevention</td>
</tr>
<tr>
<td style="text-align: left;"><strong>I4</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.04152.pdf">Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions</a></td>
<td style="text-align: center;"><a href="https://github.com/DCDmllm/Cheetah">Link</a></td>
<td style="text-align: center;">A benchmark to comprehensively evaluate the instruction following ability on complicated interleaved vision-language instructions</td>
</tr>
<tr>
<td style="text-align: left;"><strong>SciGraphQA</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.03349.pdf">SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs</a></td>
<td style="text-align: center;"><a href="https://github.com/findalexli/SciGraphQA#data">Link</a></td>
<td style="text-align: center;">A large-scale chart-visual question-answering dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MM-Vet</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2308.02490.pdf">MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</a></td>
<td style="text-align: center;"><a href="https://github.com/yuweihao/MM-Vet">Link</a></td>
<td style="text-align: center;">An evaluation benchmark that examines large multimodal models on complicated multimodal tasks</td>
</tr>
<tr>
<td style="text-align: left;"><strong>SEED-Bench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2307.16125.pdf">SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</a></td>
<td style="text-align: center;"><a href="https://github.com/AILab-CVC/SEED-Bench">Link</a></td>
<td style="text-align: center;">A benchmark for evaluation of generative comprehension in MLLMs</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MMBench</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2307.06281.pdf">MMBench: Is Your Multi-modal Model an All-around Player?</a></td>
<td style="text-align: center;"><a href="https://github.com/open-compass/MMBench">Link</a></td>
<td style="text-align: center;">A systematically-designed objective benchmark for robustly evaluating the various abilities of vision-language models</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Lynx</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2307.02469.pdf">What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?</a></td>
<td style="text-align: center;"><a href="https://github.com/bytedance/lynx-llm#prepare-data">Link</a></td>
<td style="text-align: center;">A comprehensive evaluation benchmark including both image and video tasks</td>
</tr>
<tr>
<td style="text-align: left;"><strong>GAVIE</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.14565.pdf">Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</a></td>
<td style="text-align: center;"><a href="https://github.com/FuxiaoLiu/LRV-Instruction#evaluationgavie">Link</a></td>
<td style="text-align: center;">A benchmark to evaluate the hallucination and instruction following ability</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MME</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.13394.pdf">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a></td>
<td style="text-align: center;"><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation">Link</a></td>
<td style="text-align: center;">A comprehensive MLLM Evaluation benchmark</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LVLM-eHub</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.09265.pdf">LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</a></td>
<td style="text-align: center;"><a href="https://github.com/OpenGVLab/Multi-Modality-Arena">Link</a></td>
<td style="text-align: center;">An evaluation platform for MLLMs</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LAMM-Benchmark</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.06687.pdf">LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</a></td>
<td style="text-align: center;"><a href="https://github.com/OpenLAMM/LAMM#lamm-benchmark">Link</a></td>
<td style="text-align: center;">A benchmark for evaluating  the quantitative performance of MLLMs on various2D/3D vision tasks</td>
</tr>
<tr>
<td style="text-align: left;"><strong>M3Exam</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.05179.pdf">M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models</a></td>
<td style="text-align: center;"><a href="https://github.com/DAMO-NLP-SG/M3Exam">Link</a></td>
<td style="text-align: center;">A multilingual, multimodal, multilevel benchmark for evaluating MLLM</td>
</tr>
<tr>
<td style="text-align: left;"><strong>OwlEval</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2304.14178.pdf">mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality</a></td>
<td style="text-align: center;"><a href="https://github.com/X-PLUG/mPLUG-Owl/tree/main/OwlEval">Link</a></td>
<td style="text-align: center;">Dataset for evaluation on multiple capabilities</td>
</tr>
</tbody>
</table>
<h2 id="others_1">Others</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Link</th>
<th style="text-align: center;">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>IMAD</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2305.10512.pdf">IMAD: IMage-Augmented multi-modal Dialogue</a></td>
<td style="text-align: center;"><a href="https://github.com/VityaVitalich/IMAD">Link</a></td>
<td style="text-align: center;">Multimodal dialogue dataset</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Video-ChatGPT</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2306.05424.pdf">Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</a></td>
<td style="text-align: center;"><a href="https://github.com/mbzuai-oryx/Video-ChatGPT#quantitative-evaluation-bar_chart">Link</a></td>
<td style="text-align: center;">A quantitative evaluation framework for video-based dialogue models</td>
</tr>
<tr>
<td style="text-align: left;"><strong>CLEVR-ATVC</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2303.05983.pdf">Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</a></td>
<td style="text-align: center;"><a href="https://drive.google.com/drive/folders/1TqBzkyqxOSg1hgCXF8JjpYIAuRV-uVft">Link</a></td>
<td style="text-align: center;">A synthetic multimodal fine-tuning dataset for learning to reject instructions</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Fruit-ATVC</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2303.05983.pdf">Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</a></td>
<td style="text-align: center;"><a href="https://drive.google.com/drive/folders/1Saaia2rRRb1nz5sKdmpzYdS4jHiMDaP0">Link</a></td>
<td style="text-align: center;">A manually pictured multimodal fine-tuning dataset for learning to reject instructions</td>
</tr>
<tr>
<td style="text-align: left;"><strong>InfoSeek</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2302.11713.pdf">Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?</a></td>
<td style="text-align: center;"><a href="https://open-vision-language.github.io/infoseek/">Link</a></td>
<td style="text-align: center;">A VQA dataset that focuses on asking information-seeking questions</td>
</tr>
<tr>
<td style="text-align: left;"><strong>OVEN</strong></td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2302.11154.pdf">Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities</a></td>
<td style="text-align: center;"><a href="https://open-vision-language.github.io/oven/">Link</a></td>
<td style="text-align: center;">A dataset that focuses on recognizing the Visual Entity on the Wikipedia, from images in the wild</td>
</tr>
</tbody>
</table>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.indexes"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5090c770.min.js"></script>
      
    
  </body>
</html>